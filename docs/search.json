[
  {
    "objectID": "Pr9/countmodel-points.html",
    "href": "Pr9/countmodel-points.html",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\]\nwhere \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\]\nwhen using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-points.html#data-organisation",
    "href": "Pr9/countmodel-points.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Data organisation",
    "text": "Data organisation\nNote there is no Effort field for these point count data.\n\nnewthrasher <- merge(thrasherDetectionData, thrasherSiteData, by=\"siteID\", all=TRUE)\nnames(newthrasher) <- sub(\"observer\", \"obs\", names(newthrasher))\nnames(newthrasher) <- sub(\"dist\", \"distance\", names(newthrasher))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nSpecification of run-specific parameters to analyse the Sage thrasher data set. Particularly note the logical value assigned to pointtransect. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height. The same predictors, with the exception of observer could be used to model the thrasher counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- TRUE        # survey conducted using lines or points\ndettrunc <- 170  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 100      # number of bootstrap replicates\nset.seed(7079)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-points.html#fit-detection-function",
    "href": "Pr9/countmodel-points.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Sage thrasher data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newthrasher, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(shrub), \n           transect=surveytype, quiet=TRUE)\nwoo.herb <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(herb), \n           transect=surveytype, quiet=TRUE)\nwoo.height <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(height), \n           transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newthrasher, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo, woo.o, woo.shrub, woo.herb, woo.height),\n             digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\",\n             row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~obs\n0.009\n0.292\n0.039\n0.000\n\n\n\nHalf-normal with cosine adjustment term of order 2\n~1\n0.097\n0.395\n0.124\n12.889\n\n\n\nHalf-normal\n~scale(herb)\n0.032\n0.331\n0.034\n17.893\n\n\n\nHalf-normal\n~scale(shrub)\n0.035\n0.331\n0.034\n17.901\n\n\n\nHalf-normal\n~scale(height)\n0.037\n0.333\n0.034\n19.168\n\n\n\n\n\nNone of the covariates contribute to fit of the detection function models for thrashers. Inference should be based upon the no covariate model (that passes the goodness of fit test), but for testing purposes, we will use the model with the observer covariate. Use of the observer covariate model will retain between-point variability in effective area; useful for testing purposes.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Sage thrasher\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nadd_df_covar_line(woo.o, data.frame(obs=\"obs6\"), col=\"coral\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:6,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\", \"coral\"))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. Division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nThe function has an argument pointflag used to indicate whether point transect sampling was used. If so, the correct effective area calculations are performed.\n\nsitesadj <- effAreafn(woo.o, newthrasher, 10000, dettrunc, pointflag = pointtransect)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nAs for the line transect example, fit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nknitr::kable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-2.3977\n0.8794\n-2.7266\n0.0064\n\n\nshrub\n0.0818\n0.0419\n1.9538\n0.0507"
  },
  {
    "objectID": "Pr9/countmodel-points.html#visualise",
    "href": "Pr9/countmodel-points.html#visualise",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\]\nwhere \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment."
  },
  {
    "objectID": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame thrasherSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Sage thrasher density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(thrasherSiteData$siteID, replace=TRUE, size=length(unique(thrasherSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newthrasher[newthrasher$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, transect=surveytype, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = pointtransect)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and bird density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(thrasherSiteData[ , univarpredictor]),\n                                 max(thrasherSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=24, y=1.5, label= b0label, size=5) +\n  annotate(geom=\"text\", x=24, y=1.4, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Sage thrasher density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.18, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Sage Thrasher density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Sage Thrasher."
  },
  {
    "objectID": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. In the case of the Sage Thrasher, the species appears to exhibit little response to the presence of shrub cover during this study. Note that the range of shrub cover is not extensive (~16% to ~26%). We can make no inference regarding the density:habitat relationship outside this range in shrub cover."
  },
  {
    "objectID": "Pr9/countmodel-lines.html",
    "href": "Pr9/countmodel-lines.html",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\]\nwhere \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\]\nwhen using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#data-organisation",
    "href": "Pr9/countmodel-lines.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Data organisation",
    "text": "Data organisation\nIn Carlisle’s data set, sightings information is kept separate from information about each site. For our purposes, we will merge those together. In addition, some field names are changed for consistency with functions in the Distance package.\n\nnewsparrow <- merge(sparrowDetectionData, sparrowSiteData, by=\"siteID\", all=TRUE)\nnames(newsparrow) <- sub(\"observer\", \"obs\", names(newsparrow))\nnames(newsparrow) <- sub(\"dist\", \"distance\", names(newsparrow))\nnames(newsparrow) <- sub(\"length\", \"Effort\", names(newsparrow))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nAlthough not formally written as a set of functions, we bring to the front of the code arguments the user will need to change to alter to suite their needs. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height, shrubclass. The same predictors, with the exception of observer could be used to model the sparrow counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- FALSE        # survey conducted using lines or points\ndettrunc <- 100  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 500       # number of bootstrap replicates\nset.seed(19191)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#fit-detection-function",
    "href": "Pr9/countmodel-lines.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Brewer’s sparrow data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newsparrow, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.obare <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(bare),\n                transect=surveytype, quiet=TRUE)\nwoo.oht <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.oshrc <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+shrubclass, \n             transect=surveytype, quiet=TRUE)\nwoo.ht <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.shrc <- ds(data=newsparrow, truncation=dettrunc, formula=~shrubclass, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(shrub), \n            transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newsparrow, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo.o,  woo.obare, woo.oht, woo.oshrc,\n                                 woo.ht, woo.shrc, woo.shrub, woo), digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\", row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~shrubclass\n0.511\n0.559\n0.025\n0.000\n\n\n\nHalf-normal\n~scale(height)\n0.492\n0.558\n0.025\n0.325\n\n\n\nHalf-normal\n~scale(shrub)\n0.476\n0.559\n0.025\n1.002\n\n\n\nHalf-normal\n~1\n0.469\n0.563\n0.025\n3.154\n\n\n\nHalf-normal\n~obs + scale(bare)\n0.659\n0.555\n0.025\n5.109\n\n\n\nHalf-normal\n~obs + shrubclass\n0.652\n0.556\n0.025\n5.533\n\n\n\nHalf-normal\n~obs + scale(height)\n0.658\n0.556\n0.025\n5.672\n\n\n\nHalf-normal\n~obs\n0.587\n0.559\n0.025\n7.148\n\n\n\n\n\nAll of the candidate models fit the Brewer’s sparrow line transect data. Also note that the estimate detection probability of all six models is the same to the third decimal. There is a small difference in AIC between the factor covariate shrubclass and the continuous covariate height. Simply for the purposes of demonstration, we will base our inference on the detection function that includes observer as a covariate. There is likely little effect of this model selection choice upon the ecological question of interest.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:5,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\"))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. For line transect example of Brewers Sparrows, Effort was measured in meters, but we wish to produce our estimates of density in numbers per hectare. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. In both cases, division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nsitesadj <- effAreafn(woo.o, newsparrow, 10000, dettrunc, pointflag = FALSE)"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nFit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function. Consequently, the offset must also use the log transform.\nTo generalise the code, and recognising the same call to glm() will need to be made elsewhere in this analysis, we specify the GLM model we wish to fit as an object of type formula. This was specified in the Analysis parameters specification section above.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nkable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-1.0565\n0.1397\n-7.5649\n0\n\n\nshrub\n0.0789\n0.0096\n8.1836\n0"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#visualise",
    "href": "Pr9/countmodel-lines.html#visualise",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\]\nwhere \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment.\nWe plot the estimated density against the continuous univariate predictor.\n\nsitesadj$density <- sitesadj$myCount / sitesadj$effArea \n# plot(sitesadj[, univarpredictor], sitesadj$density, pch=20,\n# firstplot <- recordPlot()"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame sparrowSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Brewer’s sparrow density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(sparrowSiteData$siteID, replace=TRUE, size=length(unique(sparrowSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newsparrow[newsparrow$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = FALSE)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and Brewer’s sparrow density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(sparrowSiteData[ , univarpredictor]),\n                                 max(sparrowSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=20, y=0.6, label= b0label, size=5) +\n  annotate(geom=\"text\", x=20, y=0.3, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Brewer's sparrow density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=.025, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.13, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Brewer’s sparrow density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Brewer’s sparrow."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. For the habitat characteristic shrub, there is a positive response of Brewer’s sparrow to increasing shrub cover.\n\nxlabel <- paste(\"Estimated slope of\", univarpredictor, \"and count relationship.\")\nhist.slope <- hist(slope.est, main=\"Sampling distribution of slope parameter\",\n                   xlab=xlabel) \ncibounds <- quantile(slope.est, probs = c(.025,.975), na.rm=TRUE)\nabline(v=cibounds, lty=3) \ntext(cibounds, max(hist.slope$counts), round(cibounds,3))\n\n\n\n\nSampling distribution of the parameter of interest for Brewer’s sparrow."
  },
  {
    "objectID": "Pr9/multi-analysis.html",
    "href": "Pr9/multi-analysis.html",
    "title": "Multispecies and multisession distance sampling analysis",
    "section": "",
    "text": "Survey design\nBuckland’s design consisted of visiting each of the 19 transects in his study twice. To examine some of the errors that can arise from improper analysis, I choose to treat the two visits as strata for the express purpose of generating stratum (visit) -specific density estimates. Density estimates reported in Buckland (2006) are in units of birds \\(\\cdot hectare^{-1}\\).\n\nbirds$Region.Label <- birds$visit\ncu <- convert_units(\"meter\", \"kilometer\", \"hectare\")\n\n\n\nAnalysis of only one species (incorrectly)\nThe direct approach to producing a density estimate for the chaffinch would be to subset the original data frame and use the species-specific data frame for analysis. Begin by performing the subset operation.\n\nchaf <- birds[birds$species==\"c\", ]\n\nWhen the data are subset, the integrity of the survey design is not preserved. A simple frequency table of the species-specific data frame flags up a number of transect/visit combinations where no chaffinches were detected. The result is that the subset data frame suggests 3 of the 19 transects lacked chaffinch detections on the first visit and one of the 19 transects lacked chaffinch detections on the second visit. This revelation, in itself, causes no problems for our estimate of density of chaffinches.\n\ndetects <- table(chaf$Sample.Label, chaf$visit)\ndetects <- as.data.frame(detects)\nnames(detects) <- c(\"Transect\", \"Visit\", \"Detections\")\ndetects$Detections <- cell_spec(detects$Detections, \n                          background = ifelse(detects$Detections==0, \"red\", \"white\"))\nknitr::kable(detects, escape=FALSE) %>%\n  kable_paper(full_width=FALSE)\n\n\n\n \n  \n    Transect \n    Visit \n    Detections \n  \n \n\n  \n    1 \n    1 \n    3 \n  \n  \n    2 \n    1 \n    3 \n  \n  \n    3 \n    1 \n    4 \n  \n  \n    4 \n    1 \n    3 \n  \n  \n    5 \n    1 \n    5 \n  \n  \n    6 \n    1 \n    4 \n  \n  \n    7 \n    1 \n    2 \n  \n  \n    8 \n    1 \n    0 \n  \n  \n    9 \n    1 \n    1 \n  \n  \n    10 \n    1 \n    1 \n  \n  \n    11 \n    1 \n    0 \n  \n  \n    13 \n    1 \n    1 \n  \n  \n    14 \n    1 \n    1 \n  \n  \n    15 \n    1 \n    3 \n  \n  \n    16 \n    1 \n    2 \n  \n  \n    17 \n    1 \n    3 \n  \n  \n    18 \n    1 \n    3 \n  \n  \n    19 \n    1 \n    0 \n  \n  \n    1 \n    2 \n    1 \n  \n  \n    2 \n    2 \n    4 \n  \n  \n    3 \n    2 \n    3 \n  \n  \n    4 \n    2 \n    2 \n  \n  \n    5 \n    2 \n    4 \n  \n  \n    6 \n    2 \n    3 \n  \n  \n    7 \n    2 \n    3 \n  \n  \n    8 \n    2 \n    1 \n  \n  \n    9 \n    2 \n    0 \n  \n  \n    10 \n    2 \n    2 \n  \n  \n    11 \n    2 \n    1 \n  \n  \n    13 \n    2 \n    1 \n  \n  \n    14 \n    2 \n    1 \n  \n  \n    15 \n    2 \n    1 \n  \n  \n    16 \n    2 \n    1 \n  \n  \n    17 \n    2 \n    1 \n  \n  \n    18 \n    2 \n    4 \n  \n  \n    19 \n    2 \n    1 \n  \n\n\n\n\n\nHowever, there is a problem hidden within the table above. Transect 12 does not appear in the table because there were no detections of chaffinches on either visit. Consequently, there were 4 transects without chaffinches on the first visit and 2 transects without chaffinches on the second visit, rather than the 3 transects and 1 transect you might mistakenly conclude do not have chaffinch detections if you relied completely upon the table.\nLet’s see what the ds() function thinks about the survey effort using information from the species-specific data frame.\n\nchaf.wrong <- ds(chaf, key=\"hn\", convert_units = cu, truncation=95, formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(chaf.wrong$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    82.061 \n    4.319 \n    39 \n    15 \n    9.029868 \n    1.1159303 \n    0.1235821 \n  \n  \n    2 \n    33.2 \n    83.562 \n    4.398 \n    34 \n    17 \n    7.730787 \n    0.9798153 \n    0.1267420 \n  \n  \n    Total \n    66.4 \n    165.623 \n    8.717 \n    73 \n    32 \n    8.380327 \n    0.7425191 \n    0.0886026 \n  \n\n\n\n\n\nExamine the column labelled k (the number of transects) for each of the visits. Rather than the 19 transects that were surveyed on each visit, the ds() function erroneously believes there were only 15 transects surveyed on the first visit and 17 transects surveyed on the second visit.\nNote also the number of detections per kilometer; roughly 9 on the first visit and 7.7 on the second visit. These encounter rates exclude kilometers of effort on transects where there were no detections. We will return to this comparison later.\n\n\nUse explicit data hierarchy\n\n\n\n\n\n\nDescribing the survey design to ds\n\n\n\nAdditional arguments can be passed to ds() to resolve this problem. Consulting the ds() documentation\n\nregion_table data.frame with two columns:\n\nRegion.Label label for the region\nArea area of the region\nregion_table has one row for each stratum. If there is no stratification then region_table has one entry with Area corresponding to the total survey area. If Area is omitted density estimates only are produced.\n\nsample_table data.frame mapping the regions to the samples (i.e. transects). There are three columns:\n\nSample.Label label for the sample\nRegion.Label label for the region that the sample belongs to.\nEffort the effort expended in that sample (e.g. transect length).\n\n\n\n\nThis analysis that produces erroneous results can be remedied by explicitly letting the ds() function know about the study design; specifically, how many strata and the number of transects within each stratum (and associated transect lengths).\nConstruct the region table and sample table showing the two strata with equal areas and each labelled transect (of given length) is repeated two times.\n\nbirds.regiontable <- data.frame(Region.Label=as.factor(c(1,2)), Area=c(33.2,33.2))\nbirds.sampletable <- data.frame(Region.Label=as.factor(rep(c(1,2), each=19)),\n                                Sample.Label=rep(1:19, times=2),\n                                Effort=c(0.208, 0.401, 0.401, 0.299, 0.350,\n                                         0.401, 0.393, 0.405, 0.385, 0.204,\n                                         0.039, 0.047, 0.204, 0.271, 0.236,\n                                         0.189, 0.177, 0.200, 0.020))\n\n\n\nSimple detection function model\nThe chaffinch analysis is performed again, this time supplying the region_table and sample_table information to ds(). The correct number of transects (19) sampled on both visits (even though chaffinch was not detected on 4 transects on visit 1 and 2 transects on visit 2) is now recognised. Hence, the use of region table and sample table solves the problem of effort miscalculation if a species is not detected on all transects.\n\ntr <- 95   # as per Buckland (2006)\nonlycf <- ds(data=birds[birds$species==\"c\", ], \n             region_table = birds.regiontable,\n             sample_table = birds.sampletable,\n             trunc=tr, convert_units=cu, key=\"hn\", formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(onlycf$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    91.77 \n    4.83 \n    39 \n    19 \n    8.074534 \n    1.2196305 \n    0.1510465 \n  \n  \n    2 \n    33.2 \n    91.77 \n    4.83 \n    34 \n    19 \n    7.039338 \n    1.0612781 \n    0.1507639 \n  \n  \n    Total \n    66.4 \n    183.54 \n    9.66 \n    73 \n    38 \n    7.556936 \n    0.8083641 \n    0.1069698 \n  \n\n\n\n\n\n\n\nConsequence of incorrect analysis\nTo drive home the consequence of failing to properly specify the survey effort, contrast the encounter rate for the two visits from the incorrect calculations above (9.0 and 7.7 respectively), with the correct calculation (8.1 and 7.0 respectively). The number of transects is incorrect with the knock-on effect of effort being incorrect. If effort is incorrect then so too is covered area.\nThe ripple effect from incomplete information about the survey design results in positively biased estimates of density.\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2"
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "It is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\nPhoto by Jamshaid Mughal on Unsplash"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html",
    "href": "Pr9/Pr9-instructions.html",
    "title": "Analyses using multipliers",
    "section": "",
    "text": "We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets 2 and 3 deal with instantaneous cues and so only cue rate needs to be taken into account."
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#getting-started",
    "href": "Pr9/Pr9-instructions.html#getting-started",
    "title": "Analyses using multipliers",
    "section": "Getting started",
    "text": "Getting started\nThese data (called sikadeer) are available in the Distance package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?\n\nlibrary(Distance)\ndata(sikadeer)\n# Work out conversion units\nconversion.factor <- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "href": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "title": "Analyses using multipliers",
    "section": "Fit detection function to dung pellets",
    "text": "Fit detection function to dung pellets\nFit the usual series of models (i.e. half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don’t spend too long on this). Call your model deer.df. This detection function will be used to obtain \\(\\hat D_{\\textrm{pellet groups}}\\).\nHave a look at the Summary statistics for this model - what do you notice about the allocation of search effort in each woodland?"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#multipliers",
    "href": "Pr9/Pr9-instructions.html#multipliers",
    "title": "Analyses using multipliers",
    "section": "Multipliers",
    "text": "Multipliers\nThe next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.\nData to calculate this has been collected in the file IntroDS_9.1.csv in your directory. Following code comes from Meredith (2017).\n\nMIKE.persistence <- function(DATA) {\n  \n#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data \n#  Input: data frame with at least two columns:\n#         DAYS - calendar day on which dung status was observed\n#         STATE - dung status: 1-intact, 0-decayed\n#  Output: point estimate, standard error and CV of mean persistence time\n#\n#  Attribution: code from Mike Meredith website: \n#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm\n#   Citing: CITES elephant protocol\n#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf\n  \n  ##   Fit logistic regression model to STATE on DAYS, extract coefficients\n  dung.glm <- glm(STATE ~ DAYS, data=DATA, family=binomial(link = \"logit\"))\n  betas <- coefficients(dung.glm)\n  ##   Calculate mean persistence time\n  mean.decay <- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]\n  ## Calculate the variance of the estimate\n  vcovar <- vcov(dung.glm)\n  var0 <- vcovar[1,1]  # variance of beta0\n  var1 <- vcovar[2,2]  # variance of beta1\n  covar <- vcovar[2,1] # covariance\n  deriv0 <- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]\n  deriv1 <- -mean.decay/betas[2]\n  var.mean <- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2\n  ## Calculate the SE and CV and return\n  se.mean <- sqrt(var.mean)\n  cv.mean <- se.mean/mean.decay\n  out <- c(mean.decay, se.mean, 100*cv.mean)\n  names(out) <- c(\"Mean persistence time\", \"SE\", \"%CV\")\n  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab=\"Days since initiation\",\n       ylab=\"Dung persists (yes=1)\",\n       main=\"Eight dung piles revisited over time\")\n  curve(predict(dung.glm, data.frame(DAYS=x), type=\"resp\"), add=TRUE)\n  abline(v=mean.decay, lwd=2, lty=3)\n  return(out)\n}\ndecay <- read.csv(\"IntroDS_9.1.csv\")\npersistence.time <- MIKE.persistence(decay)\nprint(persistence.time)\n\nRunning the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been ‘jittered’ to avoid over-plotting.\nAn estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.\nAs stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:\n\ncreation contains estimates of the dung production rate and associated standard error\ndecay contains the dung decay rate and associated standard error where XX and YY are the estimates you obtained from the dung decay rate analysis.\n\n\n# Create list of multipliers\nmult <- list(creation = data.frame(rate=25, SE=0),\n#             decay    = data.frame(rate=XX, SE=YY))\nprint(mult)\n\nThe final step is to use these multipliers to convert \\(\\hat D_{\\textrm{pellet groups}}\\) to \\(\\hat D_{\\textrm{deer}}\\) (as in the equations above) - for this we need to employ the dht2 function. In the command below the multipliers= argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:\n\nstrat_formula=~Region.Label is specified to take into account the design (i.e. different woodlands or blocks).\nstratification=\"effort_sum\" is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.\ndeer.df is the detection function you have fitted.\n\n\n# Weight by effort because we have repeats\ndeer.ests <- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                 convert_units=conversion.factor, multipliers=mult, \n                 stratification=\"effort_sum\", total_area=13.9)\nprint(deer.ests)\n\nThe function dht2 also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html",
    "href": "Pr8/Group-size-covariate.html",
    "title": "Size bias—how large is the problem?",
    "section": "",
    "text": "Supplement\n\n\n\nCorrecting size bias via size as a covariate. When does it matter?"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis including group size covariate",
    "text": "Analysis including group size covariate\n\nrunsim.cov <- run.simulation(size.cov, run.parallel = TRUE)\n\n\n\n\n\n\nThe distribution of computed average group size centred on the true size of 10 and there was no problem with fitting a detection function. The average over the simulations estimated number of individuals was 2272.47."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without group size covariate",
    "text": "Analysis without group size covariate\nAs a comparison, what happens if we don’t include size as a covariate in our detection function?\n\n\n\n\n\nThe distribution of computed average groups sizes is shown above. We would expect an overestimate of mean group size because small groups at large distances are missing from our sample; but that effect is small in this instance. As a consequence, the average \\(\\hat{N}_{indiv}\\) across all simulations is 2304.95."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis with covariate",
    "text": "Analysis with covariate\n\n\n\n\n\nWhen including size as a covariate, estimates of average group size are not affected (figure above). Likewise, mean \\(\\hat{N}_{indiv}\\) is effectively unbiased: 5453.55."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without the covariate",
    "text": "Analysis without the covariate\n\n\n\n\n\nNow mean \\(\\hat{N}_{indiv}\\) is considerably biased: 5829.77, 32.9 percent larger than the true number of individuals in the population, 4388."
  },
  {
    "objectID": "Pr6/effort-and-power.html",
    "href": "Pr6/effort-and-power.html",
    "title": "Effort and power calculations for line transect surveys",
    "section": "",
    "text": "Demonstration\n\n\n\nEffort needed to achieve objective"
  },
  {
    "objectID": "Pr6/effort-and-power.html#cv-graph",
    "href": "Pr6/effort-and-power.html#cv-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "CV graph",
    "text": "CV graph\nThe result tab showing the relationship between cumulative population change and necessary CV is shown in the CV graph tab. There is a positive relationship between these: greater population loss requires less precision to detect; or small population change requires more precise density estimates. The red dot and horizontal dotted line indicate the precision of annual estimates to detect the specified cumulative change over the specified number of annual surveys with the desired power. For these calculations, the encounter rate from the pilot survey plays no role."
  },
  {
    "objectID": "Pr6/effort-and-power.html#effort-graph",
    "href": "Pr6/effort-and-power.html#effort-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Effort graph",
    "text": "Effort graph\nThe result tab labelled Effort graph brings information from the pilot survey into the calculations. The CV graph indicates the necessary precision to achieve the desired results, this CV is fed into the formula from Buckland et al. (2015) to estimate the amount of survey effort to be expended annually to achieve the precision derived from the power calculations.\nThe red ball and horizontal dotted line now indicates the amount of effort needed to achieve the specified objectives. Small changes in abundance require exponentially larger amounts of annual survey effort to detect. The steepness of that exponential curve is less extreme when encounter rates are large."
  },
  {
    "objectID": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "href": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Numerical result of power calculation",
    "text": "Numerical result of power calculation\nThe final result tab simply provides a single numerical solution to the required CV and effort necessary to achieve that CV for the specified combination of cumulative change, number of annual surveys, power and pilot study encounter rate."
  },
  {
    "objectID": "Pr5/points.html",
    "href": "Pr5/points.html",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\n\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method.\n\nPhoto by Vincent van Zalinge on Unsplash"
  },
  {
    "objectID": "extras/critique.html",
    "href": "extras/critique.html",
    "title": "Literature critique of distance sampling papers",
    "section": "",
    "text": "In addition to understanding how to conduct distance sampling surveys that are defensible and robust, I want you also to detect reinforcement and threats to robust inference in published distance sampling studies. I provide you with examples of three recently published distance sampling studies. I ask that you select one of these papers and invest 30 minutes in scanning the methods and results sections, understanding what they have done and asking yourself if there is anything you would disagree with in their approach or conclusions. Do they provide evidence to support the conclusions they present?\nI’ve found a few papers representing three taxonomic groups: terrestrial mammals, terrestrial birds and marine mammals. Select a paper of interest to you and create a list of positive and negative aspects of the way in which the design and analysis are reported. Do you feel the results are defensible in light of the methodology employed and described.\n\nBuuveibaatar, B., Strindberg, S., Kaczensky, P., Payne, J., Chimeddorj, B., Naranbaatar, G., Amarsaikhan, S., Dashnyam, B., Munkhzul, T., Purevsuren, T., Hosack, D. A., & Fuller, T. K. (2017). Mongolian Gobi supports the world’s largest populations of khulan Equus hemionus and goitered gazelles Gazella subgutturosa. Oryx 51(4):639–647. https://doi.org/10.1017/S0030605316000417\nZelelew, S. A., Bekele, A., & Archibald, G. (2020). Detection function, cluster size, density, and population size of Black Crowned Crane Balearica pavonina ceciliae in the upper Blue Nile River, Lake Tana area. Scientific African 10:e00557. https://doi.org/10.1016/j.sciaf.2020.e00557\nStrindberg, S., Ersts, P. J., Collins, T., Sounguet, G.-P., & Rosenbaum, H. C. (2020). Line transect estimates of humpback whale abundance and distribution on their wintering grounds in the coastal waters of Gabon. J. Cetacean Res. Manage. 153–160. https://doi.org/10.47536/jcrm.vi3.324"
  },
  {
    "objectID": "extras/mystery.html",
    "href": "extras/mystery.html",
    "title": "Your skills as a distance sampling analyst",
    "section": "",
    "text": "If you wish to perform a distance sampling analysis upon a data set, I have one for you to download. It is a bit of a scrappy data set, not very many line transects (12), not very many detections (43). As such, it might present a couple of challenges to you, but hopefully not too many. 🕵\n\nThere was one covariate recorded ✍ with each detection–whether the animal was male or female .\nUnits of measure 📏 are: perpendicular distances in meters, effort (transect lengths) in kilometers and area of study region in square kilometers.\n\nDon’t forget to set the units properly\n\nlibrary(Distance)\nconversion <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\nRemember some of the hints I have provided (in decision sequence above) along with decisions you will need to make regarding truncation.\nDo a competent job with your analysis and we will share our findings (preferred model, point estimate of abundance and precision of abundance estimate) on Thursday during the practical session.\nThe data set (in CSV format) is available in the file space of this project, shown in the File panel, named mystery.csv.\nThis will be your first adventure looking at data that is not contained within the Distance package. Therefore, your first analysis task will be to read the data from the .csv file into R for subsequent analysis. Example code below:\n\nmydata <- read.csv(\"mystery.csv\")\n\nWhat happens after this is up to you. 🥴 🤔\n\n\n\n\nworkflow"
  },
  {
    "objectID": "extras/mystery.html#section",
    "href": "extras/mystery.html#section",
    "title": "Your skills as a distance sampling analyst 🕵",
    "section": "🥴 🤔",
    "text": "🥴 🤔\nknitr::include_graphics(“new-analysis-hints.svg”)"
  },
  {
    "objectID": "extras/extras.html",
    "href": "extras/extras.html",
    "title": "Development of additional skills",
    "section": "",
    "text": "The practicals you have seen thus far have been targeted on specific pedagogical tasks: fitting detection functions, assessing precision, designing surveys, etc. The exercises presented as “extras” take a broader view.\nEven if you never conduct a distance sampling analysis, you will have the opportunity to “consume” the results of distance sampling surveys. In that consumption, you will have to determine the credibility of published findings. Given your skills understanding how to design surveys and analyse data from those surveys, you can apply those skills in assessing the defensibility of work presented by others. This is the intent of the literature critique exercise. Examine one of the three published papers as a critical consumer and make determinations about the credibility of the findings.\nA “mystery” data set is provided to take an analysis from start to finish. With this data set, you will make the series of decisions associated with distance sampling analysis. More importantly, you should emphasize the evidence used to support the decisions made."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling lab book",
    "section": "",
    "text": "Material contained herein will acquaint you with design, collection and analysis of distance sampling data.\nThe materials are intended to be experiential rather than used as a reference. Exercises are paced to bring you up to speed with the fundamentals of distance sampling; how this method of population assessment differs from other forms of population sampling. This leads to the introduction of a detection function and ways to model it.\nFurther topics of model selection, assessing precision of population estimates and collection and analysis of point transect data round out the first half of the materials. At this point, you will have acquired sufficient experience to analyse basic distance sampling data.\nThe second half of the material exposes you to slightly more advanced concepts: design of distance sampling surveys, including the use of stratification. Also discussed are analytical methods associated with stratified surveys. This gives way to including predictors other than distance in modelling the detection process. Finally multipliers and methods associated with indirect animal surveys are introduced.\nAs well as the exercises and their solutions, there are numerous supplements, touching upon topics or demonstrating issues related to the analysis of distance sampling data. The supplements are not fundamental to successfully employing distance sampling methods, however, the more you understand tools such as distance sampling (via these supplements), the better you will be able to employ such methods."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html",
    "href": "Pr7/stratumspecific-bias.html",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nDistance sampling simulation where detection functions differ between strata. When stratum-specific abundance estimates are produced using a pooled detection function, bias arises. The magnitude of the bias depends upon the magnitude of the difference in the detection functions."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "href": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "North Sea study area",
    "text": "North Sea study area\nInterest is in estimating the density of minke whales in the western portion of the North Sea, off the east coast of Britain. The study area is divided into north and south strata, with the north stratum being roughly 1.9 times the size of the south stratum, as shown in the map below.\n\nm <- leaflet() %>% addProviderTiles(providers$Esri.OceanBasemap)\nm <- m %>% \n  setView(1.4, 55.5, zoom=5)\nminkes <- read_sf(myshapefilelocation)\nstudy.area.trans <- st_transform(minkes, '+proj=longlat +datum=WGS84')\nm <- addPolygons(m, data=study.area.trans$geometry, weight=2)\nm"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "href": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Properties of the design",
    "text": "Properties of the design\nTo demonstrate the estimated number of transects in each stratum, the run.coverage function is used to show the number of replicates in each stratum is allocated roughly according to stratum size.\n\ndesign.properties <- run.coverage(equal.cover, reps = 10, quiet=TRUE)\nmine <- data.frame(Num.transects=design.properties@design.statistics$sampler.count[3,],\n                   Proportion.covered=design.properties@design.statistics$p.cov.area[3,])\nkable(mine)\n\n\n\n\n\nNum.transects\nProportion.covered\n\n\n\n\nSouth\n17\n8.16\n\n\nNorth\n23\n8.11\n\n\nTotal\n40\n8.12"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "href": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata",
    "text": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata\n\ndelta.multiplier <- c(seq(from=0.5, to=1.1, by=0.1),\n#                      seq(from=0.85, to=1.15, by=0.1),\n                      seq(from=1.2, to=2.4, by=0.2))\nsigma.south <- 0.3\nnorth.sigma <- sigma.south*delta.multiplier\n\nScale parameter (\\(\\sigma\\)) for the southern stratum remains fixed at 0.3, but in the northern stratum, the scale parameter is a multiple of the southern stratum \\(\\sigma\\), ranging from a low of 0.15 to a maximum of 0.72.\n\nhn <- function(sigma, x) {return(exp(-x^2/(2*sigma^2)))}\nfor (i in seq_along(north.sigma)) {\n  curve(hn(north.sigma[i],x),from=0,to=0.8,add=i!=1,  \n        xlab=\"Distance\", ylab=\"Detection probability\", \n        main=\"Range of detection probability disparity\\nSouth function in blue\")\n}\ncurve(hn(sigma.south,x),from=0,to=0.8, lwd=2, col='blue', add=TRUE)\n\n\n\n\n\nequalcover <- list()\nwhichmodel <- list()\nnum.sims <- 10\nfor (i in seq_along(delta.multiplier)) {\n  sigma.strata <- c(sigma.south, sigma.south*delta.multiplier[i])\n  detect <- make.detectability(key.function = \"hn\",\n                               scale.param = sigma.strata,\n                               truncation = 0.8)\n  equalcover.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = pooled.hn)\n  whichmodel.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = strat.specific.or.not)\n  equalcover[[i]] <- run.simulation(equalcover.sim, run.parallel = TRUE, max.cores=10)\n  whichmodel[[i]] <- run.simulation(whichmodel.sim, run.parallel = TRUE, max.cores=10)\n}"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "href": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Conclusions from this portion of study",
    "text": "Conclusions from this portion of study\nNote bias in the estimated density for the entire study area is never greater than 10%, yet another demonstration of pooling robustness. Even with widely differing detection functions, the estimated density ignoring stratum-specific differences is essentially unbiased.\n\n\n\n\n\n\n\n\nConfidence interval coverage for stratum-specific estimates approaches nominal levels when \\(\\Delta \\approx 1\\). Coverage for the density estimate in the entire study area is nominal for all values of \\(\\Delta\\) with the exception of \\(\\Delta<0.7\\)."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "href": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Model selection sensitivity",
    "text": "Model selection sensitivity\nThis small simulation demonstrates the peril of making stratum-specific estimates when using a detection function that does not recognise stratum-specific detection function differences. This situation can arise when numbers of stratum-specific detections are too small to support stratum-specific detection functions. This set of simulations was devised such that there was sufficient effort in each stratum to avoid small numbers of detections. Even so, use of the “wrong” (pooled) detection function leads to considerable bias in density estimates.\n\nplot(delta.multiplier, modelsel, \n     main=\"Stratum-specific model chosen\", type=\"b\", pch=20,\n     xlab=expression(Delta), ylab=\"Stratum covariate chosen\")\nabline(h=0.50)\n\n\n\n\nThere are two messages from this model selection assessment. Only when \\(\\Delta < 0.8\\) or \\(\\Delta > 1.2\\) is there a better than even chance AIC will detect the difference in detectability between strata. Values of \\(\\Delta\\) in this region do not lead to extreme bias in stratum-specific density estimates when the pooled detection function model is used. There is roughly a 10% negative bias in density estimates of the north stratum and a 5% positive bias in density estimates of the southern stratum."
  },
  {
    "objectID": "Pr1/Prac1_solution.html",
    "href": "Pr1/Prac1_solution.html",
    "title": "Line transect detection function fitting solution",
    "section": "",
    "text": "Solution\n\n\n\nEstimation of duck nest density by hand\n\n\nIn this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nFrequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g. counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]"
  },
  {
    "objectID": "Pr2/Prac2_solution.html",
    "href": "Pr2/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks <- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor <- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn <- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel : Half-normal key function \nAIC   : 928.1338 \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos <- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm <- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\npar(mfrow=c(1,3))\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "Pr3/Prac3_solution.html",
    "href": "Pr3/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions"
  },
  {
    "objectID": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos <- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos <- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos <- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and qq plots are shown below:\n\n# Divide plot window\npar(mfrow=c(3,2))\npar(mar=c(4,4,.2,.1))\nplot(caper.hn.cos, main=\"Half normal\")\ngof_ds(caper.hn.cos)\nplot(caper.hr.cos, main=\"Hazard rate\")\ngof_ds(caper.hr.cos)\nplot(caper.uf.cos, main=\"Uniform\")\ngof_ds(caper.uf.cos)\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.613\n0.682\n0.098\n0.280\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.154\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results."
  },
  {
    "objectID": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‘correct’ perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, …, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins <- c(0, seq(from=7.5, to=67.5, by=10), 80)\n# Check bins\nbins\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\n# Specify model with binned distances\ncaper.hn.bin <- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\n# Plot\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n# Summarise results\ncaper.hn.bin$dht$individuals$summary\n\n            Region Area CoveredArea Effort   n k        ER se.ER cv.ER\n1 Monaughty Forest 1472        3840    240 112 1 0.4666667     0     0\n  mean.size se.mean\n1         1       0\n\ncaper.hn.bin$dht$individuals$D[1:6]\n\n  Label   Estimate          se        cv        lcl        ucl\n1 Total 0.04531495 0.006899599 0.1522588 0.02587611 0.07935679\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data."
  },
  {
    "objectID": "Pr4/Prac4_solution.html",
    "href": "Pr4/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimatino for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor <- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn <- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nsysvar2.hn$dht$individuals$D\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nsysvar2.hn$dht$individuals$N\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot <- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n     median   mean     se    lcl     ucl   cv\nNhat 924.05 982.86 283.65 520.99 1590.85 0.31\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\n## Post-stratification by O2 estimator\n# ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_2$Sample.Label <- as.numeric(Systematic_variance_2$Sample.Label)\n# Using the Fewster et al 2009, \"O2\" estimator \nest.O2 <- dht2(sysvar2.hn, flatfile=Systematic_variance_2,   strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009) .\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label <- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn <- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nsysvar1.hn$dht$individuals$D\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nsysvar1.hn$dht$individuals$N\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\n# Now use Fewster et al 2009, \"O2\" estimator \nest2.O2 <- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "Pr5/Prac5_solution.html",
    "href": "Pr5/Prac5_solution.html",
    "title": "Exercise 5 Point transect sampling solution",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise"
  },
  {
    "objectID": "Pr5/Prac5_solution.html#truncation-of-20m",
    "href": "Pr5/Prac5_solution.html#truncation-of-20m",
    "title": "Exercise 5 Point transect sampling solution",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m <- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m <- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m <- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648"
  },
  {
    "objectID": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Exercise 5 Point transect sampling solution",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\n\n\n\n\npar(mfrow=c(2,2))\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates."
  },
  {
    "objectID": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Exercise 5 Point transect sampling solution",
    "section": "Probability density functions for Buckland’s winter wren point transects",
    "text": "Probability density functions for Buckland’s winter wren point transects\n\n# Plot detection functions\npar(mfrow=c(1,2))\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data."
  }
]