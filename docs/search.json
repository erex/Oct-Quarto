[
  {
    "objectID": "Pr4/Prac4_solution.html",
    "href": "Pr4/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimation for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor <- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn <- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot <- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n      median    mean    se    lcl     ucl   cv\nNhat 1021.45 1031.38 282.4 544.14 1607.52 0.28\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label <- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 <- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label <- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn <- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 <- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html",
    "href": "Pr7/stratumspecific-bias.html",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nDistance sampling simulation where detection functions differ between strata. When stratum-specific abundance estimates are produced using a pooled detection function, bias arises. The magnitude of the bias depends upon the magnitude of the difference in the detection functions."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "href": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "North Sea study area",
    "text": "North Sea study area\nInterest is in estimating the density of minke whales in the western portion of the North Sea, off the east coast of Britain. The study area is divided into north and south strata, with the north stratum being roughly 1.9 times the size of the south stratum, as shown in the map below.\n\nm <- leaflet() %>% addProviderTiles(providers$Esri.OceanBasemap)\nm <- m %>% \n  setView(1.4, 55.5, zoom=5)\nminkes <- read_sf(myshapefilelocation)\nstudy.area.trans <- st_transform(minkes, '+proj=longlat +datum=WGS84')\nm <- addPolygons(m, data=study.area.trans$geometry, weight=2)\nm"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "href": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Properties of the design",
    "text": "Properties of the design\nTo demonstrate the estimated number of transects in each stratum, the run.coverage function is used to show the number of replicates in each stratum is allocated roughly according to stratum size.\n\ndesign.properties <- run.coverage(equal.cover, reps = 10, quiet=TRUE)\nmine <- data.frame(Num.transects=design.properties@design.statistics$sampler.count[3,],\n                   Proportion.covered=design.properties@design.statistics$p.cov.area[3,])\nkable(mine)\n\n\n\n\n\nNum.transects\nProportion.covered\n\n\n\n\nSouth\n17\n8.15\n\n\nNorth\n23\n8.13\n\n\nTotal\n40\n8.13"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "href": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata",
    "text": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata\n\ndelta.multiplier <- c(seq(from=0.5, to=1.1, by=0.1),\n#                      seq(from=0.85, to=1.15, by=0.1),\n                      seq(from=1.2, to=2.4, by=0.2))\nsigma.south <- 0.3\nnorth.sigma <- sigma.south*delta.multiplier\n\nScale parameter (\\(\\sigma\\)) for the southern stratum remains fixed at 0.3, but in the northern stratum, the scale parameter is a multiple of the southern stratum \\(\\sigma\\), ranging from a low of 0.15 to a maximum of 0.72.\n\nhn <- function(sigma, x) {return(exp(-x^2/(2*sigma^2)))}\nfor (i in seq_along(north.sigma)) {\n  curve(hn(north.sigma[i],x),from=0,to=0.8,add=i!=1,  \n        xlab=\"Distance\", ylab=\"Detection probability\", \n        main=\"Range of detection probability disparity\\nSouth function in blue\")\n}\ncurve(hn(sigma.south,x),from=0,to=0.8, lwd=2, col='blue', add=TRUE)\n\n\n\n\n\nequalcover <- list()\nwhichmodel <- list()\nnum.sims <- 10\nfor (i in seq_along(delta.multiplier)) {\n  sigma.strata <- c(sigma.south, sigma.south*delta.multiplier[i])\n  detect <- make.detectability(key.function = \"hn\",\n                               scale.param = sigma.strata,\n                               truncation = 0.8)\n  equalcover.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = pooled.hn)\n  whichmodel.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = strat.specific.or.not)\n  equalcover[[i]] <- run.simulation(equalcover.sim, run.parallel = TRUE, max.cores=10)\n  whichmodel[[i]] <- run.simulation(whichmodel.sim, run.parallel = TRUE, max.cores=10)\n}"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "href": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Conclusions from this portion of study",
    "text": "Conclusions from this portion of study\nNote bias in the estimated density for the entire study area is never greater than 10%, yet another demonstration of pooling robustness. Even with widely differing detection functions, the estimated density ignoring stratum-specific differences is essentially unbiased.\n\n\n\n\n\n\n\n\nConfidence interval coverage for stratum-specific estimates approaches nominal levels when \\(\\Delta \\approx 1\\). Coverage for the density estimate in the entire study area is nominal for all values of \\(\\Delta\\) with the exception of \\(\\Delta<0.7\\)."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "href": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Model selection sensitivity",
    "text": "Model selection sensitivity\nThis small simulation demonstrates the peril of making stratum-specific estimates when using a detection function that does not recognise stratum-specific detection function differences. This situation can arise when numbers of stratum-specific detections are too small to support stratum-specific detection functions. This set of simulations was devised such that there was sufficient effort in each stratum to avoid small numbers of detections. Even so, use of the “wrong” (pooled) detection function leads to considerable bias in density estimates.\n\nplot(delta.multiplier, modelsel, \n     main=\"Stratum-specific model chosen\", type=\"b\", pch=20,\n     xlab=expression(Delta), ylab=\"Stratum covariate chosen\")\nabline(h=0.50)\n\n\n\n\nThere are two messages from this model selection assessment. Only when \\(\\Delta < 0.8\\) or \\(\\Delta > 1.2\\) is there a better than even chance AIC will detect the difference in detectability between strata. Values of \\(\\Delta\\) in this region do not lead to extreme bias in stratum-specific density estimates when the pooled detection function model is used. There is roughly a 10% negative bias in density estimates of the north stratum and a 5% positive bias in density estimates of the southern stratum."
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html",
    "href": "Pr8/alternative-selection-metrics.html",
    "title": "Other model selection metrics",
    "section": "",
    "text": "Demonstration\n\n\n\nAlternative model selection metrics"
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#aicc",
    "href": "Pr8/alternative-selection-metrics.html#aicc",
    "title": "Other model selection metrics",
    "section": "AICc",
    "text": "AICc\nDefined as:\n\\[AICc = -2ln(\\mathscr{L}) - 2k + \\frac{2k(k+1)}{n-k-1}\\]\nwhere k is the number of parameters in the model and n is the number of observations. In general, if \\(n\\) is many times larger than \\(k^2\\), then the extra term will be negligible. What do we know, in general, about the magnitude of k and n in typical distance sampling situations? We encourage the collection of 60 to 80 detections (\\(n\\)). A hazard rate key function with a four-level factor covariate has \\(k=5\\) parameters. The value of the the \\(c\\) term added to the regular AIC ranges from 1.11 with \\(n=60\\) to 0.81 with \\(n=80\\).\nFor a two-parameter model (hazard rate or half normal with a single continuous covariate) the value of the additional term would be 0.21 with \\(n=60\\) to 0.16 with \\(n=80\\). Recognise the magnitude of the other terms in the AIC are in the hundreds or thousands (see below)."
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#bic",
    "href": "Pr8/alternative-selection-metrics.html#bic",
    "title": "Other model selection metrics",
    "section": "BIC",
    "text": "BIC\nDefined as:\n\\[BIC = -2ln(\\mathscr{L}) - k \\cdot ln(n)\\]\nThe penalty term changes as a function of sample size; the larger the number of detections, the greater the penalty term. With a 2 parameter model using AIC, the penalty term would be 4 (\\(2 \\times 2\\)). For a model with the the same number of parameters and 80 detections, the penalty term would be 8.8."
  },
  {
    "objectID": "Pr8/covariates.html",
    "href": "Pr8/covariates.html",
    "title": "Covariates practical 8",
    "section": "",
    "text": "It is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata–we examine whether the geographic stratum effect can be modelled as a covariate.\nPhoto by Roman Mager on Unsplash"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html",
    "href": "Pr8/Group-size-covariate.html",
    "title": "Size bias—how large is the problem?",
    "section": "",
    "text": "Supplement\n\n\n\nCorrecting size bias via size as a covariate. When does it matter?"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis including group size covariate",
    "text": "Analysis including group size covariate\n\nrunsim.cov <- run.simulation(size.cov, run.parallel = TRUE)\n\n\n\n\n\n\nThe distribution of computed average group size centred on the true size of 10 and there was no problem with fitting a detection function. The average over the simulations estimated number of individuals was 2272.47."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without group size covariate",
    "text": "Analysis without group size covariate\nAs a comparison, what happens if we don’t include size as a covariate in our detection function?\n\n\n\n\n\nThe distribution of computed average groups sizes is shown above. We would expect an overestimate of mean group size because small groups at large distances are missing from our sample; but that effect is small in this instance. As a consequence, the average \\(\\hat{N}_{indiv}\\) across all simulations is 2304.95."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis with covariate",
    "text": "Analysis with covariate\n\n\n\n\n\nWhen including size as a covariate, estimates of average group size are not affected (figure above). Likewise, mean \\(\\hat{N}_{indiv}\\) is effectively unbiased: 5453.55."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without the covariate",
    "text": "Analysis without the covariate\n\n\n\n\n\nNow mean \\(\\hat{N}_{indiv}\\) is considerably biased: 5829.77, 32.9 percent larger than the true number of individuals in the population, 4388."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html",
    "href": "Pr8/Pr8-instructions.html",
    "title": "Covariates in detection function model",
    "section": "",
    "text": "This exercise consists of three data sets of increasing difficulty. The first problem, MCDS with point transects, is complicated and (using the functionality available in R) also includes some basic exploratory analysis of the covariates. Section 2 and 3 are optional but will take you deeper into the heart of understanding multiple covariates."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "href": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "title": "Covariates in detection function model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIt is important to gain an understanding of the data prior to fitting detection functions (Buckland et al., 2015). With this in mind, preliminary analysis of distance sampling data involves:\n\nassessing the shape of the collected data,\nconsidering the level of truncation of distances, and\nexploring patterns in potential covariates.\n\nWe begin by assessing the distribution of distances to decide on a truncation distance.\n\nhist(amakihi$distance)\n\nTo see if there are differences in the distribution of distances recorded by the different observers and in each hour after sunrise, boxplots can be used. Note how the ~ symbol is used to define the discrete groupings (i.e. observer and hour).\n\n# Boxplots by obs\nboxplot(amakihi$distance~amakihi$OBs, xlab=\"Observer\", ylab=\"Distance (m)\")\n# Boxplots by hour after sunrise\nboxplot(amakihi$distance~amakihi$HAS, xlab=\"Hour\", ylab=\"Distance (m)\")\n\nThe components of the boxplot are:\n\nthe thick black line indicates the median\nthe lower limit of the box is the first quartile (25th percentile) and the upper limit is the third quartile (75th percentile)\nthe height of the box is the interquartile range (75th - 25th quartiles)\nthe whiskers extend to the most extreme points which are no more than 1.5 times the interquartile range.\ndots indicate ‘outliers’ if there are any, i.e. points beyond the range of the whiskers.\n\nFor minutes after sunrise (a continuous variable), we create a scatterplot of MAS (on the \\(x\\)-axis) against distances (on the \\(y\\)-axis). The plotting symbol (or character) is selected with the argument pch:\n\n# Plot of MAS vs distance (using dots)\nplot(x=amakihi$MAS, y=amakihi$distance, xlab=\"Minutes after sunrise\",\n     ylab=\"Distance (m)\", pch=20)\n\nYou may also want to think about potential collinerity (linear relationship) between the covariates - if collinear variables are included in the detection function, they will be explaining some of the same variation in the distances and this will reduce their importance as a potential covariate. How might you investigate the relationship between HAS and MAS?\nFrom these plots can you tell if any of the covariates will be useful in explaining the distribution of distances?"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "href": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "title": "Covariates in detection function model",
    "section": "Adjusting the raw covariates",
    "text": "Adjusting the raw covariates\nWe would like to treat OBs and HAS as factor variables as in the original analysis; OBs is, by default, treated as a factor variable because it consists of characters rather than numbers. HAS, on the other hand, consists of numbers and so by default would be treated as a continuous variable (i.e. non-factor). That is fine if we want the effect of HAS to be monotonic (i.e. detectability either increases or decreases as a function of HAS). If we want HAS to have a non-linear effect on detectability, then we need to indicate to R to treat it as a factor as shown below.\n\n# Convert HAS to a factor\namakihi$HAS <- factor(amakihi$HAS)\n\nThe next adjustment is to change the reference level of the observer and hour factor covariates - the only reason to do this is to get the estimated parameters in the detection function to match the parameters estimated in T. A. Marques et al. (2007). You would not carry out this step on your own data. By default R uses the first factor level but by using the relevel function, this can be changed:\n\n# Set the reference level \namakihi$OBs <- relevel(amakihi$OBs, ref=\"TKP\")\namakihi$HAS <- relevel(amakihi$HAS, ref=\"5\")"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#candidate-models",
    "href": "Pr8/Pr8-instructions.html#candidate-models",
    "title": "Covariates in detection function model",
    "section": "Candidate models",
    "text": "Candidate models\nWith three potential covariates, there are 8 possible models for the detection function:\n\nNo covariates\nOBs\nHAS\nMAS\nOBs + HAS\nOBs + MAS\nHAS + MAS\nOBs + HAS + MAS\n\nEven without considering covariates there are also several possible key function/adjustment term combinations available: if all key function/covariate combinations are considered the number of potential models is large. Note that covariates are not allowed if a uniform key function is chosen and if covariate terms are included, adjustment terms are not allowed. Even with these restrictions, it is not best practice to take a scatter gun approach to detection function model fitting. Buckland et al. (2015) considered 13 combinations of key function/covariates. Here, we look at a subset of these.\nFit a hazard rate model with no covariates or adjustment terms and make a note of the AIC. Note, that 10% of the largest distances are truncated - you may have decided on a different truncation distance.\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr <- ds(amakihi, transect=\"point\", key=\"hr\", truncation=\"10%\",\n              adjustment=NULL, convert_units = conversion.factor)\n\nMake a note of the AIC for this model.\nNow fit a hazard rate model with OBs as a covariate in the detection function and make a note of the AIC. Has the AIC reduced by including a covariate?\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr.obs <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs,\n                  truncation=\"10%\", convert_units = conversion.factor)\n\nFit a hazard rate model with OBs and HAS in the detection function:\n\namak.hr.obs.has <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs+HAS,\n                      truncation=\"10%\", convert_units = conversion.factor)\n\nTry fitting other possible formula and decide which model is best in terms of AIC. To quickly compare AIC values from different models, use the AIC command as follows (note only models with the same truncation distance can be compared):\n\n# AIC values\nAIC(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nAnother useful function is summarize_ds_models - this has the advantage of ordering the models by AIC (smallest to largest).\n\n# Compare models\nsummarize_ds_models(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nOnce you have decided on a model, plot your selected detection function."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#analysis",
    "href": "Pr8/Pr8-instructions.html#analysis",
    "title": "Covariates in detection function model",
    "section": "Analysis",
    "text": "Analysis\nThe data are available in the Distance package:\n\ndata(ETP_Dolphin)\nhead(ETP_Dolphin, n=3)\n\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint - try dividing the histogram of distances into a large number of intervals.)\nAs there are a number of potential covariates to be used in this example (i.e. search method, cue, Beaufort class and month), try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except group size and because they have numeric codes, use the factor function to let R know to treat them as factors.\nNote that both distances and transect lengths were recorded in nautical miles and area in nautical miles squared and so the argument convert_units does not need to be specified.\nKeep in mind that this is a large dataset (> 1000 observations), and hence estimation may take a while. You will likely end up with quite a few models as there are several potential covariates and no ‘right’ answers. Discuss your choice of final model (or models) with your colleagues - did you make the same choices?"
  },
  {
    "objectID": "Pr9/countmodel-lines.html",
    "href": "Pr9/countmodel-lines.html",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\]\nwhere \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\]\nwhen using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#data-organisation",
    "href": "Pr9/countmodel-lines.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Data organisation",
    "text": "Data organisation\nIn Carlisle’s data set, sightings information is kept separate from information about each site. For our purposes, we will merge those together. In addition, some field names are changed for consistency with functions in the Distance package.\n\nnewsparrow <- merge(sparrowDetectionData, sparrowSiteData, by=\"siteID\", all=TRUE)\nnames(newsparrow) <- sub(\"observer\", \"obs\", names(newsparrow))\nnames(newsparrow) <- sub(\"dist\", \"distance\", names(newsparrow))\nnames(newsparrow) <- sub(\"length\", \"Effort\", names(newsparrow))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nAlthough not formally written as a set of functions, we bring to the front of the code arguments the user will need to change to alter to suite their needs. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height, shrubclass. The same predictors, with the exception of observer could be used to model the sparrow counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- FALSE        # survey conducted using lines or points\ndettrunc <- 100  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 500       # number of bootstrap replicates\nset.seed(19191)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#fit-detection-function",
    "href": "Pr9/countmodel-lines.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Brewer’s sparrow data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newsparrow, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.obare <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(bare),\n                transect=surveytype, quiet=TRUE)\nwoo.oht <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.oshrc <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+shrubclass, \n             transect=surveytype, quiet=TRUE)\nwoo.ht <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.shrc <- ds(data=newsparrow, truncation=dettrunc, formula=~shrubclass, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(shrub), \n            transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newsparrow, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo.o,  woo.obare, woo.oht, woo.oshrc,\n                                 woo.ht, woo.shrc, woo.shrub, woo), digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\", row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~shrubclass\n0.511\n0.559\n0.025\n0.000\n\n\n\nHalf-normal\n~scale(height)\n0.492\n0.558\n0.025\n0.325\n\n\n\nHalf-normal\n~scale(shrub)\n0.476\n0.559\n0.025\n1.002\n\n\n\nHalf-normal\n~1\n0.469\n0.563\n0.025\n3.154\n\n\n\nHalf-normal\n~obs + scale(bare)\n0.659\n0.555\n0.025\n5.109\n\n\n\nHalf-normal\n~obs + shrubclass\n0.652\n0.556\n0.025\n5.533\n\n\n\nHalf-normal\n~obs + scale(height)\n0.658\n0.556\n0.025\n5.672\n\n\n\nHalf-normal\n~obs\n0.587\n0.559\n0.025\n7.148\n\n\n\n\n\nAll of the candidate models fit the Brewer’s sparrow line transect data. Also note that the estimate detection probability of all six models is the same to the third decimal. There is a small difference in AIC between the factor covariate shrubclass and the continuous covariate height. Simply for the purposes of demonstration, we will base our inference on the detection function that includes observer as a covariate. There is likely little effect of this model selection choice upon the ecological question of interest.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:5,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\"))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. For line transect example of Brewers Sparrows, Effort was measured in meters, but we wish to produce our estimates of density in numbers per hectare. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. In both cases, division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nsitesadj <- effAreafn(woo.o, newsparrow, 10000, dettrunc, pointflag = FALSE)"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nFit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function. Consequently, the offset must also use the log transform.\nTo generalise the code, and recognising the same call to glm() will need to be made elsewhere in this analysis, we specify the GLM model we wish to fit as an object of type formula. This was specified in the Analysis parameters specification section above.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nkable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-1.0565\n0.1397\n-7.5649\n0\n\n\nshrub\n0.0789\n0.0096\n8.1836\n0"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#visualise",
    "href": "Pr9/countmodel-lines.html#visualise",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\]\nwhere \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment.\nWe plot the estimated density against the continuous univariate predictor.\n\nsitesadj$density <- sitesadj$myCount / sitesadj$effArea \n# plot(sitesadj[, univarpredictor], sitesadj$density, pch=20,\n# firstplot <- recordPlot()"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame sparrowSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Brewer’s sparrow density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(sparrowSiteData$siteID, replace=TRUE, size=length(unique(sparrowSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newsparrow[newsparrow$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = FALSE)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and Brewer’s sparrow density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(sparrowSiteData[ , univarpredictor]),\n                                 max(sparrowSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=20, y=0.6, label= b0label, size=5) +\n  annotate(geom=\"text\", x=20, y=0.3, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Brewer's sparrow density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=.025, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.13, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Brewer’s sparrow density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Brewer’s sparrow."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. For the habitat characteristic shrub, there is a positive response of Brewer’s sparrow to increasing shrub cover.\n\nxlabel <- paste(\"Estimated slope of\", univarpredictor, \"and count relationship.\")\nhist.slope <- hist(slope.est, main=\"Sampling distribution of slope parameter\",\n                   xlab=xlabel) \ncibounds <- quantile(slope.est, probs = c(.025,.975), na.rm=TRUE)\nabline(v=cibounds, lty=3) \ntext(cibounds, max(hist.slope$counts), round(cibounds,3))\n\n\n\n\nSampling distribution of the parameter of interest for Brewer’s sparrow."
  },
  {
    "objectID": "Pr9/countmodel-points.html",
    "href": "Pr9/countmodel-points.html",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\]\nwhere \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\]\nwhen using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-points.html#data-organisation",
    "href": "Pr9/countmodel-points.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Data organisation",
    "text": "Data organisation\nNote there is no Effort field for these point count data.\n\nnewthrasher <- merge(thrasherDetectionData, thrasherSiteData, by=\"siteID\", all=TRUE)\nnames(newthrasher) <- sub(\"observer\", \"obs\", names(newthrasher))\nnames(newthrasher) <- sub(\"dist\", \"distance\", names(newthrasher))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nSpecification of run-specific parameters to analyse the Sage thrasher data set. Particularly note the logical value assigned to pointtransect. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height. The same predictors, with the exception of observer could be used to model the thrasher counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- TRUE        # survey conducted using lines or points\ndettrunc <- 170  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 100      # number of bootstrap replicates\nset.seed(7079)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-points.html#fit-detection-function",
    "href": "Pr9/countmodel-points.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Sage thrasher data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newthrasher, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(shrub), \n           transect=surveytype, quiet=TRUE)\nwoo.herb <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(herb), \n           transect=surveytype, quiet=TRUE)\nwoo.height <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(height), \n           transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newthrasher, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo, woo.o, woo.shrub, woo.herb, woo.height),\n             digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\",\n             row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~obs\n0.009\n0.292\n0.039\n0.000\n\n\n\nHalf-normal with cosine adjustment term of order 2\n~1\n0.097\n0.395\n0.124\n12.889\n\n\n\nHalf-normal\n~scale(herb)\n0.032\n0.331\n0.034\n17.893\n\n\n\nHalf-normal\n~scale(shrub)\n0.035\n0.331\n0.034\n17.901\n\n\n\nHalf-normal\n~scale(height)\n0.037\n0.333\n0.034\n19.168\n\n\n\n\n\nNone of the covariates contribute to fit of the detection function models for thrashers. Inference should be based upon the no covariate model (that passes the goodness of fit test), but for testing purposes, we will use the model with the observer covariate. Use of the observer covariate model will retain between-point variability in effective area; useful for testing purposes.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Sage thrasher\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nadd_df_covar_line(woo.o, data.frame(obs=\"obs6\"), col=\"coral\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:6,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\", \"coral\"))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. Division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nThe function has an argument pointflag used to indicate whether point transect sampling was used. If so, the correct effective area calculations are performed.\n\nsitesadj <- effAreafn(woo.o, newthrasher, 10000, dettrunc, pointflag = pointtransect)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nAs for the line transect example, fit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nknitr::kable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-2.3977\n0.8794\n-2.7266\n0.0064\n\n\nshrub\n0.0818\n0.0419\n1.9538\n0.0507"
  },
  {
    "objectID": "Pr9/countmodel-points.html#visualise",
    "href": "Pr9/countmodel-points.html#visualise",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\]\nwhere \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment."
  },
  {
    "objectID": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame thrasherSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Sage thrasher density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(thrasherSiteData$siteID, replace=TRUE, size=length(unique(thrasherSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newthrasher[newthrasher$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, transect=surveytype, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = pointtransect)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and bird density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(thrasherSiteData[ , univarpredictor]),\n                                 max(thrasherSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=24, y=1.5, label= b0label, size=5) +\n  annotate(geom=\"text\", x=24, y=1.4, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Sage thrasher density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.18, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Sage Thrasher density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Sage Thrasher."
  },
  {
    "objectID": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. In the case of the Sage Thrasher, the species appears to exhibit little response to the presence of shrub cover during this study. Note that the range of shrub cover is not extensive (~16% to ~26%). We can make no inference regarding the density:habitat relationship outside this range in shrub cover."
  },
  {
    "objectID": "Pr9/multi-analysis.html",
    "href": "Pr9/multi-analysis.html",
    "title": "Multispecies and multisession distance sampling analysis",
    "section": "",
    "text": "A multispecies data set with multiple visits\nIt is increasingly common for investigators to conduct surveys in which multiple species are detected and density estimates for several species are of interest. There are many ways of analysing such data sets, but care must be taken. Not all approaches will produce correct density estimates. To demonstrate one of the ways to produce incorrect estimates, we will use the line transect survey data reported in Buckland (2006). This survey (and data file) recorded detections of four species of songbirds. We conduct an analysis of chaffinch (Fringilla coelebs) (coded c in the data file), but similar results would arise with the other species.\nBegin by reading the flat file in a comma delimited format. Note the URL for the data file is very long, double check that you can read the URL including the Github token.\n\nURLpart1 <- \"https://raw.githubusercontent.com/distanceexamples/Distance-multispecies/main/montrave-line.csv\"\nURLpart2 <- \"?token=GHSAT0AAAAAABP6QDHAQ677QTIJEKSK2WYEYWG4EYA\"\nbirds <- read.csv(file=paste0(URLpart1, URLpart2))\n\n\n\nSurvey design\nBuckland’s design consisted of visiting each of the 19 transects in his study twice. To examine some of the errors that can arise from improper analysis, I choose to treat the two visits as strata for the express purpose of generating stratum (visit) -specific density estimates. Density estimates reported in Buckland (2006) are in units of birds \\(\\cdot hectare^{-1}\\).\n\nbirds$Region.Label <- birds$visit\ncu <- convert_units(\"meter\", \"kilometer\", \"hectare\")\n\n\n\nAnalysis of only one species (incorrectly)\nThe direct approach to producing a density estimate for the chaffinch would be to subset the original data frame and use the species-specific data frame for analysis. Begin by performing the subset operation.\n\nchaf <- birds[birds$species==\"c\", ]\n\nWhen the data are subset, the integrity of the survey design is not preserved. A simple frequency table of the species-specific data frame flags up a number of transect/visit combinations where no chaffinches were detected. The result is that the subset data frame suggests 3 of the 19 transects lacked chaffinch detections on the first visit and one of the 19 transects lacked chaffinch detections on the second visit. This revelation, in itself, causes no problems for our estimate of density of chaffinches.\n\ndetects <- table(chaf$Sample.Label, chaf$visit)\ndetects <- as.data.frame(detects)\nnames(detects) <- c(\"Transect\", \"Visit\", \"Detections\")\ndetects$Detections <- cell_spec(detects$Detections, \n                          background = ifelse(detects$Detections==0, \"red\", \"white\"))\nknitr::kable(detects, escape=FALSE) %>%\n  kable_paper(full_width=FALSE)\n\n\n\n \n  \n    Transect \n    Visit \n    Detections \n  \n \n\n  \n    1 \n    1 \n    3 \n  \n  \n    2 \n    1 \n    3 \n  \n  \n    3 \n    1 \n    4 \n  \n  \n    4 \n    1 \n    3 \n  \n  \n    5 \n    1 \n    5 \n  \n  \n    6 \n    1 \n    4 \n  \n  \n    7 \n    1 \n    2 \n  \n  \n    8 \n    1 \n    0 \n  \n  \n    9 \n    1 \n    1 \n  \n  \n    10 \n    1 \n    1 \n  \n  \n    11 \n    1 \n    0 \n  \n  \n    13 \n    1 \n    1 \n  \n  \n    14 \n    1 \n    1 \n  \n  \n    15 \n    1 \n    3 \n  \n  \n    16 \n    1 \n    2 \n  \n  \n    17 \n    1 \n    3 \n  \n  \n    18 \n    1 \n    3 \n  \n  \n    19 \n    1 \n    0 \n  \n  \n    1 \n    2 \n    1 \n  \n  \n    2 \n    2 \n    4 \n  \n  \n    3 \n    2 \n    3 \n  \n  \n    4 \n    2 \n    2 \n  \n  \n    5 \n    2 \n    4 \n  \n  \n    6 \n    2 \n    3 \n  \n  \n    7 \n    2 \n    3 \n  \n  \n    8 \n    2 \n    1 \n  \n  \n    9 \n    2 \n    0 \n  \n  \n    10 \n    2 \n    2 \n  \n  \n    11 \n    2 \n    1 \n  \n  \n    13 \n    2 \n    1 \n  \n  \n    14 \n    2 \n    1 \n  \n  \n    15 \n    2 \n    1 \n  \n  \n    16 \n    2 \n    1 \n  \n  \n    17 \n    2 \n    1 \n  \n  \n    18 \n    2 \n    4 \n  \n  \n    19 \n    2 \n    1 \n  \n\n\n\n\n\nHowever, there is a problem hidden within the table above. Transect 12 does not appear in the table because there were no detections of chaffinches on either visit. Consequently, there were 4 transects without chaffinches on the first visit and 2 transects without chaffinches on the second visit, rather than the 3 transects and 1 transect you might mistakenly conclude do not have chaffinch detections if you relied completely upon the table.\nLet’s see what the ds() function thinks about the survey effort using information from the species-specific data frame.\n\nchaf.wrong <- ds(chaf, key=\"hn\", convert_units = cu, truncation=95, formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(chaf.wrong$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    82.061 \n    4.319 \n    39 \n    15 \n    9.029868 \n    1.1159303 \n    0.1235821 \n  \n  \n    2 \n    33.2 \n    83.562 \n    4.398 \n    34 \n    17 \n    7.730787 \n    0.9798153 \n    0.1267420 \n  \n  \n    Total \n    66.4 \n    165.623 \n    8.717 \n    73 \n    32 \n    8.380327 \n    0.7425191 \n    0.0886026 \n  \n\n\n\n\n\nExamine the column labelled k (the number of transects) for each of the visits. Rather than the 19 transects that were surveyed on each visit, the ds() function erroneously believes there were only 15 transects surveyed on the first visit and 17 transects surveyed on the second visit.\nNote also the number of detections per kilometer; roughly 9 on the first visit and 7.7 on the second visit. These encounter rates exclude kilometers of effort on transects where there were no detections. We will return to this comparison later.\n\n\nUse explicit data hierarchy\n\n\n\n\n\n\nDescribing the survey design to ds\n\n\n\nAdditional arguments can be passed to ds() to resolve this problem. Consulting the ds() documentation\n\nregion_table data.frame with two columns:\n\nRegion.Label label for the region\nArea area of the region\nregion_table has one row for each stratum. If there is no stratification then region_table has one entry with Area corresponding to the total survey area. If Area is omitted density estimates only are produced.\n\nsample_table data.frame mapping the regions to the samples (i.e. transects). There are three columns:\n\nSample.Label label for the sample\nRegion.Label label for the region that the sample belongs to.\nEffort the effort expended in that sample (e.g. transect length).\n\n\n\n\nThis analysis that produces erroneous results can be remedied by explicitly letting the ds() function know about the study design; specifically, how many strata and the number of transects within each stratum (and associated transect lengths).\nConstruct the region table and sample table showing the two strata with equal areas and each labelled transect (of given length) is repeated two times.\n\nbirds.regiontable <- data.frame(Region.Label=as.factor(c(1,2)), Area=c(33.2,33.2))\nbirds.sampletable <- data.frame(Region.Label=as.factor(rep(c(1,2), each=19)),\n                                Sample.Label=rep(1:19, times=2),\n                                Effort=c(0.208, 0.401, 0.401, 0.299, 0.350,\n                                         0.401, 0.393, 0.405, 0.385, 0.204,\n                                         0.039, 0.047, 0.204, 0.271, 0.236,\n                                         0.189, 0.177, 0.200, 0.020))\n\n\n\nSimple detection function model\nThe chaffinch analysis is performed again, this time supplying the region_table and sample_table information to ds(). The correct number of transects (19) sampled on both visits (even though chaffinch was not detected on 4 transects on visit 1 and 2 transects on visit 2) is now recognised. Hence, the use of region table and sample table solves the problem of effort miscalculation if a species is not detected on all transects.\n\ntr <- 95   # as per Buckland (2006)\nonlycf <- ds(data=birds[birds$species==\"c\", ], \n             region_table = birds.regiontable,\n             sample_table = birds.sampletable,\n             trunc=tr, convert_units=cu, key=\"hn\", formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(onlycf$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    91.77 \n    4.83 \n    39 \n    19 \n    8.074534 \n    1.2196305 \n    0.1510465 \n  \n  \n    2 \n    33.2 \n    91.77 \n    4.83 \n    34 \n    19 \n    7.039338 \n    1.0612781 \n    0.1507639 \n  \n  \n    Total \n    66.4 \n    183.54 \n    9.66 \n    73 \n    38 \n    7.556936 \n    0.8083641 \n    0.1069698 \n  \n\n\n\n\n\n\n\nConsequence of incorrect analysis\nTo drive home the consequence of failing to properly specify the survey effort, contrast the encounter rate for the two visits from the incorrect calculations above (9.0 and 7.7 respectively), with the correct calculation (8.1 and 7.0 respectively). The number of transects is incorrect with the knock-on effect of effort being incorrect. If effort is incorrect then so too is covered area.\nThe ripple effect from incomplete information about the survey design results in positively biased estimates of density.\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2"
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "It is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\nPhoto by Jamshaid Mughal on Unsplash"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html",
    "href": "Pr9/Pr9-instructions.html",
    "title": "Analyses using multipliers",
    "section": "",
    "text": "We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets 2 and 3 deal with instantaneous cues and so only cue rate needs to be taken into account."
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#getting-started",
    "href": "Pr9/Pr9-instructions.html#getting-started",
    "title": "Analyses using multipliers",
    "section": "Getting started",
    "text": "Getting started\nThese data (called sikadeer) are available in the Distance package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?\n\nlibrary(Distance)\ndata(sikadeer)\n# Work out conversion units\nconversion.factor <- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "href": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "title": "Analyses using multipliers",
    "section": "Fit detection function to dung pellets",
    "text": "Fit detection function to dung pellets\nFit the usual series of models (i.e. half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don’t spend too long on this). Call your model deer.df. This detection function will be used to obtain \\(\\hat D_{\\textrm{pellet groups}}\\).\nHave a look at the Summary statistics for this model - what do you notice about the allocation of search effort in each woodland?"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#multipliers",
    "href": "Pr9/Pr9-instructions.html#multipliers",
    "title": "Analyses using multipliers",
    "section": "Multipliers",
    "text": "Multipliers\nThe next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.\nData to calculate this has been collected in the file IntroDS_9.1.csv in your directory. Following code comes from Meredith (2017).\n\nMIKE.persistence <- function(DATA) {\n  \n#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data \n#  Input: data frame with at least two columns:\n#         DAYS - calendar day on which dung status was observed\n#         STATE - dung status: 1-intact, 0-decayed\n#  Output: point estimate, standard error and CV of mean persistence time\n#\n#  Attribution: code from Mike Meredith website: \n#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm\n#   Citing: CITES elephant protocol\n#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf\n  \n  ##   Fit logistic regression model to STATE on DAYS, extract coefficients\n  dung.glm <- glm(STATE ~ DAYS, data=DATA, family=binomial(link = \"logit\"))\n  betas <- coefficients(dung.glm)\n  ##   Calculate mean persistence time\n  mean.decay <- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]\n  ## Calculate the variance of the estimate\n  vcovar <- vcov(dung.glm)\n  var0 <- vcovar[1,1]  # variance of beta0\n  var1 <- vcovar[2,2]  # variance of beta1\n  covar <- vcovar[2,1] # covariance\n  deriv0 <- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]\n  deriv1 <- -mean.decay/betas[2]\n  var.mean <- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2\n  ## Calculate the SE and CV and return\n  se.mean <- sqrt(var.mean)\n  cv.mean <- se.mean/mean.decay\n  out <- c(mean.decay, se.mean, 100*cv.mean)\n  names(out) <- c(\"Mean persistence time\", \"SE\", \"%CV\")\n  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab=\"Days since initiation\",\n       ylab=\"Dung persists (yes=1)\",\n       main=\"Eight dung piles revisited over time\")\n  curve(predict(dung.glm, data.frame(DAYS=x), type=\"resp\"), add=TRUE)\n  abline(v=mean.decay, lwd=2, lty=3)\n  return(out)\n}\ndecay <- read.csv(\"IntroDS_9.1.csv\")\npersistence.time <- MIKE.persistence(decay)\nprint(persistence.time)\n\nRunning the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been ‘jittered’ to avoid over-plotting.\nAn estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.\nAs stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:\n\ncreation contains estimates of the dung production rate and associated standard error\ndecay contains the dung decay rate and associated standard error where XX and YY are the estimates you obtained from the dung decay rate analysis.\n\n\n# Create list of multipliers\nmult <- list(creation = data.frame(rate=25, SE=0),\n#             decay    = data.frame(rate=XX, SE=YY))\nprint(mult)\n\nThe final step is to use these multipliers to convert \\(\\hat D_{\\textrm{pellet groups}}\\) to \\(\\hat D_{\\textrm{deer}}\\) (as in the equations above) - for this we need to employ the dht2 function. In the command below the multipliers= argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:\n\nstrat_formula=~Region.Label is specified to take into account the design (i.e. different woodlands or blocks).\nstratification=\"effort_sum\" is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.\ndeer.df is the detection function you have fitted.\n\n\n# Weight by effort because we have repeats\ndeer.ests <- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                 convert_units=conversion.factor, multipliers=mult, \n                 stratification=\"effort_sum\", total_area=13.9)\nprint(deer.ests)\n\nThe function dht2 also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata."
  }
]