[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is his first step in this adventure."
  },
  {
    "objectID": "announce/2022-11-30-announce1/index.html",
    "href": "announce/2022-11-30-announce1/index.html",
    "title": "Work to do prior to the workshop beginning",
    "section": "",
    "text": "This exercise is not computer-based. Therefore, you won‚Äôt find anything in the RStudio/Cloud account related to Practical 1. Instead, below find links to an instruction file and a sketch of the data to complete. Print the sketch and use it to fit a detection function to the data and complete the exercise. The HTML file contains the instructions explaining how to estimate duck nest density from your figure.."
  },
  {
    "objectID": "announce/2022-12-01-announce2/index.html",
    "href": "announce/2022-12-01-announce2/index.html",
    "title": "Announcement Number 2",
    "section": "",
    "text": "If I add considerably more text, this might extend the reading time beyond zero minutes.\nI think I would struggle to complete reading of this announcement down to 5 minutes. There is a lot of text here."
  },
  {
    "objectID": "announce.html",
    "href": "announce.html",
    "title": "Announcements",
    "section": "",
    "text": "0 min\n\n\nBe sure to complete Practical 2\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\nA pencil and paper exercise fitting a detection function to a histogram of perpendicular detection distances.\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/critique.html",
    "href": "extras/critique.html",
    "title": "Literature critique of distance sampling papers",
    "section": "",
    "text": "I‚Äôve found a few papers representing three taxonomic groups: terrestrial mammals, terrestrial birds and marine mammals. Select a paper of interest to you and create a list of positive and negative aspects of the way in which the design and analysis are reported. Do you feel the results are defensible in light of the methodology employed and described.\n\nBuuveibaatar, B., Strindberg, S., Kaczensky, P., Payne, J., Chimeddorj, B., Naranbaatar, G., Amarsaikhan, S., Dashnyam, B., Munkhzul, T., Purevsuren, T., Hosack, D. A., & Fuller, T. K. (2017). Mongolian Gobi supports the world‚Äôs largest populations of khulan Equus hemionus and goitered gazelles Gazella subgutturosa. Oryx 51(4):639‚Äì647. https://doi.org/10.1017/S0030605316000417\nZelelew, S. A., Bekele, A., & Archibald, G. (2020). Detection function, cluster size, density, and population size of Black Crowned Crane Balearica pavonina ceciliae in the upper Blue Nile River, Lake Tana area. Scientific African 10:e00557. https://doi.org/10.1016/j.sciaf.2020.e00557\nStrindberg, S., Ersts, P. J., Collins, T., Sounguet, G.-P., & Rosenbaum, H. C. (2020). Line transect estimates of humpback whale abundance and distribution on their wintering grounds in the coastal waters of Gabon. J. Cetacean Res. Manage. 153‚Äì160. https://doi.org/10.47536/jcrm.vi3.324"
  },
  {
    "objectID": "extras/extras.html",
    "href": "extras/extras.html",
    "title": "Development of additional skills",
    "section": "",
    "text": "Even if you never conduct a distance sampling analysis, you will have the opportunity to ‚Äúconsume‚Äù the results of distance sampling surveys. In that consumption, you will have to determine the credibility of published findings. Given your skills understanding how to design surveys and analyse data from those surveys, you can apply those skills in assessing the defensibility of work presented by others. This is the intent of the literature critique exercise. Examine one of the three published papers as a critical consumer and make determinations about the credibility of the findings.\nA ‚Äúmystery‚Äù data set is provided to take an analysis from start to finish. With this data set, you will make the series of decisions associated with distance sampling analysis. More importantly, you should emphasize the evidence used to support the decisions made."
  },
  {
    "objectID": "extras/mystery.html",
    "href": "extras/mystery.html",
    "title": "Your skills as a distance sampling analyst",
    "section": "",
    "text": "There was one covariate recorded ‚úç with each detection‚Äìwhether the animal was male or female .\nUnits of measure üìè are: perpendicular distances in meters, effort (transect lengths) in kilometers and area of study region in square kilometers.\n\nDon‚Äôt forget to set the units properly\n\nlibrary(Distance)\nconversion <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\nRemember some of the hints I have provided (in decision sequence above) along with decisions you will need to make regarding truncation.\nDo a competent job with your analysis and we will share our findings (preferred model, point estimate of abundance and precision of abundance estimate) on Thursday during the practical session.\nThe data set (in CSV format) is available in the file space of this project, shown in the File panel, named mystery.csv.\nThis will be your first adventure looking at data that is not contained within the Distance package. Therefore, your first analysis task will be to read the data from the .csv file into R for subsequent analysis. Example code below:\n\nmydata <- read.csv(\"https://raw.githubusercontent.com/erex/Oct-Quarto/main/extras/mystery.csv\")\n\nWhat happens after this is up to you. ü•¥ ü§î\n\n\n\n\nworkflow"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling workshop",
    "section": "",
    "text": "The materials are intended to be experiential rather than used as a reference. Exercises are paced to bring you up to speed with the fundamentals of distance sampling; how this method of population assessment differs from other forms of population sampling. This leads to the introduction of a detection function and ways to model it.\nFurther topics of model selection, assessing precision of population estimates and collection and analysis of point transect data round out the first half of the materials. These topics are depicted by the clothesline diagram below. At this point, you will have acquired sufficient experience to analyse basic distance sampling data.\n\n\n\nThe second half of the material (second clothesline diagram) exposes you to slightly more advanced concepts: design of distance sampling surveys, including the use of stratification. Also discussed are analytical methods associated with stratified surveys. This gives way to including predictors other than distance in modelling the detection process. Finally multipliers and methods associated with indirect animal surveys are introduced.\n\n\n\nAs well as the exercises (green) and their solutions (yellow), there are numerous supplements (grey), touching upon topics or demonstrating issues related to the analysis of distance sampling data. The supplements are not fundamental to successfully employing distance sampling methods, however, the more you understand tools such as distance sampling (via these supplements), the better you will be able to employ such methods.\n\nEach set of exercises is accompanied by a discussion of distance sampling principles. PDFs of those discussion materials are linked in the table below (as well as linked from the landing page of each exercise).\n\n\n\nTopic\nDate\n\n\n\n\nFundamental principles\n09 January\n\n\nDetection functions\n10 January\n\n\nModel criticism (selection and fit assessment)\n11 January\n\n\nMeasuring precision and controlling variance\n12 January\n\n\nPoint transect analysis and detailed example\n13 January\n\n\nDesign of distance sampling surveys\n16 January\n\n\nAnalysis of stratified surveys\n17 January\n\n\nCovariates in the detection function\n18 January\n\n\nIncluding multipliers in distance sampling analysis\n19 January\n\n\nField methods and summary\n20 January"
  },
  {
    "objectID": "Pr1/Pr1-instructions.html",
    "href": "Pr1/Pr1-instructions.html",
    "title": "Line transect detection function fitting",
    "section": "",
    "text": "Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e.¬†the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF provided, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e.¬†the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\] \\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]"
  },
  {
    "objectID": "Pr1/Prac1_solution.html",
    "href": "Pr1/Prac1_solution.html",
    "title": "Line transect detection function fitting solution",
    "section": "",
    "text": "In this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nFrequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g.¬†counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]"
  },
  {
    "objectID": "Pr1/sampling.html",
    "href": "Pr1/sampling.html",
    "title": "Sampling practical 1",
    "section": "",
    "text": "Lecture slides for 09 January 2023"
  },
  {
    "objectID": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "href": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "title": "Sampling practical 1",
    "section": "Estimating \\(\\widehat{P_a}\\) with a pencil",
    "text": "Estimating \\(\\widehat{P_a}\\) with a pencil\nThis exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region.\n\n\nPhoto by Shelby Cohron on Unsplash"
  },
  {
    "objectID": "Pr2/detnfns.html",
    "href": "Pr2/detnfns.html",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "Lecture slides for 10 January 2023"
  },
  {
    "objectID": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "href": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "title": "Detection functions practical 2",
    "section": "Estimating density of duck nests",
    "text": "Estimating density of duck nests\nThe data are the famous ‚Äúducknest‚Äù data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You‚Äôll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit.\n\nPhoto by Freysteinn G. Jonsson on Unsplash"
  },
  {
    "objectID": "Pr2/Pr2-instructions.html",
    "href": "Pr2/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "Objectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‚ÄòDefault‚Äô\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it with this pair of commands:\n\ninstall.packages(remotes)\nremotes::install_github(\"DistanceDevelopment/Distance\")\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn <- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn, nc=8)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e.¬†half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos <- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm <- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141‚Äì146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1‚Äì28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "Pr2/Prac2_solution.html",
    "href": "Pr2/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Inspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks <- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor <- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn <- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel : Half-normal key function \nAIC   : 928.1338 \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos <- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm <- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "Pr2/truncation-decisions.html",
    "href": "Pr2/truncation-decisions.html",
    "title": "Effective of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let‚Äôs see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment <- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result <- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this <- paste0(i-1,\"%\")\n    m <- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] <- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "Pr2/truncation-decisions.html#duck-nest-result",
    "href": "Pr2/truncation-decisions.html#duck-nest-result",
    "title": "Effective of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange <- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data."
  },
  {
    "objectID": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "href": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effective of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc <- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data."
  },
  {
    "objectID": "Pr3/criticism.html",
    "href": "Pr3/criticism.html",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "Lecture slides for 11 January 2023"
  },
  {
    "objectID": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "href": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "title": "Model criticism practical 3",
    "section": "Goodness of fit and model selection",
    "text": "Goodness of fit and model selection\nThis practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Rmarkdown (.rmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.\n\n\nPhoto by Blake Cheek on Unsplash"
  },
  {
    "objectID": "Pr3/modelsel-demo.html",
    "href": "Pr3/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003)."
  },
  {
    "objectID": "Pr3/modelsel-demo.html#half-normal-cosine",
    "href": "Pr3/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos <- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2806.01\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --> ID0\n   D --> ID1\n   D --> ID2\n   ID0(hn0<br>AIC=2817)\n   ID1(hn1<br>AIC=2806)\n   ID2(hn2<br>AIC=2808)\n   FIN(hn1)\n   ID0 --> FIN\n   ID1 --> FIN\n   ID2 --> FIN"
  },
  {
    "objectID": "Pr3/modelsel-demo.html#uniform-cosine",
    "href": "Pr3/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos <- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --> ID1\n   D --> ID2\n   D --> ID3\n   D --> ID4\n   ID1(unif1<br>AIC=2811)\n   ID2(unif2<br>AIC=2808)\n   ID3(unif3<br>AIC=2807)\n   ID4(unif4<br>AIC=2808)\n   FIN(unif3)\n   ID1 --> FIN\n   ID2 --> FIN\n   ID3 --> FIN\n   ID4 --> FIN"
  },
  {
    "objectID": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "href": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos <- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.47\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --> ID0\n   D --> ID1\n   ID0(hr0<br>AIC=2805)\n   ID1(hr1<br>AIC=2807)\n   FIN(hr0)\n   ID0 --> FIN\n   ID1 --> FIN\n\n\n\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms"
  },
  {
    "objectID": "Pr3/Pr3-instructions.html",
    "href": "Pr3/Pr3-instructions.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#accessing-the-data",
    "href": "Pr3/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let‚Äôs start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‚ÄòLine 11‚Äô.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#truncation",
    "href": "Pr3/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet‚Äôs start by fitting a basic model, i.e.¬†no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn <- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e.¬†objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m <- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\n\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per <- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)"
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#exploring-different-models",
    "href": "Pr3/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don‚Äôt spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‚Äòwrong‚Äô model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos <- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, ‚Ä¶, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins <- seq(from=0, to=80, by=10)\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin <- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)"
  },
  {
    "objectID": "Pr3/Prac3_solution.html",
    "href": "Pr3/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions"
  },
  {
    "objectID": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos <- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos <- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos <- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and QQ plots are shown below:\n\nplot(caper.hn.cos, main=\"Half normal\")\nx <- gof_ds(caper.hn.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.hr.cos, main=\"Hazard rate\")\nx <- gof_ds(caper.hr.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.uf.cos, main=\"Uniform\")\nx <- gof_ds(caper.uf.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nQQ plot half normal\n\n\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\n\nQQ plot hazard rate\n\n\n\n\n\n\n\n\n\nUniform with adjustment\n\n\n\n\n\n\n\nQQ plot uniform adj\n\n\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.613\n0.682\n0.098\n0.280\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.154\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results."
  },
  {
    "objectID": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‚Äòcorrect‚Äô perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, ‚Ä¶, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins <- c(0, seq(from=7.5, to=67.5, by=10), 80)\nprint(bins)\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\ncaper.hn.bin <- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n# See a portion of the results\nknitr::kable(caper.hn.bin$dht$individuals$summary, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\nMonaughty Forest\n1472\n3840\n240\n112\n1\n0.4666667\n0\n0\n1\n0\n\n\n\n\nknitr::kable(caper.hn.bin$dht$individuals$D[1:6], row.names = FALSE, digits=3)\n\n\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\n\n\n\n\nTotal\n0.045\n0.007\n0.152\n0.026\n0.079\n\n\n\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data."
  },
  {
    "objectID": "Pr4/left-truncation.html",
    "href": "Pr4/left-truncation.html",
    "title": "Supplement Analysis options for left truncation",
    "section": "",
    "text": "References\n\nAlldredge, J. R., & Gates, C. E. (1985). Line transect estimators for left-truncated distributions. Biometrics, 41(1), 273‚Äì280. https://doi.org/10.2307/2530663\n\n\nBuckland, S. T., Anderson, D. R., Burnham, K. P., Laake, J. L., Borchers, D. L., & Thomas, L. (2001). Introduction to distance sampling: Estimating abundance of biological populations. Oxford, New York: Oxford University Press."
  },
  {
    "objectID": "Pr4/Pr4-instructions.html",
    "href": "Pr4/Pr4-instructions.html",
    "title": "Variance estimation for systematic survey designs",
    "section": "",
    "text": "We will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g.¬†the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strongtrend in density. The systematically placed search strips areshaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon‚Äôt forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor <- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn <- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nsysvar2.hn$dht$individuals$D\nsysvar2.hn$dht$individuals$N\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e.¬†1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot <- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‚ÄòO2‚Äô (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label <- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 <- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e.¬†transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibita trend in density. The systematically placed search strips areshaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‚ÄòO2‚Äô estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label <- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn <- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nsysvar1.hn$dht$individuals$D\nsysvar1.hn$dht$individuals$N\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 <- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225‚Äì236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "Pr4/Prac4_solution.html",
    "href": "Pr4/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Basic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor <- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn <- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot <- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n      median    mean     se    lcl     ucl   cv\nNhat 1013.67 1033.53 255.36 640.87 1547.71 0.25\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label <- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 <- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label <- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn <- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 <- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225‚Äì236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "Pr4/precision.html",
    "href": "Pr4/precision.html",
    "title": "Precision practical 4",
    "section": "",
    "text": "Lecture slides for 12 January 2023"
  },
  {
    "objectID": "Pr4/precision.html#precision-of-density-estimates",
    "href": "Pr4/precision.html#precision-of-density-estimates",
    "title": "Precision practical 4",
    "section": "Precision of density estimates",
    "text": "Precision of density estimates\nThis exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set.\n\n\nPhoto by Andrea Sonda on Unsplash"
  },
  {
    "objectID": "Pr5/animal_distribution.html",
    "href": "Pr5/animal_distribution.html",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "",
    "text": "Simulation of line transect survey, I won‚Äôt show the simulation details, but here is the distribution of animals (3000) and placement of 40 transects in the study area.\nFrom this survey, sample pairs of transects to visually examine the uniformity of animal distances."
  },
  {
    "objectID": "Pr5/animal_distribution.html#two-transects",
    "href": "Pr5/animal_distribution.html#two-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Two transects",
    "text": "Two transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#five-transects",
    "href": "Pr5/animal_distribution.html#five-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Five transects",
    "text": "Five transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#ten-transects",
    "href": "Pr5/animal_distribution.html#ten-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Ten transects",
    "text": "Ten transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#fifteen-transects",
    "href": "Pr5/animal_distribution.html#fifteen-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Fifteen transects",
    "text": "Fifteen transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#twenty-transects",
    "href": "Pr5/animal_distribution.html#twenty-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Twenty transects",
    "text": "Twenty transects"
  },
  {
    "objectID": "Pr5/points.html",
    "href": "Pr5/points.html",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Lecture slides for 13 January 2023"
  },
  {
    "objectID": "Pr5/points.html#analysis-of-point-transect-data",
    "href": "Pr5/points.html#analysis-of-point-transect-data",
    "title": "Point transects practical 5",
    "section": "Analysis of point transect data",
    "text": "Analysis of point transect data\nHaving mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\n\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates‚Äìas you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof.¬†Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the ‚Äúsnapshot‚Äù method.\n\n\nPhoto by Vincent van Zalinge on Unsplash"
  },
  {
    "objectID": "Pr5/Pr5-instructions.html",
    "href": "Pr5/Pr5-instructions.html",
    "title": "Point transect sampling",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds."
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#probability-density-function",
    "href": "Pr5/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)"
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method."
  },
  {
    "objectID": "Pr5/Prac5_solution.html",
    "href": "Pr5/Prac5_solution.html",
    "title": "Point transect sampling solution",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise"
  },
  {
    "objectID": "Pr5/Prac5_solution.html#truncation-of-20m",
    "href": "Pr5/Prac5_solution.html#truncation-of-20m",
    "title": "Point transect sampling solution",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m <- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m <- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m <- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648"
  },
  {
    "objectID": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Point transect sampling solution",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\n\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\n\n\nHalf normal without truncation\n\n\n\n\n\n\n\nHalf normal 20m truncation\n\n\n\n\n\n\n\n\n\nHazard rate 20m truncation\n\n\n\n\n\n\n\nUniform cosine 20m truncation\n\n\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates."
  },
  {
    "objectID": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Point transect sampling solution",
    "section": "Probability density functions for Buckland‚Äôs winter wren point transects",
    "text": "Probability density functions for Buckland‚Äôs winter wren point transects\n\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\n\n\nDetection function 5 minute count\n\n\n\n\n\n\n\nDetection function snapshot\n\n\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data."
  },
  {
    "objectID": "Pr6/design.html",
    "href": "Pr6/design.html",
    "title": "Survey design practical 6",
    "section": "",
    "text": "Lecture slides for 16 January 2023"
  },
  {
    "objectID": "Pr6/design.html#designing-surveys-and-determining-effort-to-achieve-objectives",
    "href": "Pr6/design.html#designing-surveys-and-determining-effort-to-achieve-objectives",
    "title": "Survey design practical 6",
    "section": "Designing surveys and determining effort to achieve objectives",
    "text": "Designing surveys and determining effort to achieve objectives\nFundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with this R package, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add ‚Äúincentives‚Äù to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area.\n\nPhoto by ray rui on Unsplash"
  },
  {
    "objectID": "Pr6/effort-and-power.html",
    "href": "Pr6/effort-and-power.html",
    "title": "Effort and power calculations for line transect surveys",
    "section": "",
    "text": "Demonstration\n\n\n\nEffort needed to achieve objective"
  },
  {
    "objectID": "Pr6/effort-and-power.html#cv-graph",
    "href": "Pr6/effort-and-power.html#cv-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "CV graph",
    "text": "CV graph\nThe result tab showing the relationship between cumulative population change and necessary CV is shown in the CV graph tab. There is a positive relationship between these: greater population loss requires less precision to detect; or small population change requires more precise density estimates. The red dot and horizontal dotted line indicate the precision of annual estimates to detect the specified cumulative change over the specified number of annual surveys with the desired power. For these calculations, the encounter rate from the pilot survey plays no role."
  },
  {
    "objectID": "Pr6/effort-and-power.html#effort-graph",
    "href": "Pr6/effort-and-power.html#effort-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Effort graph",
    "text": "Effort graph\nThe result tab labelled Effort graph brings information from the pilot survey into the calculations. The CV graph indicates the necessary precision to achieve the desired results, this CV is fed into the formula from Buckland et al. (2015) to estimate the amount of survey effort to be expended annually to achieve the precision derived from the power calculations.\nThe red ball and horizontal dotted line now indicates the amount of effort needed to achieve the specified objectives. Small changes in abundance require exponentially larger amounts of annual survey effort to detect. The steepness of that exponential curve is less extreme when encounter rates are large."
  },
  {
    "objectID": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "href": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Numerical result of power calculation",
    "text": "Numerical result of power calculation\nThe final result tab simply provides a single numerical solution to the required CV and effort necessary to achieve that CV for the specified combination of cumulative change, number of annual surveys, power and pilot study encounter rate."
  },
  {
    "objectID": "Pr6/leaflet-demo.html",
    "href": "Pr6/leaflet-demo.html",
    "title": "Distance sampling survey design supplement",
    "section": "",
    "text": "Survey design with dssd\nThis exercise demonstrated how to examine the properties of various distance sampling survey designs. The exercise showed how to write survey locations to a GPX file, then import into Google Earth, but there‚Äôs a way to make visualisations all within R. I present here visualisations of the Tentsmuir point transect survey and a line transect survey of the coast of Ireland. The leaflet R package is used to show the placement of the survey effort.\n\n\n\n\n\nTentsmuir survey\nDesign of the survey begins with reading the unprojected shape file (coordinates likely degrees) and converting to a shape with distances measured in meters. A design is examined with allocation of point transects disproportionately between the two strata, with the smallest stratum receiving the highest allocation of effort. The final line of code in this chunk generates the coordinates of sampling stations for a realisation of this design.\n\nshapefile.name <- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape <- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string <- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape <- st_transform(sf.shape, crs = proj4string)\nregion.tm <- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\ncover.tm <- make.coverage(region.tm, n.grid.points = 100)\ndesign.tm <- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tm <- generate.transects(design.tm)\n\nUse either the +/- tools or mouse wheel to zoom and move around the map. The resulting map depicts the sampling stations as markers denoted with a binoculars icon. Hovering over the marker shows the latitude/longitude of each station. The red circles centred on each station is a circle of 100m radius, indicating the truncation distance specified in the make.design argument above. Use the measurement tool (upper right) to confirm the radius of the circles is 100m.\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\n\nTentsmuir study design, measuring tool top right.\n\n\n\n\nLine transect survey design with strata\nA different base map is used with leaflet to depict this marine survey. The basemap here shows some features of ocean bathymetry.\n\n\n\nIn contrast with the Tentsmuir survey, this begins with a projected study area map, with units of measure already in metres. Each of the six strata are given a different design.angle so as to approximate transects roughly perpendicular to the shore. Design specification is to have 15km spacing between lines within a stratum. Final line of code in this chunk produces coordinates of transects for a single realisation of this design.\n\nireland.name <- system.file(\"extdata\", \"AreaRProjStrata.shp\", package = \"dssd\")\nireland <- read_sf(ireland.name)\nst_crs(ireland)\n\nCoordinate Reference System:\n  User input: Albers-9 \n  wkt:\nPROJCRS[\"Albers-9\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",35,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-9,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nregion <- make.region(region.name = \"Area R Ireland coast\",\n                      units = \"m\",\n                      shape = ireland.name)\ncover <- make.coverage(region, n.grid.points = 100)\ndesign.space15k <- make.design(region = region,\n                               transect.type = \"line\",\n                               design = \"systematic\",\n                               spacing = 15000,\n                               design.angle = c(0, 160, 85, 90, 85, 160),\n                               edge.protocol = \"minus\",\n                               truncation = 2000,\n                               coverage.grid = cover)\nireland.trans <- generate.transects(object = design.space15k)\n\nUse the measurement tool (lower left corner) to check that line spacing is indeed 15km.\n\n\n\n\nMultiple strata for Irish survey."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html",
    "href": "Pr6/Pr6-instructions.html",
    "title": "Distance sampling survey design",
    "section": "",
    "text": "We provide two exercises in survey design so you can choose the one you feel is most useful to you.\n\nThe first example involves designing a line transect survey to estimate the abundance of porpoise, common dolphins and seals in and around St Andrews Bay.\n\nIt considers how you choose your design based on effort limitations.\nIt also compares an aerial survey based on systematic parallel lines with a boat based survey using zigzags.\n\nThe second example involves designing a point transect bird survey in Tentsmuir Forest.\n\nThis looks at how to project your study area from latitude and longitude on to a flat plane using R.\nIt also involves defining a design for multiple strata with different coverage in each strata."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#study-region",
    "href": "Pr6/Pr6-instructions.html#study-region",
    "title": "Distance sampling survey design",
    "section": "Study Region",
    "text": "Study Region\nFirst of all we will set up the study region and plot it. The shapefile for this study area is contained within the dssd R library. This shapefile has already been projected from latitude and longitude on to a flat plane and its units are in metres. The first line of code below returns the path for the shapefile within the R library and may vary on different computers. You will then pass this shapefile pathway to the make.region function to set up the survey region. As this shapefile does not have a projection (.prj) file associated with it we should tell dssd the units (m) when we create the survey region.\n\n#Find the pathway to the file\nshapefile.name <- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\n#Create the region using this shapefile\nregion <- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\n#Plot the region\nplot(region)\n\n\n\n\nStudy Region"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage",
    "href": "Pr6/Pr6-instructions.html#coverage",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover <- make.coverage(region, n.grid.points = 500)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "href": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "title": "Distance sampling survey design",
    "section": "Systematic Parallel Design",
    "text": "Systematic Parallel Design\nThe small survey plane available can complete a total flight time of around 250 km (excluding the flight time to and from the landing strip at Fife Ness). Generally, systematic parallel line designs are preferable for aerial surveys as they allow some rest time for observers as the plane travels between transects and avoids the sharp turns associated with zigzag designs.\nFirstly, we will consider the design angle. Often animal density is affected by distance to coast so it is probably wise for this survey to orientate lines approximately perpendicular to the coast. To do this we can select a design angle of 90 degrees. We can therefore expect to spend a little more than 40 km (the height of the survey region) on off-effort transit time and might hope to be able to complete around 200 km of transects. dssd lets us specify the desired line length as a design parameter and will then choose an appropriate value for transect spacing. We will choose a minus sampling strategy and set the truncation distance to 2 km. Note that as our survey region coordinates are in metres we also need to supply the design parameters in metres.\n\n# Define the design\ndesign.LL200 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      line.length = 200000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nNow we have defined the design we should check it visually by creating a survey (a single set of transects).\n\n\n\n\n# Create a single survey from the design\nsurvey.LL200 <- generate.transects(design.LL200)\n# Plot the region and the survey\nplot(region, survey.LL200)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nWe can see that the survey consists of parallel systematically spaced transects running horizontally across the survey region roughly perpendicular to the coast as we wanted. We can also view the details of the survey which will tell us what spacing dssd used to try and achieve a line length of 200 km.\n\n# Display the survey details\nsurvey.LL200\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced parallel transects\nSpacing:  4937.5\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nDesign angle:  90\nEdge protocol:  minus\nCovered area:  779476845\nStrata coverage: 78.93%\nStrata area:  987500079\n\n   Study Area Totals:\n   _________________\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nCovered area:  779476845\nAverage coverage: 78.93%\n\n\nWe can see that the spacing used by dssd was 4937.5 m which gives us 8 samplers and a coverage of just under 80%. In addition, this example survey has a line length of just under 200 km and a trackline length of just over 248 km. However, given the random nature of the design and the fact that the width of the study region is not constant everybody should get slightly different values. Although my trackline length was just under 250 km it is not sufficient to only look at one survey, we need to know that all surveys under this design will have a trackline length of < 250 km.\nTo assess the design statistics across many surveys we will now run a coverage simulation. This simulation will randomly generate many surveys from our design and record coverage as well as various statistics including line length and trackline length. As we know that coverage for a parallel line design is largely uniform (apart from edge effects due to minus sampling) we do not need to run too many repetitions, 100 should be sufficient to give us an indication of the range of line lengths and trackline lengths for this design.\n\n# Run the coverage simulation\ndesign.LL200 <- run.coverage(design.LL200, reps = 100)\ndesign.LL200\n\nAfter you have run the coverage simulation take a look at the design statistics. The mean line length should be around 200 km (200,000 m). Now look at the maximum trackline length, we need this value to be less than 250 km (250,000 m).\nUse the results of this simulation to create some new designs based on various spacings to find the maximum line length that can be achieved without risking exceeding the maximum trackline length of 250 km (remember to generate a line length of 200 km dssd selected a spacing of 4938m). Maybe try spacings of 5000 m or 5500 m.\n\n# Define the design\ndesign.space500 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\n\n\nWhat spacing would you select for this design?\n\nWhat is the maximum trackline length for the design you have selected?\n\nWhat on-effort line length are we likely to achieve?"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#zigzag-design",
    "href": "Pr6/Pr6-instructions.html#zigzag-design",
    "title": "Distance sampling survey design",
    "section": "Zigzag Design",
    "text": "Zigzag Design\nZigzag designs are often more efficient in their use of effort having less off-effort transit time between transects. For this survey another option would be to complete a boat-based survey. The boat survey will have the same total effort available allowing us a trackline length of 250 km.\nLet us now define a zigzag design for the same region. For zigzag designs the design angle has a different definition, it describes the angle across which the zigzags are constructed. For this example we want a vertical design angle so we will set it to 0. Zigzag designs also require an additional argument as zigzags can only be created inside convex shapes. We therefore need to specify the bounding shape, here we will choose a convex hull as it is more efficient than a minimum bounding rectangle. A convex hull works as if we were stretching an elastic band around the survey region. The code below shows you how to create the zigzag design, you should then create a single realisation of this design and plot it to check it looks acceptable.\n\n# Define the zigzag design\ndesign.zz.4500 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nsurvey.zz <- generate.transects(design.zz.4500)\nplot(region, survey.zz)\n\n\n\n\nSingle survey generated from the equalspaced zigzag design\n\n\n\n\nNext we will run a coverage simulation to verify that we have stayed within the restraints of our survey effort; a total trackline length of < 250 km. This time when we run the coverage simulation we will ask it to complete more repetitions so we can also assess the coverage.\nFirst we can output the design statistics.\n\n\nDoes this design meet our survey effort constraint?\n\nWhat is the maximum total trackline length for this design?\n\nWhat line length are we likely to achieve with this design?\n\nIs this higher or lower than the systematic parallel design?\n\n\n# Run the coverage simulation\ndesign.zz.4500 <- run.coverage(design.zz.4500, reps = 500)\n# Display the design statistics\ndesign.zz.4500\n\nNext we can check the coverage. Sometimes with zigzag surveys generated inside convex hulls we can get areas of higher coverage in narrower parts of the survey region at either end of the design axis. One of the easiest ways to assess coverage is visually by plotting the coverage grid.\n\n# Plot the coverage grid\nplot(design.zz.4500)\n\n\n\nDo you think the coverage scores look uniform across the study region?\n\nWhere are they higher/lower?\n\nWhy do you think this is?\n\n\nNote, you can go back to one of your parallel line designs and plot the coverage scores to compare (although there are fewer repetitions you can still get an idea of coverage)."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "href": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "title": "Distance sampling survey design",
    "section": "Projecting your Study Region",
    "text": "Projecting your Study Region\nThis exercise demonstrates how to deal with unprojected shapefiles. Study areas should always be projected onto a flat plane before you use them to design your survey. This is because in most parts of the world one degree latitude is not the same in distance as one degree longitude. If we didn‚Äôt project, our study region and any surveys generated in it, would be distorted possibly leading to non-uniform coverage.\nWe will now load the study region and project it onto a flat plane using an Albers Equal Area Conical projection. As we have to project the shapefile we load the shape object separately instead of directly into a region object.\n\n#Load the unprojected shapefile\nshapefile.name <- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape <- read_sf(shapefile.name)\n# Check current coordinate reference system\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n# Define a European Albers Equal Area projection\nproj4string <- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\n# Project the study area on to a flat plane\nprojected.shape <- st_transform(sf.shape, crs = proj4string)\n\nWe can now create the region object for dssd using the projected shape and plot it to check what it looks like.\n\n# Create the survey region in dssd\nregion.tm <- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n# Plot the survey region\nplot(region.tm)\n\n\n\n\nTentsmuir Forest: showing the main stratumand the Morton Loch stratum."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage-1",
    "href": "Pr6/Pr6-instructions.html#coverage-1",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover.tm <- make.coverage(region.tm, n.grid.points = 1000)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#design",
    "href": "Pr6/Pr6-instructions.html#design",
    "title": "Distance sampling survey design",
    "section": "Design",
    "text": "Design\nYou are now going to set up a systematic point transect design. We will assume that we have sufficient resources to survey 40 point transects. As the Morton Lochs stratum is of special interest we will give it higher coverage. We will therefore explicitly allocate 25 samplers to the main stratum and 15 to the Morton Lochs stratum (note that the area of the Morton Lochs stratum is much small than the main stratum). If we wanted to allocate the same effort to both stratum we could provide the samplers argument with the single value of 40 and it would divide the effort equally between the strata. We will leave the design angle as 0 and set the truncation distance to 100 m. We will use a minus sampling approach at the edges.\n\n\nWhat are the analysis implications of a design with unequal coverage?\n\n\n# Set up a multi strata systematic point transect design\ndesign.tm <- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#generate-a-survey",
    "href": "Pr6/Pr6-instructions.html#generate-a-survey",
    "title": "Distance sampling survey design",
    "section": "Generate a Survey",
    "text": "Generate a Survey\nYou will now generate a single survey from this design and plot it inside the survey region to check what it looks like. If you want to check whether the covered areas of the samplers in the Morton Lochs stratum overlap add the argument ‚Äòcovered.area = TRUE‚Äô to the plot function.\n\n# Create a single survey from the design\nsurvey.tm <- generate.transects(design.tm)\n# Plot the region and the survey\nplot(region.tm, survey.tm, covered.area = TRUE)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nNow look at the survey information.\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers?\n\nDid your survey achieve exactly the number of samplers you requested?\n\nHow much does coverage differ between the two strata for this realisation?\n\n\n# Display survey information\nsurvey.tm\n\n\nSave coordinates to a file\nIf this survey is to be conducted in the field, you will want the coordinates that you can load into a handheld GPS. The function write.transects() can write waypoints of the survey (in this case the point transect stations) to text, comma-separated value or GPX files.\n\nwrite.transects(survey.tm,\n                dsn = \"tentsmuir-points.gpx\",\n                layer = \"points\",\n                dataset.options = \"GPX_USE_EXTENSIONS=yes\",\n                proj4string=sf::st_crs(sf.shape))\n\nThe GPX file can be transferred to a GPS, or viewed using Google Earth.\n\n\n\n\n\nRealised survey with locations written to GPX file and imported into Google Earth."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "href": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "title": "Distance sampling survey design",
    "section": "Assessing Coverage and Design Statistics",
    "text": "Assessing Coverage and Design Statistics\nWe will now run a coverage simulation to assess how much the number of samplers and average coverage varies between surveys. We will also be able to assess how coverage varies spatially to see if edge effects are of concern.\nView the design statistics, then answer these questions.\n\n\nWhat is the minimum number of samplers you will achieve in each strata?\n\nIs this sufficient to complete separate analyses in each stratum?\n\nNext plot the coverage scores.\n\n\nDoes it appear that there is even coverage within each strata?\n\n\nAs there is such a difference in the range of coverage scores between strata you may need to plot each strata individually.\n\n\n\n# View the design statistics\ndesign.tm\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  100\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         13.0  36.0\nMean         25.1         15.1  40.2\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            0.9          1.1   1.3\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 683662.63    363258.99 1076099.58\nMean    768186.34    420283.45 1188469.79\nMedian  772479.34    417730.22 1188447.46\nMaximum 816669.52    468461.70 1264365.86\nsd       25071.81     25394.87   32875.01\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.85        50.79  7.26\nMean         5.44        58.76  8.02\nMedian       5.48        58.40  8.02\nMaximum      5.79        65.49  8.53\nsd           0.18         3.55  0.22\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00000000    0.2200000 0.00000000\nMean    0.05483087    0.5750000 0.08094378\nMedian  0.05000000    0.6450000 0.06000000\nMaximum 0.15000000    0.7800000 0.78000000\nsd      0.02997747    0.1456197 0.12170445\n\n# Plot the coverage scores\nplot(design.tm)\n\n\n\n# Plot coverage scores for individual strata\n# plot(design.tm, strata.id = 1)\n# plot(design.tm, strata.id = 2)"
  },
  {
    "objectID": "Pr6/Pr6-solution.html",
    "href": "Pr6/Pr6-solution.html",
    "title": "Distance sampling survey design solution",
    "section": "",
    "text": "Solution\n\n\n\nDistance sampling survey design"
  },
  {
    "objectID": "Pr6/Pr6-solution.html#systematic-parallel-line-design",
    "href": "Pr6/Pr6-solution.html#systematic-parallel-line-design",
    "title": "Distance sampling survey design solution",
    "section": "Systematic Parallel Line Design",
    "text": "Systematic Parallel Line Design\nWhat spacing would you select for this design? What is the maximum trackline length for the design you have selected? What on-effort line length are we likely to achieve?\nThe spacing chosen by dssd of 4937.5m to generate a line length of 200km resulted in a maximum trackline length of around 261km (each exact answer will vary due to the random generate of surveys). If we choose this design then it is possible that when we randomly generate our survey we may not be able to complete it with the effort we have available.\nWe should therefore increase the spacing between the transects and re-run the coverage simulations. A spacing of 5000m gave a maximum trackline length of around 249km (see summary table of Trackline length in the output below) so we can be fairly confident that we will be able to complete any survey which we randomly generate from this design. This spacing should allow us to achieve an on-effort line length of 199km (see Line length section of design summary below). The minimum line length we would expect to achieve is 184km and the maximum is 206km. [Note your values might differ slightly to those below]\n\nshapefile.name <- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\nregion.sab <- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\ncover.sabay <- make.coverage(region.sab, n.grid.points = 5000)\ndesign.spacing5km <- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.spacing5km <- run.coverage(design.spacing5km, reps = 250, quiet=TRUE)\nplot(design.spacing5km)\n\n\n\n\nCoverage grid plot for parallel design of St Andrews Bay.\n\n\n\n\n\nprint(design.spacing5km)\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced transects\nSpacing:  5000\nNumber of samplers:  NA\nLine length: NA\nDesign angle:  90\nEdge protocol:  minus\n\nStrata areas:  987500079\nRegion and effort units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        St Andrews Bay Total\nMinimum            7.0   7.0\nMean               7.9   7.9\nMedian             8.0   8.0\nMaximum            8.0   8.0\nsd                 0.3   0.3\n\n    Covered area:\n    \n        St Andrews Bay     Total\nMinimum      726120697 726120697\nMean         761566787 761566787\nMedian       766957698 766957698\nMaximum      778650766 778650766\nsd            15560206  15560206\n\n    % of region covered:\n    \n        St Andrews Bay Total\nMinimum          73.53 73.53\nMean             77.12 77.12\nMedian           77.67 77.67\nMaximum          78.85 78.85\nsd                1.58  1.58\n\n    Line length:\n    \n        St Andrews Bay     Total\nMinimum      184427.22 184427.22\nMean         197514.10 197514.10\nMedian       198870.66 198870.66\nMaximum      205772.41 205772.41\nsd             5946.04   5946.04\n\n    Trackline length:\n    \n        St Andrews Bay     Total\nMinimum      220530.48 220530.48\nMean         242619.03 242619.03\nMedian       246270.51 246270.51\nMaximum      248796.27 248796.27\nsd             8077.55   8077.55\n\n    Cyclic trackline length:\n    \n        St Andrews Bay     Total\nMinimum      251943.08 251943.08\nMean         279376.37 279376.37\nMedian       283849.57 283849.57\nMaximum      285982.65 285982.65\nsd             9523.22   9523.22\n\n    Coverage Score Summary:\n    \n        St Andrews Bay     Total\nMinimum      0.3680000 0.3680000\nMean         0.7714334 0.7714334\nMedian       0.7960000 0.7960000\nMaximum      0.8280000 0.8280000\nsd           0.0842902 0.0842902"
  },
  {
    "objectID": "Pr6/Pr6-solution.html#equal-spaced-zigzag-design",
    "href": "Pr6/Pr6-solution.html#equal-spaced-zigzag-design",
    "title": "Distance sampling survey design solution",
    "section": "Equal Spaced Zigzag Design",
    "text": "Equal Spaced Zigzag Design\nDoes this design meet our survey effort constraint? What is the maximum total trackline length for this design? What line length are we likely to achieve with this design? Is this higher or lower than the systematic parallel design?\nYou were asked to then run a coverage simulation and check if the trackline length was within our effort constraints. I found the maximum trackline length to be 242km (see Trackline length summary table in the output below) so within our constraint of 250km. I then got a mean line length of 221km and minimum and maximum line lengths of 212km and 227km, respectively (see Line length summary table in the output below). We can therefore expect to achieve just over 20km more on-effort survey line length with the zigzag design than the systematic parallel line design - 10% gain. [Note your values may differ slightly]\n\ndesign.zz.4500 <- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.zz.4500 <- run.coverage(design.zz.4500, reps = 250, quiet=TRUE)\n# Plot coverage\nplot(design.zz.4500)\n\n\n\n\nCoverage grid plot for zigzag design of St Andrews Bay.\n\n\n\n\nDo you think the coverage scores look uniform across the study region? Where are they higher/lower? Why do you think this is?\nYou were finally asked to look at the coverage scores across the survey region to see if this design has even coverage. There are some points with lower coverage around the survey region boundary. This is actually down to the fact we are using a minus sampling strategy. If we plotted coverage scores from a systematic parallel design we would see a similar pattern. Usually edge effects from minus sampling are minor unless we have a very long survey region boundary containing a small study area. If the fact that we are using a zigzag design was causing us issues with coverage we would expect to see higher coverage at the very top or very bottom of the survey region (as our design angle is 0). We do not see this. The survey region boundaries at the top and bottom are both quite wide and perpendicular to the design angle, in this situation zigzag designs perform well with regard to even coverage."
  },
  {
    "objectID": "Pr6/Pr6-solution.html#coverage",
    "href": "Pr6/Pr6-solution.html#coverage",
    "title": "Distance sampling survey design solution",
    "section": "Coverage",
    "text": "Coverage\nOrganise the study area shape file.\n\nshapefile.name <- system.file(\"extdata\", \"TentsmuirUnproj.shp\", \n                              package = \"dssd\")\nsf.shape <- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string <- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape <- st_transform(sf.shape, crs = proj4string)\nregion.tm <- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n\nCreate the coverage grid.\n\ncover.tm <- make.coverage(region.tm, n.grid.points = 5000)\ndesign.tm <- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tentsmuir <- generate.transects(design.tm)\n\n\nprint(survey.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  751.2295\nNumber of samplers:  27\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  809013\nStrata coverage: 5.73%\nStrata area:  14108643\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  218.3674\nNumber of samplers:  17\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  453781.4\nStrata coverage: 63.44%\nStrata area:  715264.9\n\n   Study Area Totals:\n   _________________\nNumber of samplers:  44\nCovered area:  1262794\nAverage coverage: 8.52%\n\n\nA spacing of 751m was used in the main stratum and 218m in the Morton Lochs stratum - these values are calculated based on the stratum areas and should not vary between surveys generated from the same design. You may or may not have achieved the number of transects you requested, this will depend on the random start point calculated for your particular survey. There will also be some variability in coverage, my survey achieved a coverage of 5.7% in the main strata and 64.8% in the Morton Loch strata.\nView the design statistics. What is the minimum number of samplers you will achieve in each strata? Is this sufficient to complete separate analyses in each stratum?\n\ncoverage.tentsmuir <- run.coverage(design.tm, reps=250, quiet=TRUE)\nprint(coverage.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         11.0  35.0\nMean         24.9         15.0  39.9\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            1.1          1.2   1.6\n\n    Covered area:\n    \n        Main Area Morton Lochs     Total\nMinimum 682512.01    332404.08 1052968.5\nMean    763558.27    414012.13 1177570.4\nMedian  765989.19    412220.42 1178210.6\nMaximum 821747.35    469068.05 1276134.0\nsd       30633.34     26469.33   42267.2\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.84        46.47  7.10\nMean         5.41        57.88  7.94\nMedian       5.43        57.63  7.95\nMaximum      5.82        65.58  8.61\nsd           0.22         3.70  0.29\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00400000    0.2000000 0.00400000\nMean    0.05408155    0.5819667 0.07943017\nMedian  0.05600000    0.6360000 0.05600000\nMaximum 0.09600000    0.7280000 0.72800000\nsd      0.01607712    0.1135346 0.11663505\n\n\nMy design statistics indicated I should achieve between 22 and 27 transects in the main stratum and between 12 and 18 in the Morton Lochs stratum. I might be a bit concerned about the possibility of only achieving 12 transects in the Morton Lochs stratum (remember I cannot just discard a survey due to the number of transects and generate another as it will affect my coverage properties) but whether this is sufficient will depend on a number of things‚Ä¶ what are the objectives of the study? how many detections are you likely to get from each transect? etc. Information from a pilot study would be useful to help decide how many transects are required as a minimum.\nDoes it appear that you that there is even coverage within strata?\n\nplot(coverage.tentsmuir, strata=1)\nplot(coverage.tentsmuir, strata=2)\n\n\n\n\n\n\nCoverage scores main stratum Tentsmuir Forest.\n\n\n\n\n\n\n\nCoverage scores Morton Lochs stratum Tentsmuir Forest.\n\n\n\n\n\n\nThe main strata looks to have fairly uniform coverage. The values appear to have such small levels of variability that the variability that is seen will be down to stochasticity as it is seen across the entire strata. The Morton Lochs strata we can see has areas of lower coverage around the edge of the study region. This grid is a bit too coarse to allow us to properly judge how much of an issue edge effects will be in this strata. It may be wise to re-run the coverage simulation with a finer coverage grid and more repetitions too. Edge effects could potentially be problematic in such small areas."
  },
  {
    "objectID": "Pr7/detecting-differences.html",
    "href": "Pr7/detecting-differences.html",
    "title": "Detecting differences in density estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nComparing two density estimates"
  },
  {
    "objectID": "Pr7/detecting-differences.html#output-interpretation",
    "href": "Pr7/detecting-differences.html#output-interpretation",
    "title": "Detecting differences in density estimates",
    "section": "Output interpretation",
    "text": "Output interpretation\nThe first two lines of output echo much of the input information: number of detections, encounter rate CV, line length, number of transects, average group size, CV of average groups size, number of parameters in the detection function. Also echoed are the stratum-specific density estimates and their CV along with the estimate of the \\(f(0)\\) parameter of the detection function and its CV.\nThe last line of output shows the difference of estimated densities and its standard error. Following this is the test statistic which is distributed as a t-statistic (Equation¬†2) and its associated degrees of freedom (Equation¬†3) and an associated significance level for a two-tailed test. The final pair of values are the bounds of the confidence interval on the estimated difference computed from Equation¬†5.\nNote: When a pooled detection function is used, the difference is relatively small (~0.05) and non-significant. When separate detection functions are used, the magnitude of the estimated difference increases (~0.08), with a standard error of roughly the same magnitude. Quite a few degrees of freedom are lost by having to estimate two additional parameters from separate detection functions. The total width of the confidence interval changes little (0.1662 for separate detection functions versus 0.1614 for pooled detection function). However because of the shift in the estimated difference from 0.0484 to 0.0802, the significance level shifts from 0.234 to 0.058.\nLesson for survey design: The uncertainty in all of the density estimates come from encounter rate variability. If the purpose of the study was to demonstrate a difference in density between these strata, a better survey design, with more than 13 and 12 transects would have been required to produce better estimates of stratum-specific encounter rate uncertainty.\n\n\n\n\n\n\nFootnote\n\n\n\nWhat if I used stratum as a covariate in the detection function?\nAt the time Buckland et al. (2001) was published, the use of covariates in the detection function was in early stages of development; hence that case was not considered when testing for differences between density estimates were described. Consequently, the appendix below does not address this situation.\nBut code exists for addressing assessing differences in density between strata when stratum is a covariate in the detection function. The code along with two examples of its use, are presented in our case study website."
  },
  {
    "objectID": "Pr7/detecting-differences.html#this-is-section-3.6.5-of-buckland2001",
    "href": "Pr7/detecting-differences.html#this-is-section-3.6.5-of-buckland2001",
    "title": "Detecting differences in density estimates",
    "section": "This is Section 3.6.5 of Buckland et al. (2001)",
    "text": "This is Section 3.6.5 of Buckland et al. (2001)\nFrequently, we wish to draw inference on change in density over time, or difference in density between habitats. We consider here simple comparisons between two density estimates.\nConsider two density estimates, \\(\\hat{D}_{1}\\) and \\(\\hat{D}_{2}.\\) Suppose first that they are independently estimated. We then estimate the difference in density by \\(\\hat{D}_{1}-\\hat{D}_{2}\\) with variance \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)=\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}\\right)+\\widehat{\\operatorname{var}}\\left(\\hat{D}_{2}\\right)\n\\tag{1}\\] Distance provides approximate degrees of freedom \\(\\mathrm{df}_{1}\\) for \\(\\hat{D}_{1}\\) and \\(\\mathrm{df}_{2}\\) for \\(\\hat{D}_{2},\\) based on Satterthwaite‚Äôs approximation. We can use these to obtain an approximate \\(t\\)-statistic:\n\\[\nT=\\frac{\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)-\\left(D_{1}-D_{2}\\right)}{\\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}} \\sim t_{\\mathrm{df}}\n\\tag{2}\\]\nwhere \\[\n\\mathrm{d} \\mathrm{f} \\simeq \\frac{\\left\\{\\widehat{v a r}\\left(\\hat{D}_{1}\\right)+\\widehat{v a r}\\left(\\hat{D}_{2}\\right)\\right\\}^{2}}{\\left\\{\\widehat{v a r}\\left(\\hat{D}_{1}\\right)\\right\\}^{2} / \\mathrm{df}_{1}+\\left\\{\\widehat{v a r}\\left(\\hat{D}_{2}\\right)\\right\\}^{2} / \\mathrm{df}_{2}}\n\\tag{3}\\]\nProvided df are around 30 or more, the simpler \\(z\\)-statistic provides a good approximation: \\[\nZ=\\frac{\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)-\\left(D_{1}-D_{2}\\right)}{\\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}} \\sim N(0,1)\n\\tag{4}\\] For either statistic, we can test the null hypothesis \\(H_{0}: D_{1}=D_{2}\\) by substituting \\(D_{1}-D_{2}=0\\) in Equation¬†2 or Equation¬†4, and looking at the resulting value in \\(t\\)-tables or \\(z\\)-tables. Approximate \\(100(1-2 \\alpha) \\%\\) confidence limits for \\(\\left(D_{1}-D_{2}\\right)\\) are given by \\[\n\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) \\pm t_{\\mathrm{df}}(\\alpha) \\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}\n\\tag{5}\\]\nfor df \\(<30,\\) or \\[\n\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) \\pm z(\\alpha) \\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}  \n\\tag{6}\\] otherwise. Often, a single detection function is fitted to pooled data, so that \\[\n\\hat{D}_{1}=\\frac{n_{1} \\hat{f}(0) \\hat{E}_{1}(s)}{2 L_{1}} \\quad \\text { and } \\quad \\hat{D}_{2}=\\frac{n_{2} \\hat{f}(0) \\hat{E}_{2}(s)}{2 L_{2}}\n\\tag{7}\\] (where the terms \\(\\hat{E}_{i}(s)\\) are omitted if the objects are not in clusters). Because \\(\\hat{f}(0)\\) appears in both equations, we can no longer assume that \\(\\hat{D}_{1}\\) and \\(\\hat{D}_{2}\\) are independent. Instead, we can write \\[\n\\hat{D}_{i}=\\hat{M}_{i} \\hat{f}(0), \\quad i=1,2\n\\tag{8}\\] where \\[\n\\hat{M}_{i}=\\frac{n_{i} \\hat{E}_{i}(s)}{2 L_{i}}\n\\tag{9}\\] As the \\(M_{i}\\) are independently estimated, and are assumed to be independent of \\(\\hat{f}(0),\\) we can now find the variance of \\(\\hat{D}_{1}-\\hat{D}_{2}\\) using the delta method: \\[\n\\hat{D}_{1}-\\hat{D}_{2}=\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right) \\hat{f}(0)\n\\tag{10}\\] so that \\[\n\\begin{aligned}\n\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) &=\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)^{2}\\left[\\frac{\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)}{\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)^{2}}+\\frac{\\widehat{\\operatorname{var}}\\{\\hat{f}(0)\\}}{\\{\\hat{f}(0)\\}^{2}}\\right] \\\\\n&=\\{\\hat{f}(0)\\}^{2} \\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)+\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)^{2} \\widehat{var}\\{\\hat{f}(0)\\}\n\\end{aligned}\n\\tag{11}\\] where \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)=\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}\\right)+\\widehat{\\operatorname{var}}\\left(\\hat{M}_{2}\\right)\n\\tag{12}\\] and \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{M}_{i}\\right)=\\hat{M}_{i}^{2}\\left[\\frac{\\widehat{\\operatorname{var}}\\left(n_{i}\\right)}{n_{i}^{2}}+\\frac{\\widehat{\\operatorname{var}}\\left\\{\\hat{E}_{i}(s)\\right\\}}{\\left\\{\\hat{E}_{i}(s)\\right\\}^{2}}\\right], \\quad i=1,2\n\\tag{13}\\] Note that the second form of Equation¬†11 still applies when \\(\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)=0,\\) whereas the first form leads to a ratio of zero over zero. Inference can now proceed as before, either with additional applications of Satterthwaite‚Äôs approximation in conjunction with Equation¬†2 if an approximate \\(t\\)-statistic is required, or more usually, by straightforward application of Equation¬†4."
  },
  {
    "objectID": "Pr7/Pr7-instructions.html",
    "href": "Pr7/Pr7-instructions.html",
    "title": "Analysis of stratified survey data",
    "section": "",
    "text": "Figure 1. An example of the sort of survey design used and a typical minke density gradient. The irregular bottom border is the ice-edge. The ‚Äòstepped‚Äô black line defines the boundary between the strata; dotted lines are transects and dots are detections.\n\nObjectives\nThe objectives of this exercise are to:\n\nCreate subsets of the data\nDecide whether to fit separate detection functions or a pooled detection function\nSpecify different stratification options using the dht2 function.\n\n\n\nGetting started\nBegin by reading in the data. Distances are in kilometers and a truncation distance of 1.5km is specified and used in the following detection function fitting. Perpendicular distances, transect lengths and study area size are all measured in kilometers; hence convert_units argument to ds is 1 and has been omitted. To keep things simple, a hazard rate detection function with no adjustments is used for all detection functions.\n\nlibrary(Distance)\ndata(minke)\nhead(minke)\n# Specify truncation distance\nminke.trunc <- 1.5\n\nYou will see that these data contain a column called ‚ÄòRegion.Label‚Äô: this contains values ‚ÄòNorth‚Äô or ‚ÄòSouth‚Äô.\n\n\nFull geographical stratification\nFirst, we want to fit encounter rate and detection function separately in each strata. This is easily performed by splitting the data by region and using ds on each subset. The commands below do this for the southern region (note, there are alternative ways to select a subset of data).\n\n# Create dataset for South \nminke.S <- minke[minke$Region.Label==\"South\", ]\n# Fit df to south\nminke.df.S.strat <- ds(minke.S, key=\"hr\", adjustment=NULL, truncation=minke.trunc)\nsummary(minke.df.S.strat)\n\nMake a note of the AIC. Perform a similar commands to obtain estimates for the northern region. What is the total AIC?\nAlso make a note of the abundance in each region. What is the total abundance in the study region?\n\n\nFitting a pooled detection function\nWe want to compare the total AIC found previously with the AIC from fitting a detection function to all data combined. This is easy to obtain:\n\nminke.df.all <- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\nGiven the AIC value for the detection function from the pooled data, would you fit a separate detection function in each strata or not?\n\n\nStratification options using dht2\nThe command summary(minke.df.all) will provide the abundance estimates for each region and the total and for this simple example, this is sufficient. However, if we want to consider different stratification options, then the dht2 function is useful.\nAfter fitting a detection function, the dht2 function, allows abundance estimates to be computed over some specified regions. In the command below, the pooled detection function is used to obtain estimates in each strata and over all (like the summary function previously used).\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~Region.Label, stratification=\"geographical\")\n\nThe arguments are:\n\nddf\n\nthe detection function (fitted by ds)\n\nflatfile\n\nthe data object containing all the necessary information\n\nData is referred to as being in a flatfile format if it contains information on region, transects and observations. An alternative is to use a hierarchical structure and have region, transect and observation information in separate data files with links between them to ensure that transects are mapped to the relevant region and observations to the relevant transect. We‚Äôve not used the hierarchical structure during this workshop.\n\n\nstrat_formula=~Region.Label\n\nformula (hence the ~) giving the stratification structure\n\nstratification=\"geographical\"\n\nin this example, we specify that each strata (specified in strat_formula) represents a geographical region.\n\nconvert_units\n\ngetting units conversion correct, same purpose as the convert_units argument in ds. For the minke data all measurements are in the same units, so the argument is not needed in this case.\n\n\nMake a note the total abundance in the study region.\n\n\n\n\n\n\nFailure to respect design during analysis\n\n\n\nWhat happens if we were to ignore the regions and treat the data as though it came from one large study region? This can (dangerously) be done by changing the stratification formula, as shown below.\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~1, stratification=\"geographical\")\n\nHas this changed the abundance estimate? Of course it has; the question is why has this changed the abundance estimate; which estimate is proper?"
  },
  {
    "objectID": "Pr7/Prac7_solution.html",
    "href": "Pr7/Prac7_solution.html",
    "title": "Analysis of data from stratified surveys solution",
    "section": "",
    "text": "Reading in the data from the stratified survey in the Southern Ocean:\n\nlibrary(Distance)\nlibrary(kableExtra)\n# Load data\ndata(minke)\nhead(minke, n=3)\n\n  Region.Label  Area Sample.Label Effort distance object\n1        South 84734            1  86.75     0.10      1\n2        South 84734            1  86.75     0.22      2\n3        South 84734            1  86.75     0.16      3\n\n# Specify truncation distance\nminke.trunc <- 1.5\n\n\nStrata treated distinctly\nFit detection function and encounter rate separately in each strata.\n\n## Fit to each region separately - full geographical stratification\n# Create data set for South\nminke.S <- minke[minke$Region.Label==\"South\", ]\nminke.df.S.strat <- ds(minke.S, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.S.strat)\n\n\nSummary for distance analysis \nNumber of observations :  39 \nDistance range         :  0  -  1.5 \n\nModel : Hazard-rate key function \nAIC   : 8.617404 \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.5102606 0.1921723\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.242147 0.3770239\n\n                      Estimate         SE       CV\nAverage p            0.4956459  0.0662961 0.133757\nN in covered region 78.6852058 13.8143714 0.175565\n\nSummary statistics:\n  Region  Area CoveredArea Effort  n  k         ER      se.ER     cv.ER\n1  South 84734     1453.23 484.41 39 13 0.08051031 0.01809954 0.2248102\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 4587.926 1200.166 0.2615924 2687.497 7832.219 21.14052\n\nDensity:\n  Label   Estimate         se        cv        lcl        ucl       df\n1 Total 0.05414505 0.01416393 0.2615924 0.03171687 0.09243301 21.14052\n\n# Combine selection and detection function fitting for North\nminke.df.N.strat <- ds(minke[minke$Region.Label==\"North\", ],\n                       truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.N.strat)\n\n\nSummary for distance analysis \nNumber of observations :  49 \nDistance range         :  0  -  1.5 \n\nModel : Hazard-rate key function \nAIC   : 37.27825 \n\nDetection function parameters\nScale coefficient(s):  \n               estimate        se\n(Intercept) -0.01081041 0.2203526\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.022021 0.6907906\n\n                      Estimate         SE        CV\nAverage p            0.7592309 0.09987673 0.1315499\nN in covered region 64.5389973 9.62021402 0.1490605\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 9986.683 3878.031 0.3883203 4469.865 22312.49 13.98197\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 Total 0.01583725 0.006149924 0.3883203 0.007088475 0.03538397 13.98197\n\n\n\n\nDetections combined across strata\nNext we fitted a pooled detection function.\n\nminke.df.all <- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\n\nSummary for distance analysis \nNumber of observations :  88 \nDistance range         :  0  -  1.5 \n\nModel : Hazard-rate key function \nAIC   : 48.63688 \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.2967912 0.1765812\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 0.964833 0.3605009\n\n                       Estimate         SE        CV\nAverage p             0.6224396  0.0668011 0.1073214\nN in covered region 141.3791725 17.7757788 0.1257312\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n2  South  84734     1453.23  484.41 39 13 0.08051031 0.01809954 0.2248102\n3  Total 715316     5528.37 1842.79 88 25 0.04133635 0.01181436 0.2858103\n\nAbundance:\n  Label  Estimate        se        cv      lcl       ucl       df\n1 North 12181.419 4638.6284 0.3807954 5499.950 26979.692 12.96781\n2 South  3653.345  910.0975 0.2491135 2181.595  6117.971 17.96263\n3 Total 15834.764 4834.2848 0.3052957 8388.423 29891.167 15.25496\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 North 0.01931774 0.007356107 0.3807954 0.008722023 0.04278538 12.96781\n2 South 0.04311546 0.010740641 0.2491135 0.025746391 0.07220207 17.96263\n3 Total 0.02213674 0.006758251 0.3052957 0.011726878 0.04178736 15.25496\n\n\nCompute combined AIC for entire study area.\n\naic.all <- summary(minke.df.all$ddf)$aic\naic.S <- summary(minke.df.S.strat$ddf)$aic\naic.N <- summary(minke.df.N.strat$ddf)$aic\naic.SN <- aic.S + aic.N\n\n\n\nDetermining the correct stratification to use\nThe AIC value for the detection function in the South was 8.617 and the AIC for the North was 37.28. This gives a total AIC of 45.9. The AIC value for the pooled detection function was 48.64. Because 48.64 is greater than 45.9, estimation of separate detection functions in each stratum is preferable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffering abundance estimates from stratification decision\n\n\n\nIn the full geographical stratification, both encounter rate and detection function were estimated separately for each region (or strata). This resulted in the following abundances:\n\n\n\n\nAbundance estimates using full geographical stratification.\n \n  \n    Label \n    Estimate \n  \n \n\n  \n    North \n    9987 \n  \n  \n    South \n    4588 \n  \n  \n    Total \n    14575 \n  \n\n\n\n\n\nNext, the distances were combined to fit a pooled detection function but encounter rate was obtained for each region. This resulted in the following abundances:\n\n\n\n\nAbundance estimates calculating encounter rate by strata and a pooled detection function.\n \n  \n    Label \n    Estimate \n  \n \n\n  \n    North \n    12181 \n  \n  \n    South \n    3653 \n  \n  \n    Total \n    15835 \n  \n\n\n\n\n\n\n\nAnother approach to stratification (advanced)\nAn equivalent result for full geographic stratification could be produced using the dht2 function, which does not require the disaggregation of the data set into two data sets.\n\n# Geographical stratification with stratum-specific detection function \nstrat.specific.detfn <- ds(data=minke, truncation=minke.trunc, key=\"hr\", \n                           adjustment=NULL, formula=~Region.Label)\nabund.by.strata <- dht2(ddf=strat.specific.detfn, flatfile=minke, \n                        strat_formula=~Region.Label, stratification=\"geographical\")\nprint(abund.by.strata, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : R2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label   Area CoveredArea  Effort  n  k    ER se.ER cv.ER\n        North 630582     4075.14 1358.38 49 12 0.036 0.013 0.365\n        South  84734     1453.23  484.41 39 13 0.081 0.018 0.225\n        Total 715316     5528.37 1842.79 88 25 0.048 0.011 0.237\n\nAbundance estimates:\n Region.Label Estimate       se    cv  LCI   UCI     df\n        North     9865 3761.296 0.381 4451 21860 13.035\n        South     4651 1224.818 0.263 2719  7955 22.163\n        Total    14515 3970.578 0.274 8215 25648 16.065\n\nComponent percentages of variance:\n Region.Label Detection    ER\n        North      8.18 91.82\n        South     27.13 72.87\n        Total     12.49 87.51\n\n\nI won‚Äôt say anything just now about the wrinkle I introduced with the formula argument in the call to ds(). Recognise there is an alternative (easier) way to perform the full geographic stratification analysis without tearing apart the data. The abundance estimates presented in the last output do not identically match the estimates shown earlier for full geographic stratification, but they are close. The added benefit of this latter analysis is that the uncertainty in the total population size is computed within dht2 rather than needing to be calculated manually using the delta method.\n\n\n\n\n\n\nAn aside\n\n\n\nIf geographic stratification were ignored, the abundance estimate of would be 18,293 minkes. This estimate is substantially larger than the estimates above. The reason is that the survey design was geographically stratified with a smaller proportion of the north stratum receiving sampling effort and a greater proportion of the southern stratum receiving survey effort. Ignoring this inequity in the unstratified analysis would lead us to believe that the more heavily sampled southern stratum is indicative of whale density throughout the study area."
  },
  {
    "objectID": "Pr7/strata.html",
    "href": "Pr7/strata.html",
    "title": "Stratification practical 7",
    "section": "",
    "text": "Lecture slides for 17 January 2023"
  },
  {
    "objectID": "Pr7/strata.html#analysis-of-stratified-surveys-1",
    "href": "Pr7/strata.html#analysis-of-stratified-surveys-1",
    "title": "Stratification practical 7",
    "section": "Analysis of stratified surveys",
    "text": "Analysis of stratified surveys\nCarrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function.\n\nPhoto by Nick Fewings on Unsplash"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html",
    "href": "Pr7/stratumspecific-bias.html",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nDistance sampling simulation where detection functions differ between strata. When stratum-specific abundance estimates are produced using a pooled detection function, bias arises. The magnitude of the bias depends upon the magnitude of the difference in the detection functions."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "href": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "North Sea study area",
    "text": "North Sea study area\nInterest is in estimating the density of minke whales in the western portion of the North Sea, off the east coast of Britain. The study area is divided into north and south strata, with the north stratum being roughly 1.9 times the size of the south stratum, as shown in the map below.\n\nm <- leaflet() %>% addProviderTiles(providers$Esri.OceanBasemap)\nm <- m %>% \n  setView(1.4, 55.5, zoom=5)\nminkes <- read_sf(myshapefilelocation)\nstudy.area.trans <- st_transform(minkes, '+proj=longlat +datum=WGS84')\nm <- addPolygons(m, data=study.area.trans$geometry, weight=2)\nm"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "href": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Properties of the design",
    "text": "Properties of the design\nTo demonstrate the estimated number of transects in each stratum, the run.coverage function is used to show the number of replicates in each stratum is allocated roughly according to stratum size.\n\ndesign.properties <- run.coverage(equal.cover, reps = 10, quiet=TRUE)\nmine <- data.frame(Num.transects=design.properties@design.statistics$sampler.count[3,],\n                   Proportion.covered=design.properties@design.statistics$p.cov.area[3,])\nkable(mine)\n\n\n\n\n\nNum.transects\nProportion.covered\n\n\n\n\nSouth\n17\n8.12\n\n\nNorth\n23\n8.11\n\n\nTotal\n40\n8.13"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "href": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata",
    "text": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata\n\ndelta.multiplier <- c(seq(from=0.5, to=1.1, by=0.1),\n#                      seq(from=0.85, to=1.15, by=0.1),\n                      seq(from=1.2, to=2.4, by=0.2))\nsigma.south <- 0.3\nnorth.sigma <- sigma.south*delta.multiplier\n\nScale parameter (\\(\\sigma\\)) for the southern stratum remains fixed at 0.3, but in the northern stratum, the scale parameter is a multiple of the southern stratum \\(\\sigma\\), ranging from a low of 0.15 to a maximum of 0.72.\n\nhn <- function(sigma, x) {return(exp(-x^2/(2*sigma^2)))}\nfor (i in seq_along(north.sigma)) {\n  curve(hn(north.sigma[i],x),from=0,to=0.8,add=i!=1,  \n        xlab=\"Distance\", ylab=\"Detection probability\", \n        main=\"Range of detection probability disparity\\nSouth function in blue\")\n}\ncurve(hn(sigma.south,x),from=0,to=0.8, lwd=2, col='blue', add=TRUE)\n\n\n\n\n\nequalcover <- list()\nwhichmodel <- list()\nnum.sims <- 500\nfor (i in seq_along(delta.multiplier)) {\n  sigma.strata <- c(sigma.south, sigma.south*delta.multiplier[i])\n  detect <- make.detectability(key.function = \"hn\",\n                               scale.param = sigma.strata,\n                               truncation = 0.8)\n  equalcover.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = pooled.hn)\n  whichmodel.sim <- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = strat.specific.or.not)\n  equalcover[[i]] <- run.simulation(equalcover.sim, run.parallel = TRUE, max.cores=7)\n  whichmodel[[i]] <- run.simulation(whichmodel.sim, run.parallel = TRUE, max.cores=7)\n}"
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "href": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Conclusions from this portion of study",
    "text": "Conclusions from this portion of study\nNote bias in the estimated density for the entire study area is never greater than 10%, yet another demonstration of pooling robustness. Even with widely differing detection functions, the estimated density ignoring stratum-specific differences is essentially unbiased.\n\n\n\n\n\n\n\n\nConfidence interval coverage for stratum-specific estimates approaches nominal levels when \\(\\Delta \\approx 1\\). Coverage for the density estimate in the entire study area is nominal for all values of \\(\\Delta\\) with the exception of \\(\\Delta<0.7\\)."
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "href": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Model selection sensitivity",
    "text": "Model selection sensitivity\nThis small simulation demonstrates the peril of making stratum-specific estimates when using a detection function that does not recognise stratum-specific detection function differences. This situation can arise when numbers of stratum-specific detections are too small to support stratum-specific detection functions. This set of simulations was devised such that there was sufficient effort in each stratum to avoid small numbers of detections. Even so, use of the ‚Äúwrong‚Äù (pooled) detection function leads to considerable bias in density estimates.\n\nplot(delta.multiplier, modelsel, \n     main=\"Stratum-specific model chosen\", type=\"b\", pch=20,\n     xlab=expression(Delta), ylab=\"Stratum covariate chosen\")\nabline(h=0.50)\n\n\n\n\nThere are two messages from this model selection assessment. Only when \\(\\Delta < 0.8\\) or \\(\\Delta > 1.2\\) is there a better than even chance AIC will detect the difference in detectability between strata. Values of \\(\\Delta\\) in this region do not lead to extreme bias in stratum-specific density estimates when the pooled detection function model is used. There is roughly a 10% negative bias in density estimates of the north stratum and a 5% positive bias in density estimates of the southern stratum."
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html",
    "href": "Pr8/alternative-selection-metrics.html",
    "title": "Other model selection metrics",
    "section": "",
    "text": "Demonstration\n\n\n\nAlternative model selection metrics"
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#aicc",
    "href": "Pr8/alternative-selection-metrics.html#aicc",
    "title": "Other model selection metrics",
    "section": "AICc",
    "text": "AICc\nDefined as:\n\\[AICc = -2ln(\\mathscr{L}) - 2k + \\frac{2k(k+1)}{n-k-1}\\]\nwhere k is the number of parameters in the model and n is the number of observations. In general, if \\(n\\) is many times larger than \\(k^2\\), then the extra term will be negligible. What do we know, in general, about the magnitude of k and n in typical distance sampling situations? We encourage the collection of 60 to 80 detections (\\(n\\)). A hazard rate key function with a four-level factor covariate has \\(k=5\\) parameters. The value of the the \\(c\\) term added to the regular AIC ranges from 1.11 with \\(n=60\\) to 0.81 with \\(n=80\\).\nFor a two-parameter model (hazard rate or half normal with a single continuous covariate) the value of the additional term would be 0.21 with \\(n=60\\) to 0.16 with \\(n=80\\). Recognise the magnitude of the other terms in the AIC are in the hundreds or thousands (see below)."
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#bic",
    "href": "Pr8/alternative-selection-metrics.html#bic",
    "title": "Other model selection metrics",
    "section": "BIC",
    "text": "BIC\nDefined as:\n\\[BIC = -2ln(\\mathscr{L}) - k \\cdot ln(n)\\]\nThe penalty term changes as a function of sample size; the larger the number of detections, the greater the penalty term. With a 2 parameter model using AIC, the penalty term would be 4 (\\(2 \\times 2\\)). For a model with the the same number of parameters and 80 detections, the penalty term would be 8.8."
  },
  {
    "objectID": "Pr8/covariates.html",
    "href": "Pr8/covariates.html",
    "title": "Covariates practical 8",
    "section": "",
    "text": "Lecture slides for 18 January 2023"
  },
  {
    "objectID": "Pr8/covariates.html#covariates-in-detection-function",
    "href": "Pr8/covariates.html#covariates-in-detection-function",
    "title": "Covariates practical 8",
    "section": "Covariates in detection function",
    "text": "Covariates in detection function\nIt is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata‚Äìwe examine whether the geographic stratum effect can be modelled as a covariate.\n\n\nPhoto by Roman Mager on Unsplash"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html",
    "href": "Pr8/Group-size-covariate.html",
    "title": "Size bias‚Äîhow large is the problem?",
    "section": "",
    "text": "Supplement\n\n\n\nCorrecting size bias via size as a covariate. When does it matter?"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "title": "Size bias‚Äîhow large is the problem?",
    "section": "Analysis including group size covariate",
    "text": "Analysis including group size covariate\n\nrunsim.cov <- run.simulation(size.cov, run.parallel = TRUE)\n\n\n\n\n\n\nThe distribution of computed average group size centred on the true size of 10 and there was no problem with fitting a detection function. The average over the simulations estimated number of individuals was 2235.07."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "title": "Size bias‚Äîhow large is the problem?",
    "section": "Analysis without group size covariate",
    "text": "Analysis without group size covariate\nAs a comparison, what happens if we don‚Äôt include size as a covariate in our detection function?\n\n\n\n\n\nThe distribution of computed average groups sizes is shown above. We would expect an overestimate of mean group size because small groups at large distances are missing from our sample; but that effect is small in this instance. As a consequence, the average \\(\\hat{N}_{indiv}\\) across all simulations is 2297.36."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "title": "Size bias‚Äîhow large is the problem?",
    "section": "Analysis with covariate",
    "text": "Analysis with covariate\n\n\n\n\n\nWhen including size as a covariate, estimates of average group size are not affected (figure above). Likewise, mean \\(\\hat{N}_{indiv}\\) is effectively unbiased: 5462.57."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "title": "Size bias‚Äîhow large is the problem?",
    "section": "Analysis without the covariate",
    "text": "Analysis without the covariate\n\n\n\n\n\nNow mean \\(\\hat{N}_{indiv}\\) is considerably biased: 5744.95, 30.9 percent larger than the true number of individuals in the population, 4388."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html",
    "href": "Pr8/Pr8-instructions.html",
    "title": "Covariates in detection function model",
    "section": "",
    "text": "This exercise consists of three data sets of increasing difficulty. The first problem, MCDS with point transects, is complicated and (using the functionality available in R) also includes some basic exploratory analysis of the covariates. Section 2 and 3 are optional but will take you deeper into the heart of understanding multiple covariates."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "href": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "title": "Covariates in detection function model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIt is important to gain an understanding of the data prior to fitting detection functions (Buckland et al., 2015). With this in mind, preliminary analysis of distance sampling data involves:\n\nassessing the shape of the collected data,\nconsidering the level of truncation of distances, and\nexploring patterns in potential covariates.\n\nWe begin by assessing the distribution of distances to decide on a truncation distance.\n\nhist(amakihi$distance)\n\nTo see if there are differences in the distribution of distances recorded by the different observers and in each hour after sunrise, boxplots can be used. Note how the ~ symbol is used to define the discrete groupings (i.e.¬†observer and hour).\n\n# Boxplots by obs\nboxplot(amakihi$distance~amakihi$OBs, xlab=\"Observer\", ylab=\"Distance (m)\")\n# Boxplots by hour after sunrise\nboxplot(amakihi$distance~amakihi$HAS, xlab=\"Hour\", ylab=\"Distance (m)\")\n\nThe components of the boxplot are:\n\nthe thick black line indicates the median\nthe lower limit of the box is the first quartile (25th percentile) and the upper limit is the third quartile (75th percentile)\nthe height of the box is the interquartile range (75th - 25th quartiles)\nthe whiskers extend to the most extreme points which are no more than 1.5 times the interquartile range.\ndots indicate ‚Äòoutliers‚Äô if there are any, i.e.¬†points beyond the range of the whiskers.\n\nFor minutes after sunrise (a continuous variable), we create a scatterplot of MAS (on the \\(x\\)-axis) against distances (on the \\(y\\)-axis). The plotting symbol (or character) is selected with the argument pch:\n\n# Plot of MAS vs distance (using dots)\nplot(x=amakihi$MAS, y=amakihi$distance, xlab=\"Minutes after sunrise\",\n     ylab=\"Distance (m)\", pch=20)\n\nYou may also want to think about potential collinerity (linear relationship) between the covariates - if collinear variables are included in the detection function, they will be explaining some of the same variation in the distances and this will reduce their importance as a potential covariate. How might you investigate the relationship between HAS and MAS?\nFrom these plots can you tell if any of the covariates will be useful in explaining the distribution of distances?"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "href": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "title": "Covariates in detection function model",
    "section": "Adjusting the raw covariates",
    "text": "Adjusting the raw covariates\nWe would like to treat OBs and HAS as factor variables as in the original analysis; OBs is, by default, treated as a factor variable because it consists of characters rather than numbers. HAS, on the other hand, consists of numbers and so by default would be treated as a continuous variable (i.e.¬†non-factor). That is fine if we want the effect of HAS to be monotonic (i.e.¬†detectability either increases or decreases as a function of HAS). If we want HAS to have a non-linear effect on detectability, then we need to indicate to R to treat it as a factor as shown below.\n\n# Convert HAS to a factor\namakihi$HAS <- factor(amakihi$HAS)\n\nThe next adjustment is to change the reference level of the observer and hour factor covariates - the only reason to do this is to get the estimated parameters in the detection function to match the parameters estimated in T. A. Marques et al. (2007). You would not carry out this step on your own data. By default R uses the first factor level but by using the relevel function, this can be changed:\n\n# Set the reference level \namakihi$OBs <- relevel(amakihi$OBs, ref=\"TKP\")\namakihi$HAS <- relevel(amakihi$HAS, ref=\"5\")"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#candidate-models",
    "href": "Pr8/Pr8-instructions.html#candidate-models",
    "title": "Covariates in detection function model",
    "section": "Candidate models",
    "text": "Candidate models\nWith three potential covariates, there are 8 possible models for the detection function:\n\nNo covariates\nOBs\nHAS\nMAS\nOBs + HAS\nOBs + MAS\nHAS + MAS\nOBs + HAS + MAS\n\nEven without considering covariates there are also several possible key function/adjustment term combinations available: if all key function/covariate combinations are considered the number of potential models is large. Note that covariates are not allowed if a uniform key function is chosen and if covariate terms are included, adjustment terms are not allowed. Even with these restrictions, it is not best practice to take a scatter gun approach to detection function model fitting. Buckland et al. (2015) considered 13 combinations of key function/covariates. Here, we look at a subset of these.\nFit a hazard rate model with no covariates or adjustment terms and make a note of the AIC. Note, that 10% of the largest distances are truncated - you may have decided on a different truncation distance.\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr <- ds(amakihi, transect=\"point\", key=\"hr\", truncation=\"10%\",\n              adjustment=NULL, convert_units = conversion.factor)\n\nMake a note of the AIC for this model.\nNow fit a hazard rate model with OBs as a covariate in the detection function and make a note of the AIC. Has the AIC reduced by including a covariate?\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr.obs <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs,\n                  truncation=\"10%\", convert_units = conversion.factor)\n\nFit a hazard rate model with OBs and HAS in the detection function:\n\namak.hr.obs.has <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs+HAS,\n                      truncation=\"10%\", convert_units = conversion.factor)\n\nTry fitting other possible formula and decide which model is best in terms of AIC. To quickly compare AIC values from different models, use the AIC command as follows (note only models with the same truncation distance can be compared):\n\n# AIC values\nAIC(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nAnother useful function is summarize_ds_models - this has the advantage of ordering the models by AIC (smallest to largest).\n\n# Compare models\nsummarize_ds_models(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nOnce you have decided on a model, plot your selected detection function."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#analysis",
    "href": "Pr8/Pr8-instructions.html#analysis",
    "title": "Covariates in detection function model",
    "section": "Analysis",
    "text": "Analysis\nThe data are available in the Distance package:\n\ndata(ETP_Dolphin)\nhead(ETP_Dolphin, n=3)\n\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint - try dividing the histogram of distances into a large number of intervals.)\nAs there are a number of potential covariates to be used in this example (i.e.¬†search method, cue, Beaufort class and month), try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except group size and because they have numeric codes, use the factor function to let R know to treat them as factors.\nNote that both distances and transect lengths were recorded in nautical miles and area in nautical miles squared and so the argument convert_units does not need to be specified.\nKeep in mind that this is a large dataset (> 1000 observations), and hence estimation may take a while. You will likely end up with quite a few models as there are several potential covariates and no ‚Äòright‚Äô answers. Discuss your choice of final model (or models) with your colleagues - did you make the same choices?"
  },
  {
    "objectID": "Pr8/Prac8_solution.html",
    "href": "Pr8/Prac8_solution.html",
    "title": "Covariates in the detection function solution",
    "section": "",
    "text": "Solution\n\n\n\nCovariates in the detection function"
  },
  {
    "objectID": "Pr8/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "href": "Pr8/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "title": "Covariates in the detection function solution",
    "section": "Colourful plot noting effect of cue type",
    "text": "Colourful plot noting effect of cue type\nJust an example of using the function add_df_covar_line to visually explore consequences of covariates on the detection function. A regular call to plot() is first used to produce the histogram and average detection function line; subsequent calls to the new function with different values of the covariate of interest completes the plot.\n\nplot(etp.hr.cue, main=\"ETP dolphin survey\", showpoints=FALSE)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=1), col='red', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=2), col='blue', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=3), col='green', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=4), col='purple', lwd=2, lty=1)\nlegend(\"topright\", legend=c(\"Birds\",\"Splashes\",\"Unspecified\",\"Floating objects\"),\n       col=c(\"red\", \"blue\", \"green\", \"purple\"), lwd=2, title = \"Cue type\")\n\n\n\n\nDetection function with cue type as covariate."
  },
  {
    "objectID": "Pr9/countmodel-lines.html",
    "href": "Pr9/countmodel-lines.html",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting ‚ÄúWhat is the animal density?‚Äù Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodr√≠guez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable \\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#data-organisation",
    "href": "Pr9/countmodel-lines.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Data organisation",
    "text": "Data organisation\nIn Carlisle‚Äôs data set, sightings information is kept separate from information about each site. For our purposes, we will merge those together. In addition, some field names are changed for consistency with functions in the Distance package.\n\nnewsparrow <- merge(sparrowDetectionData, sparrowSiteData, by=\"siteID\", all=TRUE)\nnames(newsparrow) <- sub(\"observer\", \"obs\", names(newsparrow))\nnames(newsparrow) <- sub(\"dist\", \"distance\", names(newsparrow))\nnames(newsparrow) <- sub(\"length\", \"Effort\", names(newsparrow))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nAlthough not formally written as a set of functions, we bring to the front of the code arguments the user will need to change to alter to suite their needs. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height, shrubclass. The same predictors, with the exception of observer could be used to model the sparrow counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- FALSE        # survey conducted using lines or points\ndettrunc <- 100  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 500       # number of bootstrap replicates\nset.seed(19191)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#fit-detection-function",
    "href": "Pr9/countmodel-lines.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Brewer‚Äôs sparrow data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newsparrow, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.obare <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(bare),\n                transect=surveytype, quiet=TRUE)\nwoo.oht <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.oshrc <- ds(data=newsparrow, truncation=dettrunc, formula=~obs+shrubclass, \n             transect=surveytype, quiet=TRUE)\nwoo.ht <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(height), \n             transect=surveytype, quiet=TRUE)\nwoo.shrc <- ds(data=newsparrow, truncation=dettrunc, formula=~shrubclass, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newsparrow, truncation=dettrunc, formula=~scale(shrub), \n            transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newsparrow, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo.o,  woo.obare, woo.oht, woo.oshrc,\n                                 woo.ht, woo.shrc, woo.shrub, woo), digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\", row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~shrubclass\n0.511\n0.559\n0.025\n0.000\n\n\n\nHalf-normal\n~scale(height)\n0.492\n0.558\n0.025\n0.325\n\n\n\nHalf-normal\n~scale(shrub)\n0.476\n0.559\n0.025\n1.002\n\n\n\nHalf-normal\n~1\n0.469\n0.563\n0.025\n3.154\n\n\n\nHalf-normal\n~obs + scale(bare)\n0.659\n0.555\n0.025\n5.109\n\n\n\nHalf-normal\n~obs + shrubclass\n0.652\n0.556\n0.025\n5.533\n\n\n\nHalf-normal\n~obs + scale(height)\n0.658\n0.556\n0.025\n5.672\n\n\n\nHalf-normal\n~obs\n0.587\n0.559\n0.025\n7.148\n\n\n\n\n\nAll of the candidate models fit the Brewer‚Äôs sparrow line transect data. Also note that the estimate detection probability of all six models is the same to the third decimal. There is a small difference in AIC between the factor covariate shrubclass and the continuous covariate height. Simply for the purposes of demonstration, we will base our inference on the detection function that includes observer as a covariate. There is likely little effect of this model selection choice upon the ecological question of interest.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, lty=1, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lty=1, lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lty=1, lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, lty=1, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lty=1, lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:5,\n       lwd=2, lty=1, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\"))"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. For line transect example of Brewers Sparrows, Effort was measured in meters, but we wish to produce our estimates of density in numbers per hectare. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. In both cases, division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0‚Äôs constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nsitesadj <- effAreafn(woo.o, newsparrow, 10000, dettrunc, pointflag = FALSE)"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nFit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function. Consequently, the offset must also use the log transform.\nTo generalise the code, and recognising the same call to glm() will need to be made elsewhere in this analysis, we specify the GLM model we wish to fit as an object of type formula. This was specified in the Analysis parameters specification section above.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nkable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable¬†1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-1.0565\n0.1397\n-7.5649\n0\n\n\nshrub\n0.0789\n0.0096\n8.1836\n0"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#visualise",
    "href": "Pr9/countmodel-lines.html#visualise",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment.\nWe plot the estimated density against the continuous univariate predictor.\n\nsitesadj$density <- sitesadj$myCount / sitesadj$effArea \n# plot(sitesadj[, univarpredictor], sitesadj$density, pch=20,\n# firstplot <- recordPlot()"
  },
  {
    "objectID": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame sparrowSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Brewer‚Äôs sparrow density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table¬†1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table¬†1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(sparrowSiteData$siteID, replace=TRUE, size=length(unique(sparrowSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newsparrow[newsparrow$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = FALSE)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and Brewer‚Äôs sparrow density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(sparrowSiteData[ , univarpredictor]),\n                                 max(sparrowSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", linewidth=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', linewidth=1) +\n  annotate(geom=\"text\", x=20, y=0.6, label= b0label, size=5) +\n  annotate(geom=\"text\", x=20, y=0.3, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Brewer's sparrow density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram(binwidth = .005) +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=.025, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.13, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Brewer‚Äôs sparrow density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Brewer‚Äôs sparrow."
  },
  {
    "objectID": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density‚Äîlines",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. For the habitat characteristic shrub, there is a positive response of Brewer‚Äôs sparrow to increasing shrub cover.\n\nxlabel <- paste(\"Estimated slope of\", univarpredictor, \"and count relationship.\")\nhist.slope <- hist(slope.est, main=\"Sampling distribution of slope parameter\",\n                   xlab=xlabel) \ncibounds <- quantile(slope.est, probs = c(.025,.975), na.rm=TRUE)\nabline(v=cibounds, lty=3) \ntext(cibounds, max(hist.slope$counts), round(cibounds,3))\n\n\n\n\nSampling distribution of the parameter of interest for Brewer‚Äôs sparrow."
  },
  {
    "objectID": "Pr9/countmodel-points.html",
    "href": "Pr9/countmodel-points.html",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting ‚ÄúWhat is the animal density?‚Äù Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodr√≠guez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-points.html#data-organisation",
    "href": "Pr9/countmodel-points.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Data organisation",
    "text": "Data organisation\nNote there is no Effort field for these point count data.\n\nnewthrasher <- merge(thrasherDetectionData, thrasherSiteData, by=\"siteID\", all=TRUE)\nnames(newthrasher) <- sub(\"observer\", \"obs\", names(newthrasher))\nnames(newthrasher) <- sub(\"dist\", \"distance\", names(newthrasher))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nSpecification of run-specific parameters to analyse the Sage thrasher data set. Particularly note the logical value assigned to pointtransect. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height. The same predictors, with the exception of observer could be used to model the thrasher counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect <- TRUE        # survey conducted using lines or points\ndettrunc <- 170  # truncation for detection function\nmyglmmodel <- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot <- 100      # number of bootstrap replicates\nset.seed(7079)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-points.html#fit-detection-function",
    "href": "Pr9/countmodel-points.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Sage thrasher data set.\n\nsurveytype <- ifelse(pointtransect, \"point\", \"line\")\nwoo.o <- ds(data=newthrasher, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(shrub), \n           transect=surveytype, quiet=TRUE)\nwoo.herb <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(herb), \n           transect=surveytype, quiet=TRUE)\nwoo.height <- ds(data=newthrasher, truncation=dettrunc, formula=~scale(height), \n           transect=surveytype, quiet=TRUE)\nwoo <- ds(data=newthrasher, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo, woo.o, woo.shrub, woo.herb, woo.height),\n             digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\",\n             row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~obs\n0.009\n0.292\n0.039\n0.000\n\n\n\nHalf-normal\n~1\n0.037\n0.333\n0.034\n17.170\n\n\n\nHalf-normal\n~scale(herb)\n0.032\n0.331\n0.034\n17.893\n\n\n\nHalf-normal\n~scale(shrub)\n0.035\n0.331\n0.034\n17.901\n\n\n\nHalf-normal\n~scale(height)\n0.037\n0.333\n0.034\n19.168\n\n\n\n\n\nNone of the covariates contribute to fit of the detection function models for thrashers. Inference should be based upon the no covariate model (that passes the goodness of fit test), but for testing purposes, we will use the model with the observer covariate. Use of the observer covariate model will retain between-point variability in effective area; useful for testing purposes.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Sage thrasher\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nadd_df_covar_line(woo.o, data.frame(obs=\"obs6\"), col=\"coral\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:6,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\", \"coral\"))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. Division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0‚Äôs constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn <- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa <- NA\n  k <- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] <= truncation & !is.na(newdata[i,\"distance\"])) {\n      k <- k+1\n      newdata$Pa[i] <- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea <- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result <- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea <- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nThe function has an argument pointflag used to indicate whether point transect sampling was used. If so, the correct effective area calculations are performed.\n\nsitesadj <- effAreafn(woo.o, newthrasher, 10000, dettrunc, pointflag = pointtransect)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nAs for the line transect example, fit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function.\n\nunivarpredictor <- all.vars(myglmmodel)[2]\nglmmodel <- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum <- summary(glmmodel)\ntablecaption <- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nknitr::kable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable¬†1: GLM model output\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-2.3977\n0.8794\n-2.7266\n0.0064\n\n\nshrub\n0.0818\n0.0419\n1.9538\n0.0507"
  },
  {
    "objectID": "Pr9/countmodel-points.html#visualise",
    "href": "Pr9/countmodel-points.html#visualise",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment."
  },
  {
    "objectID": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est <- vector(\"numeric\", length=nboot)\nslope.est <- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame thrasherSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Sage thrasher density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table¬†1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table¬†1.\n\nfor (theboot in 1:nboot) {\n  newdetects <- data.frame() \n  bob <- sample(thrasherSiteData$siteID, replace=TRUE, size=length(unique(thrasherSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite <- bob[bootsite] \n    glob <- newthrasher[newthrasher$siteID==thissite, ]\n    glob$siteID <- sprintf(\"rep%02d\", bootsite)\n    newdetects <- rbind(newdetects, glob)  \n    }\n  newdetects <- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod <- ds(data=newdetects, truncation=dettrunc, formula=~obs, transect=surveytype, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj <- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = pointtransect)\n#   refit the GLM for this bootstrap replicate\n  glmresult <- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] <- coef(glmresult)[1]\n  slope.est[theboot] <- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and bird density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData <- data.frame(predictor=seq(min(thrasherSiteData[ , univarpredictor]),\n                                 max(thrasherSiteData[, univarpredictor]), length.out=50)) \norig.fit <- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform <- NULL\nfor (i in 1:nboot) {\n  mypredict <- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform <- c(longform, mypredict)\n}\nbig.df <- data.frame(predict=longform)\nbig.df$shrub <- predData$predictor\nbig.df$group <- rep(1:nboot, each=length(predData$predictor))\nalpha <- 0.05\n# point-wise confidence intervals\nquants <- big.df %>% \n  group_by(shrub) %>% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %>% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label <- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label <- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 <- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=24, y=1.5, label= b0label, size=5) +\n  annotate(geom=\"text\", x=24, y=1.4, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Sage thrasher density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds <- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 <- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.18, y=11, label=round(bounds[2],3)) \ncomplete <- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Sage Thrasher density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Sage Thrasher."
  },
  {
    "objectID": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density‚Äìpoints",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. In the case of the Sage Thrasher, the species appears to exhibit little response to the presence of shrub cover during this study. Note that the range of shrub cover is not extensive (~16% to ~26%). We can make no inference regarding the density:habitat relationship outside this range in shrub cover."
  },
  {
    "objectID": "Pr9/multi-analysis.html",
    "href": "Pr9/multi-analysis.html",
    "title": "Multispecies and multisession distance sampling analysis",
    "section": "",
    "text": "Survey design\nBuckland‚Äôs design consisted of visiting each of the 19 transects in his study twice. To examine some of the errors that can arise from improper analysis, I choose to treat the two visits as strata for the express purpose of generating stratum (visit) -specific density estimates. Density estimates reported in Buckland (2006) are in units of birds \\(\\cdot hectare^{-1}\\).\n\nbirds$Region.Label <- birds$visit\ncu <- convert_units(\"meter\", \"kilometer\", \"hectare\")\n\n\n\nAnalysis of only one species (incorrectly)\nThe direct approach to producing a density estimate for the chaffinch would be to subset the original data frame and use the species-specific data frame for analysis. Begin by performing the subset operation.\n\nchaf <- birds[birds$species==\"c\", ]\n\nWhen the data are subset, the integrity of the survey design is not preserved. A simple frequency table of the species-specific data frame flags up a number of transect/visit combinations where no chaffinches were detected. The result is that the subset data frame suggests 3 of the 19 transects lacked chaffinch detections on the first visit and one of the 19 transects lacked chaffinch detections on the second visit. This revelation, in itself, causes no problems for our estimate of density of chaffinches.\n\ndetects <- table(chaf$Sample.Label, chaf$visit)\ndetects <- as.data.frame(detects)\nnames(detects) <- c(\"Transect\", \"Visit\", \"Detections\")\ndetects$Detections <- cell_spec(detects$Detections, \n                          background = ifelse(detects$Detections==0, \"red\", \"white\"))\nknitr::kable(detects, escape=FALSE) %>%\n  kable_paper(full_width=FALSE)\n\n\n\n \n  \n    Transect \n    Visit \n    Detections \n  \n \n\n  \n    1 \n    1 \n    3 \n  \n  \n    2 \n    1 \n    3 \n  \n  \n    3 \n    1 \n    4 \n  \n  \n    4 \n    1 \n    3 \n  \n  \n    5 \n    1 \n    5 \n  \n  \n    6 \n    1 \n    4 \n  \n  \n    7 \n    1 \n    2 \n  \n  \n    8 \n    1 \n    0 \n  \n  \n    9 \n    1 \n    1 \n  \n  \n    10 \n    1 \n    1 \n  \n  \n    11 \n    1 \n    0 \n  \n  \n    13 \n    1 \n    1 \n  \n  \n    14 \n    1 \n    1 \n  \n  \n    15 \n    1 \n    3 \n  \n  \n    16 \n    1 \n    2 \n  \n  \n    17 \n    1 \n    3 \n  \n  \n    18 \n    1 \n    3 \n  \n  \n    19 \n    1 \n    0 \n  \n  \n    1 \n    2 \n    1 \n  \n  \n    2 \n    2 \n    4 \n  \n  \n    3 \n    2 \n    3 \n  \n  \n    4 \n    2 \n    2 \n  \n  \n    5 \n    2 \n    4 \n  \n  \n    6 \n    2 \n    3 \n  \n  \n    7 \n    2 \n    3 \n  \n  \n    8 \n    2 \n    1 \n  \n  \n    9 \n    2 \n    0 \n  \n  \n    10 \n    2 \n    2 \n  \n  \n    11 \n    2 \n    1 \n  \n  \n    13 \n    2 \n    1 \n  \n  \n    14 \n    2 \n    1 \n  \n  \n    15 \n    2 \n    1 \n  \n  \n    16 \n    2 \n    1 \n  \n  \n    17 \n    2 \n    1 \n  \n  \n    18 \n    2 \n    4 \n  \n  \n    19 \n    2 \n    1 \n  \n\n\n\n\n\nHowever, there is a problem hidden within the table above. Transect 12 does not appear in the table because there were no detections of chaffinches on either visit. Consequently, there were 4 transects without chaffinches on the first visit and 2 transects without chaffinches on the second visit, rather than the 3 transects and 1 transect you might mistakenly conclude do not have chaffinch detections if you relied completely upon the table.\nLet‚Äôs see what the ds() function thinks about the survey effort using information from the species-specific data frame.\n\nchaf.wrong <- ds(chaf, key=\"hn\", convert_units = cu, truncation=95, formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(chaf.wrong$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    82.061 \n    4.319 \n    39 \n    15 \n    9.029868 \n    1.1159303 \n    0.1235821 \n  \n  \n    2 \n    33.2 \n    83.562 \n    4.398 \n    34 \n    17 \n    7.730787 \n    0.9798153 \n    0.1267420 \n  \n  \n    Total \n    66.4 \n    165.623 \n    8.717 \n    73 \n    32 \n    8.380327 \n    0.7425191 \n    0.0886026 \n  \n\n\n\n\n\nExamine the column labelled k (the number of transects) for each of the visits. Rather than the 19 transects that were surveyed on each visit, the ds() function erroneously believes there were only 15 transects surveyed on the first visit and 17 transects surveyed on the second visit.\nNote also the number of detections per kilometer; roughly 9 on the first visit and 7.7 on the second visit. These encounter rates exclude kilometers of effort on transects where there were no detections. We will return to this comparison later.\n\n\nUse explicit data hierarchy\n\n\n\n\n\n\nDescribing the survey design to ds\n\n\n\nAdditional arguments can be passed to ds() to resolve this problem. Consulting the ds() documentation\n\nregion_table data.frame with two columns:\n\nRegion.Label label for the region\nArea area of the region\nregion_table has one row for each stratum. If there is no stratification then region_table has one entry with Area corresponding to the total survey area. If Area is omitted density estimates only are produced.\n\nsample_table data.frame mapping the regions to the samples (i.e.¬†transects). There are three columns:\n\nSample.Label label for the sample\nRegion.Label label for the region that the sample belongs to.\nEffort the effort expended in that sample (e.g.¬†transect length).\n\n\n\n\nThis analysis that produces erroneous results can be remedied by explicitly letting the ds() function know about the study design; specifically, how many strata and the number of transects within each stratum (and associated transect lengths).\nConstruct the region table and sample table showing the two strata with equal areas and each labelled transect (of given length) is repeated two times.\n\nbirds.regiontable <- data.frame(Region.Label=as.factor(c(1,2)), Area=c(33.2,33.2))\nbirds.sampletable <- data.frame(Region.Label=as.factor(rep(c(1,2), each=19)),\n                                Sample.Label=rep(1:19, times=2),\n                                Effort=c(0.208, 0.401, 0.401, 0.299, 0.350,\n                                         0.401, 0.393, 0.405, 0.385, 0.204,\n                                         0.039, 0.047, 0.204, 0.271, 0.236,\n                                         0.189, 0.177, 0.200, 0.020))\n\n\n\nSimple detection function model\nThe chaffinch analysis is performed again, this time supplying the region_table and sample_table information to ds(). The correct number of transects (19) sampled on both visits (even though chaffinch was not detected on 4 transects on visit 1 and 2 transects on visit 2) is now recognised. Hence, the use of region table and sample table solves the problem of effort miscalculation if a species is not detected on all transects.\n\ntr <- 95   # as per Buckland (2006)\nonlycf <- ds(data=birds[birds$species==\"c\", ], \n             region_table = birds.regiontable,\n             sample_table = birds.sampletable,\n             trunc=tr, convert_units=cu, key=\"hn\", formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(onlycf$dht$individuals$summary) %>%\n  kable_paper(full_width=FALSE) %>%\n  column_spec(6, background=\"salmon\") %>%\n  column_spec(7, background=\"steelblue\")\n\n\n\n \n  \n    Region \n    Area \n    CoveredArea \n    Effort \n    n \n    k \n    ER \n    se.ER \n    cv.ER \n  \n \n\n  \n    1 \n    33.2 \n    91.77 \n    4.83 \n    39 \n    19 \n    8.074534 \n    1.2196305 \n    0.1510465 \n  \n  \n    2 \n    33.2 \n    91.77 \n    4.83 \n    34 \n    19 \n    7.039338 \n    1.0612781 \n    0.1507639 \n  \n  \n    Total \n    66.4 \n    183.54 \n    9.66 \n    73 \n    38 \n    7.556936 \n    0.8083641 \n    0.1069698 \n  \n\n\n\n\n\n\n\nConsequence of incorrect analysis\nTo drive home the consequence of failing to properly specify the survey effort, contrast the encounter rate for the two visits from the incorrect calculations above (9.0 and 7.7 respectively), with the correct calculation (8.1 and 7.0 respectively). The number of transects is incorrect with the knock-on effect of effort being incorrect. If effort is incorrect then so too is covered area.\nThe ripple effect from incomplete information about the survey design results in positively biased estimates of density.\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345‚Äì357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2"
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "Lecture slides for 19 January 2023"
  },
  {
    "objectID": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "href": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "title": "Multipliers practical 9",
    "section": "Multipliers and indirect surveys",
    "text": "Multipliers and indirect surveys\nIt is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship‚Äìeffort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\n\n\nPhoto by Jamshaid Mughal on Unsplash"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html",
    "href": "Pr9/Pr9-instructions.html",
    "title": "Analyses using multipliers",
    "section": "",
    "text": "We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets 2 and 3 deal with instantaneous cues and so only cue rate needs to be taken into account."
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#getting-started",
    "href": "Pr9/Pr9-instructions.html#getting-started",
    "title": "Analyses using multipliers",
    "section": "Getting started",
    "text": "Getting started\nThese data (called sikadeer) are available in the Distance package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor <- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "href": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "title": "Analyses using multipliers",
    "section": "Fit detection function to dung pellets",
    "text": "Fit detection function to dung pellets\nFit the usual series of models (i.e.¬†half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don‚Äôt spend too long on this). Call your model deer.df. This detection function will be used to obtain \\(\\hat D_{\\textrm{pellet groups}}\\).\nHave a look at the Summary statistics for this model - what do you notice about the allocation of search effort in each woodland?"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#multipliers",
    "href": "Pr9/Pr9-instructions.html#multipliers",
    "title": "Analyses using multipliers",
    "section": "Multipliers",
    "text": "Multipliers\nThe next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.\nData to calculate this has been collected in the file IntroDS_9.1.csv that can be read from the Github internet repository. Following code comes from Meredith (2017).\n\nMIKE.persistence <- function(DATA) {\n  \n#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data \n#  Input: data frame with at least two columns:\n#         DAYS - calendar day on which dung status was observed\n#         STATE - dung status: 1-intact, 0-decayed\n#  Output: point estimate, standard error and CV of mean persistence time\n#\n#  Attribution: code from Mike Meredith website: \n#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm\n#   Citing: CITES elephant protocol\n#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf\n  \n  ##   Fit logistic regression model to STATE on DAYS, extract coefficients\n  dung.glm <- glm(STATE ~ DAYS, data=DATA, family=binomial(link = \"logit\"))\n  betas <- coefficients(dung.glm)\n  ##   Calculate mean persistence time\n  mean.decay <- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]\n  ## Calculate the variance of the estimate\n  vcovar <- vcov(dung.glm)\n  var0 <- vcovar[1,1]  # variance of beta0\n  var1 <- vcovar[2,2]  # variance of beta1\n  covar <- vcovar[2,1] # covariance\n  deriv0 <- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]\n  deriv1 <- -mean.decay/betas[2]\n  var.mean <- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2\n  ## Calculate the SE and CV and return\n  se.mean <- sqrt(var.mean)\n  cv.mean <- se.mean/mean.decay\n  out <- c(mean.decay, se.mean, 100*cv.mean)\n  names(out) <- c(\"Mean persistence time\", \"SE\", \"%CV\")\n  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab=\"Days since initiation\",\n       ylab=\"Dung persists (yes=1)\",\n       main=\"Eight dung piles revisited over time\")\n  curve(predict(dung.glm, data.frame(DAYS=x), type=\"resp\"), add=TRUE)\n  abline(v=mean.decay, lwd=2, lty=3)\n  return(out)\n}\ndecay <- read.csv(\"https://raw.githubusercontent.com/erex/Oct-Quarto/main/Pr9/IntroDS_9.1.csv\")\npersistence.time <- MIKE.persistence(decay)\nprint(persistence.time)\n\nRunning the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been ‚Äòjittered‚Äô to avoid over-plotting.\nAn estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.\nAs stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:\n\ncreation contains estimates of the dung production rate and associated standard error\ndecay contains the dung decay rate and associated standard error where XX and YY are the estimates you obtained from the dung decay rate analysis.\n\n\n# Create list of multipliers\nmult <- list(creation = data.frame(rate=25, SE=0),\n#             decay    = data.frame(rate=XX, SE=YY))\nprint(mult)\n\nThe final step is to use these multipliers to convert \\(\\hat D_{\\textrm{pellet groups}}\\) to \\(\\hat D_{\\textrm{deer}}\\) (as in the equations above) - for this we need to employ the dht2 function. In the command below the multipliers= argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:\n\nstrat_formula=~Region.Label is specified to take into account the design (i.e.¬†different woodlands or blocks).\nstratification=\"effort_sum\" is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.\ndeer.df is the detection function you have fitted.\n\n\n# Weight by effort because we have repeats\ndeer.ests <- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                 convert_units=conversion.factor, multipliers=mult, \n                 stratification=\"effort_sum\", total_area=13.9)\nprint(deer.ests)\n\nThe function dht2 also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata."
  },
  {
    "objectID": "Pr9/Prac9_solution.html",
    "href": "Pr9/Prac9_solution.html",
    "title": "Analysis with multipliers solution",
    "section": "",
    "text": "Dung survey of deer\nReturning to the data described in (Marques et al., 2001), the following code loads the relevant packages and data. The perpendicular distances are measured in centimetres, effort along the transects measured in kilometres and areas in square kilometres.\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor <- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")\n\nHere we did not perform a comprehensive examination of fitting a detection function to the detected pellet groups, however, as a general guideline, we truncated the longest 10% perpendicular distances.\n\ndeer.df <- ds(sikadeer, key=\"hn\", truncation=\"10%\", convert_units = conversion.factor)\nplot(deer.df)\n\n\n\nprint(deer.df$dht$individuals$summary)\n\n  Region Area CoveredArea Effort    n  k        ER      se.ER     cv.ER\n1      A 13.9    0.005950   1.70 1217 13 715.88234 119.918872 0.1675120\n2      B 10.3    0.003850   1.10  396 10 359.99999  86.859289 0.2412758\n3      C  8.6    0.001575   0.45   17  3  37.77778   8.521202 0.2255612\n4      E  8.0    0.002975   0.85   30  5  35.29412  16.568939 0.4694533\n5      F 14.0    0.000700   0.20   29  1 145.00000   0.000000 0.0000000\n6      G 15.2    0.001400   0.40   32  3  80.00000  39.686269 0.4960784\n7      H 11.3    0.000700   0.20    3  1  15.00000   0.000000 0.0000000\n8      J  9.6    0.000350   0.10    7  1  70.00000   0.000000 0.0000000\n9  Total 90.9    0.017500   5.00 1731 37 201.90876   0.000000 0.0000000\n\n\nThe summary above shows that in blocks F, H and J there was only one transect and, as a consequence, it is not possible to calculate a variance empirically for the encounter rate in those blocks.\n\n\nEstimating decay rate from data\nA paper by Laing et al. (2003) describes field protocol for collecting data to estimate the mean persistence time of dung or nests to be used as multipliers. The code segment shown earlier analyses a file of such data via logistic regression to produce an estimate of mean persistence time and its associated uncertainty.\n\n\n\n\n\nMean persistence time                    SE                   %CV \n           163.396748             14.226998              8.707026 \n\n\nUsing the output from calling the MIKE.persistence function, the multipliers can be specified:\n\n# Create list of multipliers\nmult <- list(creation = data.frame(rate=25, SE=0),\n             decay    = data.frame(rate=163, SE=14))\nprint(mult)\n\n$creation\n  rate SE\n1   25  0\n\n$decay\n  rate SE\n1  163 14\n\ndeer_ests <- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                  convert_units=conversion.factor, multipliers=mult, \n                  stratification=\"effort_sum\", total_area = 100)\nprint(deer_ests, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : effort_sum \nVariance       : R2, n/L \nMultipliers    : creation, decay \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label  Area CoveredArea Effort    n  k      ER   se.ER cv.ER\n            A  13.9    0.005950   1.70 1217 13 715.882 119.919 0.168\n            B  10.3    0.003850   1.10  396 10 360.000  86.859 0.241\n            C   8.6    0.001575   0.45   17  3  37.778   8.521 0.226\n            E   8.0    0.002975   0.85   30  5  35.294  16.569 0.469\n            F  14.0    0.000700   0.20   29  1 145.000   0.000 0.000\n            G  15.2    0.001400   0.40   32  3  80.000  39.686 0.496\n            H  11.3    0.000700   0.20    3  1  15.000   0.000 0.000\n            J   9.6    0.000350   0.10    7  1  70.000   0.000 0.000\n        Total 100.0    0.017500   5.00 1731 37 346.200  45.234 0.131\n\nAbundance estimates:\n Region.Label Estimate      se    cv  LCI  UCI        df\n            A     1027 197.474 0.192  691 1527    20.797\n            B      383  99.171 0.259  220  667    11.955\n            C       34   8.200 0.244   15   75     2.759\n            E       29  13.959 0.479    9   99     4.329\n            F      210  19.752 0.094  174  252 60314.199\n            G      126  63.399 0.505   18  858     2.147\n            H       18   1.649 0.094   15   21 60314.199\n            J       69   6.539 0.094   58   83 60314.199\n        Total     3575 575.860 0.161 2560 4990    20.215\n\nComponent percentages of variance:\n Region.Label Detection    ER Multipliers\n            A      4.07 75.96       19.97\n            B      2.24 86.76       10.99\n            C      2.52 85.14       12.34\n            E      0.66 96.13        3.22\n            F     16.93  0.00       83.07\n            G      0.59 96.52        2.89\n            H     16.93  0.00       83.07\n            J     16.93  0.00       83.07\n        Total      8.09 91.91        0.00\n\n\nThere are a few things to notice:\n\noverall estimate of density\n\nmost effort took place in woodland A where deer density was high. Therefore, the overall estimate is between the estimated density in woodland A and the lower densities in the other woodlands.\n\ncomponents of variance\n\nwe now have uncertainty associated with the encounter rate, detection function and decay rate (note there was no uncertainty associated with the production rate) and so the components of variation for all three components are provided.\n\n\nIn woodland A, there were 13 transects on which over 1,200 pellet groups were detected: uncertainty in the estimated density was 19% and the variance components were apportioned as detection probability 4%, encounter rate 76% and multipliers 20%.\nIn woodland E, there were 5 transects and 30 pellet groups resulting in a coefficient of variation (CV) of 48%: the variance components were apportioned as detection probability 0.7%, encounter rate 96% and multipliers 3%.\nIn woodland F only a single transect was placed and the CV of density of 9% was apportioned as detection probability 17% and multipliers 83%. Do you trust this assessment of uncertainty in the density of deer in this woodland? We are missing a component of variation because we were negligent in placing only a single transect in this woodland and so are left to ‚Äòassume‚Äô there is no variability in encounter rate in this woodland.\nBy the same token, we are left to assume there is no variability in production rates between deer because we have not included a measure of uncertainty in this facet of our analysis.\n\n\nCue counting survey of whales\n\ndata(CueCountingExample)\nhead(CueCountingExample, n=3)\n\n  Region.Label  Area Sample.Label Cue.rate Cue.rate.SE Cue.rate.df object distance\n1            B 85000         2.18       25           5           1     NA       NA\n2            B 85000         2.19       25           5           1     NA       NA\n3            B 85000         2.20       25           5           1     NA       NA\n  Sample.Fraction Sample.Fraction.SE Search.time bss   sp size Study.Area\n1             0.5                  0   0.1333333  NA <NA>   NA     whales\n2             0.5                  0   0.1000000  NA <NA>   NA     whales\n3             0.5                  0   0.4333333  NA <NA>   NA     whales\n\nCueCountingExample$Effort <- CueCountingExample$Search.time\ncuerates <- CueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\")]\ncuerates <- unique(cuerates)\nnames(cuerates) <- c(\"rate\", \"SE\", \"df\")\nmult <- list(creation=cuerates)\nprint(mult)\n\n$creation\n  rate SE df\n1   25  5  1\n\n\nThe estimated cue rate, \\(\\hat \\nu\\), is 25 cues per unit time (per hour in this case). Its standard error is 5, therefore the CV of cue rate is \\(5/25 = 0.2\\) (20%).\n\n# Tidy up data by removing surplus columns\nCueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\", \"Sample.Fraction\", \n                       \"Sample.Fraction.SE\")] <- list(NULL)\ntrunc <- 1.2\nwhale.df.hn <- ds(CueCountingExample, key=\"hn\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nwhale.df.hr <- ds(CueCountingExample, key=\"hr\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nknitr::kable(summarize_ds_models(whale.df.hn, whale.df.hr), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~1\n0.810\n0.239\n0.043\n0.00\n\n\n\nHazard-rate\n~1\n0.747\n0.281\n0.065\n3.07\n\n\n\n\n\nHalf the circle (point transect) was searched and so the sampling fraction \\(\\phi /2\\pi = 0.5\\). Therefore, \\(\\phi = \\pi\\) (\\(\\phi\\) must be in radians).\nThe following commands obtain density estimates assuming no stratification (strat_formula=~1).\n\nwhale.est.hn <- dht2(whale.df.hn, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hn, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    13654 5263.071 0.385 6112 30500 13.058\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      21.7 51.38       26.92\n\nwhale.est.hr <- dht2(whale.df.hr, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hr, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    11590 4777.034 0.412 5017 26774 16.593\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total     31.52 44.93       23.54\n\n\nA half normal detection function was chosen and whale abundance was estimated to be 13,654 whales with a 95% confidence interval (6,112: 30,500).\nNote the large difference between the half normal estimate and the estimate from the hazard rate model, which is 11,590 whales, with 95% confidence interval (5,017; 26773). Remember that the key parameter in a cue counting analysis is \\(h(0)\\), the slope of the fitted pdf to the observed data at distance zero. The difference between the estimates for the different key function is the difference between these slopes for the two models (Fig. 2):\n\nplot(whale.df.hn, pdf=TRUE, main=\"Half normal\")\nplot(whale.df.hr, pdf=TRUE, main=\"Hazard rate\")\n\n\n\n\n\n\nHalf normal PDF\n\n\n\n\n\n\n\nHazard rate PDF\n\n\n\n\n\n\nCue counting estimates of detection probability are more volatile than those from line transect surveys, because on a cue counting survey you have few data where you need it most to estimate \\(h(0)\\) - namely at distances close to zero. As a consequence, cue-counting surveys require higher cue sample size for reliable estimation than samples of animals for line transect surveys.\nDon‚Äôt worry too much about the apparent lack of fit in the first interval, or two, in Figure 2 - remember the sample size is very small in these intervals. Use the plot above and the goodness-of-fit statistics to guide you about the fit of your model.\n\n\nCue counting survey of songbirds (optional)\nAnalysis of the cue count data of winter wrens described by Buckland (2006).\n\ndata(wren_cuecount)\ncuerate <- unique(wren_cuecount[ , c(\"Cue.rate\",\"Cue.rate.SE\")])\nnames(cuerate) <- c(\"rate\", \"SE\")\nmult <- list(creation=cuerate)\nprint(mult)\n\n$creation\n    rate     SE\n1 1.4558 0.2428\n\n# Search time is the effort - this is 2 * 5min visits\nwren_cuecount$Effort <- wren_cuecount$Search.time\nw3.hr <- ds(wren_cuecount, transect=\"point\", key=\"hr\", adjustment=NULL, truncation=92.5)\n\nThe sampling fraction for these data will be 1 because the full circle around the observer was searched.\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\nw3.est <- dht2(w3.hr, flatfile=wren_cuecount, strat_formula=~1,\n               multipliers=mult, convert_units=conversion.factor)\n# NB \"Effort\" here is sum(Search.time) in minutes\n# NB \"CoveredArea\" here is pi * w^2 * sum(Search.time)\nprint(w3.est, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total 33.2    860.1681    320 765 32 2.391 0.236 0.099\n\nDensity estimates:\n .Label Estimate    se  cv    LCI    UCI      df\n  Total   1.2092 0.242 0.2 0.8195 1.7843 522.541\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      6.14 24.33       69.54\n\n\nNote the large proportion of the uncertainty in winter wren density stems from variability in cue (song) rate. Analyses of the cue count data are necessarily rather subjective as the data show substantial over-dispersion (a single bird may give many song bursts all from the same location during a five minute count). In this circumstance, goodness-of-fit tests are misleading and care must be taken not to over-fit the data (i.e.¬†fit a complicated detection function).\n\nplot(w3.hr, pdf=TRUE, main=\"Cue distances of winter wren.\")\ngof_ds(w3.hr)\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 1.70062 p-value = 6.04794e-05\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345‚Äì357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2\n\n\nLaing, S. E., Buckland, S. T., Burn, R. W., Lambie, D., & Amphlett, A. (2003). Dung and nest surveys: Estimating decay rate. Journal of Applied Ecology, 40, 1102‚Äì1111. https://doi.org/10.1111/j.1365-2664.2003.00861.x\n\n\nMarques, F. F. C., Buckland, S. T., Goffin, D., Dixon, C. E., Borchers, D. L., Mayle, B. A., & Peace, A. J. (2001). Estimating deer abundance from line transect surveys of dung: sika deer in southern Scotland. Journal of Applied Ecology, 38(2), 349‚Äì363. https://doi.org/10.1046/j.1365-2664.2001.00584.x"
  }
]