[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is his first step in this adventure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling lab book",
    "section": "",
    "text": "The materials are intended to be experiential rather than used as a reference. Exercises are paced to bring you up to speed with the fundamentals of distance sampling; how this method of population assessment differs from other forms of population sampling. This leads to the introduction of a detection function and ways to model it.\nFurther topics of model selection, assessing precision of population estimates and collection and analysis of point transect data round out the first half of the materials. At this point, you will have acquired sufficient experience to analyse basic distance sampling data.\nThe second half of the material exposes you to slightly more advanced concepts: design of distance sampling surveys, including the use of stratification. Also discussed are analytical methods associated with stratified surveys. This gives way to including predictors other than distance in modelling the detection process. Finally multipliers and methods associated with indirect animal surveys are introduced.\nAs well as the exercises and their solutions, there are numerous supplements, touching upon topics or demonstrating issues related to the analysis of distance sampling data. The supplements are not fundamental to successfully employing distance sampling methods, however, the more you understand tools such as distance sampling (via these supplements), the better you will be able to employ such methods."
  },
  {
    "objectID": "Pr1/Pr1-instructions.html",
    "href": "Pr1/Pr1-instructions.html",
    "title": "Line transect detection function fitting",
    "section": "",
    "text": "Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e. the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF provided, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e. the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\]\n\\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]"
  },
  {
    "objectID": "Pr1/sampling.html",
    "href": "Pr1/sampling.html",
    "title": "Sampling practical 1",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region.\n\nPhoto by Shelby Cohron on Unsplash"
  },
  {
    "objectID": "Pr2/detnfns.html",
    "href": "Pr2/detnfns.html",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit.\nPhoto by Freysteinn G. Jonsson on Unsplash"
  },
  {
    "objectID": "Pr2/Pr2-instructions.html",
    "href": "Pr2/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "Objectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it with this pair of commands:\n\ninstall.packages(remotes)\nremotes::install_github(\"DistanceDevelopment/Distance\")\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn <- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn, nc=8)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos <- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm <- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "Pr2/truncation-decisions.html",
    "href": "Pr2/truncation-decisions.html",
    "title": "Effective of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let’s see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment <- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result <- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this <- paste0(i-1,\"%\")\n    m <- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] <- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "Pr2/truncation-decisions.html#duck-nest-result",
    "href": "Pr2/truncation-decisions.html#duck-nest-result",
    "title": "Effective of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange <- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data."
  },
  {
    "objectID": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "href": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effective of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc <- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data."
  },
  {
    "objectID": "Pr3/criticism.html",
    "href": "Pr3/criticism.html",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Rmarkdown (.rmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.\n\nPhoto by Blake Cheek on Unsplash"
  },
  {
    "objectID": "Pr3/modelsel-demo.html",
    "href": "Pr3/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003)."
  },
  {
    "objectID": "Pr3/modelsel-demo.html#half-normal-cosine",
    "href": "Pr3/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos <- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2805.973\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --> ID0\n   D --> ID1\n   D --> ID2\n   ID0(hn0AIC=2817)\n   ID1(hn1AIC=2806)\n   ID2(hn2AIC=2808)\n   FIN(hn1)\n   ID0 --> FIN\n   ID1 --> FIN\n   ID2 --> FIN"
  },
  {
    "objectID": "Pr3/modelsel-demo.html#uniform-cosine",
    "href": "Pr3/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos <- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --> ID1\n   D --> ID2\n   D --> ID3\n   D --> ID4\n   ID1(unif1AIC=2811)\n   ID2(unif2AIC=2808)\n   ID3(unif3AIC=2807)\n   ID4(unif4AIC=2808)\n   FIN(unif3)\n   ID1 --> FIN\n   ID2 --> FIN\n   ID3 --> FIN\n   ID4 --> FIN"
  },
  {
    "objectID": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "href": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos <- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.47\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --> ID0\n   D --> ID1\n   ID0(hr0AIC=2805)\n   ID1(hr1AIC=2807)\n   FIN(hr0)\n   ID0 --> FIN\n   ID1 --> FIN\n\n\n\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms"
  },
  {
    "objectID": "Pr3/Pr3-instructions.html",
    "href": "Pr3/Pr3-instructions.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#accessing-the-data",
    "href": "Pr3/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#truncation",
    "href": "Pr3/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn <- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m <- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\n\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per <- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)"
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#exploring-different-models",
    "href": "Pr3/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos <- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins <- seq(from=0, to=80, by=10)\nconversion.factor <- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin <- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)"
  },
  {
    "objectID": "Pr4/left-truncation.html",
    "href": "Pr4/left-truncation.html",
    "title": "Supplement Analysis options for left truncation",
    "section": "",
    "text": "References\n\nAlldredge, J. R., & Gates, C. E. (1985). Line transect estimators for left-truncated distributions. Biometrics, 41(1), 273–280. https://doi.org/10.2307/2530663\n\n\nBuckland, S. T., Anderson, D. R., Burnham, K. P., Laake, J. L., Borchers, D. L., & Thomas, L. (2001). Introduction to distance sampling: Estimating abundance of biological populations. Oxford, New York: Oxford University Press."
  },
  {
    "objectID": "Pr4/Pr4-instructions.html",
    "href": "Pr4/Pr4-instructions.html",
    "title": "Variance estimation for systematic survey designs",
    "section": "",
    "text": "We will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g. the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strongtrend in density. The systematically placed search strips areshaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon’t forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor <- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn <- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nsysvar2.hn$dht$individuals$D\nsysvar2.hn$dht$individuals$N\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e. 1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot <- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‘O2’ (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label <- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 <- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e. transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibita trend in density. The systematically placed search strips areshaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‘O2’ estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label <- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn <- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nsysvar1.hn$dht$individuals$D\nsysvar1.hn$dht$individuals$N\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 <- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "Pr4/precision.html",
    "href": "Pr4/precision.html",
    "title": "Precision practical 4",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set.\n\nPhoto by Andrea Sonda on Unsplash"
  },
  {
    "objectID": "Pr5/animal_distribution.html",
    "href": "Pr5/animal_distribution.html",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "",
    "text": "Simulation of line transect survey, I won’t show the simulation details, but here is the distribution of animals (3000) and placement of 40 transects in the study area.\nFrom this survey, sample pairs of transects to visually examine the uniformity of animal distances."
  },
  {
    "objectID": "Pr5/animal_distribution.html#two-transects",
    "href": "Pr5/animal_distribution.html#two-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Two transects",
    "text": "Two transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#five-transects",
    "href": "Pr5/animal_distribution.html#five-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Five transects",
    "text": "Five transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#ten-transects",
    "href": "Pr5/animal_distribution.html#ten-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Ten transects",
    "text": "Ten transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#fifteen-transects",
    "href": "Pr5/animal_distribution.html#fifteen-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Fifteen transects",
    "text": "Fifteen transects"
  },
  {
    "objectID": "Pr5/animal_distribution.html#twenty-transects",
    "href": "Pr5/animal_distribution.html#twenty-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Twenty transects",
    "text": "Twenty transects"
  },
  {
    "objectID": "Pr5/points.html",
    "href": "Pr5/points.html",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method.\n\nPhoto by Vincent van Zalinge on Unsplash"
  },
  {
    "objectID": "Pr5/Pr5-instructions.html",
    "href": "Pr5/Pr5-instructions.html",
    "title": "Point transect sampling",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds."
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#probability-density-function",
    "href": "Pr5/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)"
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method."
  },
  {
    "objectID": "Pr6/design.html",
    "href": "Pr6/design.html",
    "title": "Survey design practical 6",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with this R package, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area.\nPhoto by ray rui on Unsplash"
  },
  {
    "objectID": "Pr6/effort-and-power.html",
    "href": "Pr6/effort-and-power.html",
    "title": "Exercise 6How much survey effort is needed?",
    "section": "",
    "text": "Demonstration Effort needed to achieve objective"
  },
  {
    "objectID": "Pr6/effort-and-power.html#cv-graph",
    "href": "Pr6/effort-and-power.html#cv-graph",
    "title": "Exercise 6How much survey effort is needed?",
    "section": "CV graph",
    "text": "CV graph\nThe result tab showing the relationship between cumulative population change and necessary CV is shown in the CV graph tab. There is a positive relationship between these: greater population loss requires less precision to detect; or small population change requires more precise density estimates. The red dot and horizontal dotted line indicate the precision of annual estimates to detect the specified cumulative change over the specified number of annual surveys with the desired power. For these calculations, the encounter rate from the pilot survey plays no role."
  },
  {
    "objectID": "Pr6/effort-and-power.html#effort-graph",
    "href": "Pr6/effort-and-power.html#effort-graph",
    "title": "Exercise 6How much survey effort is needed?",
    "section": "Effort graph",
    "text": "Effort graph\nThe result tab labelled Effort graph brings information from the pilot survey into the calculations. The CV graph indicates the necessary precision to achieve the desired results, this CV is fed into the formula from Buckland et al. (2015) to estimate the amount of survey effort to be expended annually to achieve the precision derived from the power calculations.\nThe red ball and horizontal dotted line now indicates the amount of effort needed to achieve the specified objectives. Small changes in abundance require exponentially larger amounts of annual survey effort to detect. The steepness of that exponential curve is less extreme when encounter rates are large."
  },
  {
    "objectID": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "href": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "title": "Exercise 6How much survey effort is needed?",
    "section": "Numerical result of power calculation",
    "text": "Numerical result of power calculation\nThe final result tab simply provides a single numerical solution to the required CV and effort necessary to achieve that CV for the specified combination of cumulative change, number of annual surveys, power and pilot study encounter rate."
  },
  {
    "objectID": "Pr6/leaflet-demo.html",
    "href": "Pr6/leaflet-demo.html",
    "title": "Distance sampling survey design supplement",
    "section": "",
    "text": "Survey design with dssd\nThis exercise demonstrated how to examine the properties of various distance sampling survey designs. The exercise showed how to write survey locations to a GPX file, then import into Google Earth, but there’s a way to make visualisations all within R. I present here visualisations of the Tentsmuir point transect survey and a line transect survey of the coast of Ireland. The leaflet R package is used to show the placement of the survey effort.\n\n\n\n\n\nTentsmuir survey\nDesign of the survey begins with reading the unprojected shape file (coordinates likely degrees) and converting to a shape with distances measured in meters. A design is examined with allocation of point transects disproportionately between the two strata, with the smallest stratum receiving the highest allocation of effort. The final line of code in this chunk generates the coordinates of sampling stations for a realisation of this design.\n\nshapefile.name <- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape <- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string <- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape <- st_transform(sf.shape, crs = proj4string)\nregion.tm <- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\ncover.tm <- make.coverage(region.tm, n.grid.points = 100)\ndesign.tm <- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tm <- generate.transects(design.tm)\n\nUse either the +/- tools or mouse wheel to zoom and move around the map. The resulting map depicts the sampling stations as markers denoted with a binoculars icon. Hovering over the marker shows the latitude/longitude of each station. The red circles centred on each station is a circle of 100m radius, indicating the truncation distance specified in the make.design argument above. Use the measurement tool (upper right) to confirm the radius of the circles is 100m.\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\n\nTentsmuir study design, measuring tool top right.\n\n\n\n\nLine transect survey design with strata\nA different base map is used with leaflet to depict this marine survey. The basemap here shows some features of ocean bathymetry.\n\n\n\nIn contrast with the Tentsmuir survey, this begins with a projected study area map, with units of measure already in metres. Each of the six strata are given a different design.angle so as to approximate transects roughly perpendicular to the shore. Design specification is to have 15km spacing between lines within a stratum. Final line of code in this chunk produces coordinates of transects for a single realisation of this design.\n\nireland.name <- system.file(\"extdata\", \"AreaRProjStrata.shp\", package = \"dssd\")\nireland <- read_sf(ireland.name)\nst_crs(ireland)\n\nCoordinate Reference System:\n  User input: Albers-9 \n  wkt:\nPROJCRS[\"Albers-9\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",35,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-9,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nregion <- make.region(region.name = \"Area R Ireland coast\",\n                      units = \"m\",\n                      shape = ireland.name)\ncover <- make.coverage(region, n.grid.points = 100)\ndesign.space15k <- make.design(region = region,\n                               transect.type = \"line\",\n                               design = \"systematic\",\n                               spacing = 15000,\n                               design.angle = c(0, 160, 85, 90, 85, 160),\n                               edge.protocol = \"minus\",\n                               truncation = 2000,\n                               coverage.grid = cover)\nireland.trans <- generate.transects(object = design.space15k)\n\nUse the measurement tool (lower left corner) to check that line spacing is indeed 15km.\n\n\n\n\nMultiple strata for Irish survey."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html",
    "href": "Pr6/Pr6-instructions.html",
    "title": "Distance sampling survey design",
    "section": "",
    "text": "We provide two exercises in survey design so you can choose the one you feel is most useful to you.\n\nThe first example involves designing a line transect survey to estimate the abundance of porpoise, common dolphins and seals in and around St Andrews Bay.\n\nIt considers how you choose your design based on effort limitations.\nIt also compares an aerial survey based on systematic parallel lines with a boat based survey using zigzags.\n\nThe second example involves designing a point transect bird survey in Tentsmuir Forest.\n\nThis looks at how to project your study area from latitude and longitude on to a flat plane using R.\nIt also involves defining a design for multiple strata with different coverage in each strata."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#study-region",
    "href": "Pr6/Pr6-instructions.html#study-region",
    "title": "Distance sampling survey design",
    "section": "Study Region",
    "text": "Study Region\nFirst of all we will set up the study region and plot it. The shapefile for this study area is contained within the dssd R library. This shapefile has already been projected from latitude and longitude on to a flat plane and its units are in metres. The first line of code below returns the path for the shapefile within the R library and may vary on different computers. You will then pass this shapefile pathway to the make.region function to set up the survey region. As this shapefile does not have a projection (.prj) file associated with it we should tell dssd the units (m) when we create the survey region.\n\n#Find the pathway to the file\nshapefile.name <- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\n#Create the region using this shapefile\nregion <- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\n#Plot the region\nplot(region)\n\n\n\n\nStudy Region"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage",
    "href": "Pr6/Pr6-instructions.html#coverage",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover <- make.coverage(region, n.grid.points = 500)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "href": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "title": "Distance sampling survey design",
    "section": "Systematic Parallel Design",
    "text": "Systematic Parallel Design\nThe small survey plane available can complete a total flight time of around 250 km (excluding the flight time to and from the landing strip at Fife Ness). Generally, systematic parallel line designs are preferable for aerial surveys as they allow some rest time for observers as the plane travels between transects and avoids the sharp turns associated with zigzag designs.\nFirstly, we will consider the design angle. Often animal density is affected by distance to coast so it is probably wise for this survey to orientate lines approximately perpendicular to the coast. To do this we can select a design angle of 90 degrees. We can therefore expect to spend a little more than 40 km (the height of the survey region) on off-effort transit time and might hope to be able to complete around 200 km of transects. dssd lets us specify the desired line length as a design parameter and will then choose an appropriate value for transect spacing. We will choose a minus sampling strategy and set the truncation distance to 2 km. Note that as our survey region coordinates are in metres we also need to supply the design parameters in metres.\n\n# Define the design\ndesign.LL200 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      line.length = 200000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nNow we have defined the design we should check it visually by creating a survey (a single set of transects).\n\n\n\n\n# Create a single survey from the design\nsurvey.LL200 <- generate.transects(design.LL200)\n# Plot the region and the survey\nplot(region, survey.LL200)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nWe can see that the survey consists of parallel systematically spaced transects running horizontally across the survey region roughly perpendicular to the coast as we wanted. We can also view the details of the survey which will tell us what spacing dssd used to try and achieve a line length of 200 km.\n\n# Display the survey details\nsurvey.LL200\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced parallel transects\nSpacing:  4937.5\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nDesign angle:  90\nEdge protocol:  minus\nCovered area:  779476845\nStrata coverage: 78.93%\nStrata area:  987500079\n\n   Study Area Totals:\n   _________________\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nCovered area:  779476845\nAverage coverage: 78.93%\n\n\nWe can see that the spacing used by dssd was 4937.5 m which gives us 8 samplers and a coverage of just under 80%. In addition, this example survey has a line length of just under 200 km and a trackline length of just over 248 km. However, given the random nature of the design and the fact that the width of the study region is not constant everybody should get slightly different values. Although my trackline length was just under 250 km it is not sufficient to only look at one survey, we need to know that all surveys under this design will have a trackline length of < 250 km.\nTo assess the design statistics across many surveys we will now run a coverage simulation. This simulation will randomly generate many surveys from our design and record coverage as well as various statistics including line length and trackline length. As we know that coverage for a parallel line design is largely uniform (apart from edge effects due to minus sampling) we do not need to run too many repetitions, 100 should be sufficient to give us an indication of the range of line lengths and trackline lengths for this design.\n\n# Run the coverage simulation\ndesign.LL200 <- run.coverage(design.LL200, reps = 100)\ndesign.LL200\n\nAfter you have run the coverage simulation take a look at the design statistics. The mean line length should be around 200 km (200,000 m). Now look at the maximum trackline length, we need this value to be less than 250 km (250,000 m).\nUse the results of this simulation to create some new designs based on various spacings to find the maximum line length that can be achieved without risking exceeding the maximum trackline length of 250 km (remember to generate a line length of 200 km dssd selected a spacing of 4938m). Maybe try spacings of 5000 m or 5500 m.\n\n# Define the design\ndesign.space500 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\n\n\nWhat spacing would you select for this design?\n\nWhat is the maximum trackline length for the design you have selected?\n\nWhat on-effort line length are we likely to achieve?"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#zigzag-design",
    "href": "Pr6/Pr6-instructions.html#zigzag-design",
    "title": "Distance sampling survey design",
    "section": "Zigzag Design",
    "text": "Zigzag Design\nZigzag designs are often more efficient in their use of effort having less off-effort transit time between transects. For this survey another option would be to complete a boat-based survey. The boat survey will have the same total effort available allowing us a trackline length of 250 km.\nLet us now define a zigzag design for the same region. For zigzag designs the design angle has a different definition, it describes the angle across which the zigzags are constructed. For this example we want a vertical design angle so we will set it to 0. Zigzag designs also require an additional argument as zigzags can only be created inside convex shapes. We therefore need to specify the bounding shape, here we will choose a convex hull as it is more efficient than a minimum bounding rectangle. A convex hull works as if we were stretching an elastic band around the survey region. The code below shows you how to create the zigzag design, you should then create a single realisation of this design and plot it to check it looks acceptable.\n\n# Define the zigzag design\ndesign.zz.4500 <- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nsurvey.zz <- generate.transects(design.zz.4500)\nplot(region, survey.zz)\n\n\n\n\nSingle survey generated from the equalspaced zigzag design\n\n\n\n\nNext we will run a coverage simulation to verify that we have stayed within the restraints of our survey effort; a total trackline length of < 250 km. This time when we run the coverage simulation we will ask it to complete more repetitions so we can also assess the coverage.\nFirst we can output the design statistics.\n\n\nDoes this design meet our survey effort constraint?\n\nWhat is the maximum total trackline length for this design?\n\nWhat line length are we likely to achieve with this design?\n\nIs this higher or lower than the systematic parallel design?\n\n\n# Run the coverage simulation\ndesign.zz.4500 <- run.coverage(design.zz.4500, reps = 500)\n# Display the design statistics\ndesign.zz.4500\n\nNext we can check the coverage. Sometimes with zigzag surveys generated inside convex hulls we can get areas of higher coverage in narrower parts of the survey region at either end of the design axis. One of the easiest ways to assess coverage is visually by plotting the coverage grid.\n\n# Plot the coverage grid\nplot(design.zz.4500)\n\n\n\nDo you think the coverage scores look uniform across the study region?\n\nWhere are they higher/lower?\n\nWhy do you think this is?\n\n\nNote, you can go back to one of your parallel line designs and plot the coverage scores to compare (although there are fewer repetitions you can still get an idea of coverage)."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "href": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "title": "Distance sampling survey design",
    "section": "Projecting your Study Region",
    "text": "Projecting your Study Region\nThis exercise demonstrates how to deal with unprojected shapefiles. Study areas should always be projected onto a flat plane before you use them to design your survey. This is because in most parts of the world one degree latitude is not the same in distance as one degree longitude. If we didn’t project, our study region and any surveys generated in it, would be distorted possibly leading to non-uniform coverage.\nWe will now load the study region and project it onto a flat plane using an Albers Equal Area Conical projection. As we have to project the shapefile we load the shape object separately instead of directly into a region object.\n\n#Load the unprojected shapefile\nshapefile.name <- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape <- read_sf(shapefile.name)\n# Check current coordinate reference system\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n# Define a European Albers Equal Area projection\nproj4string <- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\n# Project the study area on to a flat plane\nprojected.shape <- st_transform(sf.shape, crs = proj4string)\n\nWe can now create the region object for dssd using the projected shape and plot it to check what it looks like.\n\n# Create the survey region in dssd\nregion.tm <- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n# Plot the survey region\nplot(region.tm)\n\n\n\n\nTentsmuir Forest: showing the main stratumand the Morton Loch stratum."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage-1",
    "href": "Pr6/Pr6-instructions.html#coverage-1",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover.tm <- make.coverage(region.tm, n.grid.points = 1000)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#design",
    "href": "Pr6/Pr6-instructions.html#design",
    "title": "Distance sampling survey design",
    "section": "Design",
    "text": "Design\nYou are now going to set up a systematic point transect design. We will assume that we have sufficient resources to survey 40 point transects. As the Morton Lochs stratum is of special interest we will give it higher coverage. We will therefore explicitly allocate 25 samplers to the main stratum and 15 to the Morton Lochs stratum (note that the area of the Morton Lochs stratum is much small than the main stratum). If we wanted to allocate the same effort to both stratum we could provide the samplers argument with the single value of 40 and it would divide the effort equally between the strata. We will leave the design angle as 0 and set the truncation distance to 100 m. We will use a minus sampling approach at the edges.\n\n\nWhat are the analysis implications of a design with unequal coverage?\n\n\n# Set up a multi strata systematic point transect design\ndesign.tm <- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)"
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#generate-a-survey",
    "href": "Pr6/Pr6-instructions.html#generate-a-survey",
    "title": "Distance sampling survey design",
    "section": "Generate a Survey",
    "text": "Generate a Survey\nYou will now generate a single survey from this design and plot it inside the survey region to check what it looks like. If you want to check whether the covered areas of the samplers in the Morton Lochs stratum overlap add the argument ‘covered.area = TRUE’ to the plot function.\n\n# Create a single survey from the design\nsurvey.tm <- generate.transects(design.tm)\n# Plot the region and the survey\nplot(region.tm, survey.tm, covered.area = TRUE)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nNow look at the survey information.\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers?\n\nDid your survey achieve exactly the number of samplers you requested?\n\nHow much does coverage differ between the two strata for this realisation?\n\n\n# Display survey information\nsurvey.tm\n\n\nSave coordinates to a file\nIf this survey is to be conducted in the field, you will want the coordinates that you can load into a handheld GPS. The function write.transects() can write waypoints of the survey (in this case the point transect stations) to text, comma-separated value or GPX files.\n\nwrite.transects(survey.tm,\n                dsn = \"tentsmuir-points.gpx\",\n                layer = \"points\",\n                dataset.options = \"GPX_USE_EXTENSIONS=yes\",\n                proj4string=sf::st_crs(sf.shape))\n\nThe GPX file can be transferred to a GPS, or viewed using Google Earth.\n\n\n\n\n\nRealised survey with locations written to GPX file and imported into Google Earth."
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "href": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "title": "Distance sampling survey design",
    "section": "Assessing Coverage and Design Statistics",
    "text": "Assessing Coverage and Design Statistics\nWe will now run a coverage simulation to assess how much the number of samplers and average coverage varies between surveys. We will also be able to assess how coverage varies spatially to see if edge effects are of concern.\nView the design statistics, then answer these questions.\n\n\nWhat is the minimum number of samplers you will achieve in each strata?\n\nIs this sufficient to complete separate analyses in each stratum?\n\nNext plot the coverage scores.\n\n\nDoes it appear that there is even coverage within each strata?\n\n\nAs there is such a difference in the range of coverage scores between strata you may need to plot each strata individually.\n\n\n\n# View the design statistics\ndesign.tm\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  100\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         13.0  36.0\nMean         25.1         15.1  40.2\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            0.9          1.1   1.3\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 683662.63    363258.99 1076099.58\nMean    768186.34    420283.45 1188469.79\nMedian  772479.34    417730.22 1188447.46\nMaximum 816669.52    468461.70 1264365.86\nsd       25071.81     25394.87   32875.01\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.85        50.79  7.26\nMean         5.44        58.76  8.02\nMedian       5.48        58.40  8.02\nMaximum      5.79        65.49  8.53\nsd           0.18         3.55  0.22\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00000000    0.2200000 0.00000000\nMean    0.05483087    0.5750000 0.08094378\nMedian  0.05000000    0.6450000 0.06000000\nMaximum 0.15000000    0.7800000 0.78000000\nsd      0.02997747    0.1456197 0.12170445\n\n# Plot the coverage scores\nplot(design.tm)\n\n\n\n# Plot coverage scores for individual strata\n# plot(design.tm, strata.id = 1)\n# plot(design.tm, strata.id = 2)"
  },
  {
    "objectID": "Pr7/detecting-differences.html",
    "href": "Pr7/detecting-differences.html",
    "title": "Detecting differences in density estimates",
    "section": "",
    "text": "Tip\n\n\n\n##Demonstration\nComparing two density estimates"
  },
  {
    "objectID": "Pr7/detecting-differences.html#output-interpretation",
    "href": "Pr7/detecting-differences.html#output-interpretation",
    "title": "Detecting differences in density estimates",
    "section": "Output interpretation",
    "text": "Output interpretation\nThe first two lines of output echo much of the input information: number of detections, encounter rate CV, line length, number of transects, average group size, CV of average groups size, number of parameters in the detection function. Also echoed are the stratum-specific density estimates and their CV along with the estimate of the \\(f(0)\\) parameter of the detection function and its CV.\nThe last line of output shows the difference of estimated densities and its standard error. Following this is the test statistic which is distributed as a t-statistic (Eqn @ref(eq:tstat)) and its associated degrees of freedom (Eqn @ref(eq:testdf)) and an associated significance level for a two-tailed test. The final pair of values are the bounds of the confidence interval on the estimated difference computed from Eqn (@ref(eq:tconfint)).\nNote: When a pooled detection function is used, the difference is relatively small (~0.05) and non-significant. When separate detection functions are used, the magnitude of the estimated difference increases (~0.08), with a standard error of roughly the same magnitude. Quite a few degrees of freedom are lost by having to estimate two additional parameters from separate detection functions. The total width of the confidence interval changes little (0.1662 for separate detection functions versus 0.1614 for pooled detection function). However because of the shift in the estimated difference from 0.0484 to 0.0802, the significance level shifts from 0.234 to 0.058.\nLesson for survey design: The uncertainty in all of the density estimates come from encounter rate variability. If the purpose of the study was to demonstrate a difference in density between these strata, a better survey design, with more than 13 and 12 transects would have been required to produce better estimates of stratum-specific encounter rate uncertainty."
  },
  {
    "objectID": "Pr7/Pr7-instructions.html",
    "href": "Pr7/Pr7-instructions.html",
    "title": "Analysis of stratified survey data",
    "section": "",
    "text": "Figure 1. An example of the sort of survey design used and a typical minke density gradient. The irregular bottom border is the ice-edge. The ‘stepped’ black line defines the boundary between the strata; dotted lines are transects and dots are detections.\n\nObjectives\nThe objectives of this exercise are to:\n\nCreate subsets of the data\nDecide whether to fit separate detection functions or a pooled detection function\nSpecify different stratification options using the dht2 function.\n\n\n\nGetting started\nBegin by reading in the data. Distances are in kilometers and a truncation distance of 1.5km is specified and used in the following detection function fitting. Perpendicular distances, transect lengths and study area size are all measured in kilometers; hence convert_units argument to ds is 1 and has been omitted. To keep things simple, a hazard rate detection function with no adjustments is used for all detection functions.\n\nlibrary(Distance)\ndata(minke)\nhead(minke)\n# Specify truncation distance\nminke.trunc <- 1.5\n\nYou will see that these data contain a column called ‘Region.Label’: this contains values ‘North’ or ‘South’.\n\n\nFull geographical stratification\nFirst, we want to fit encounter rate and detection function separately in each strata. This is easily performed by splitting the data by region and using ds on each subset. The commands below do this for the southern region (note, there are alternative ways to select a subset of data).\n\n# Create dataset for South \nminke.S <- minke[minke$Region.Label==\"South\", ]\n# Fit df to south\nminke.df.S.strat <- ds(minke.S, key=\"hr\", adjustment=NULL, truncation=minke.trunc)\nsummary(minke.df.S.strat)\n\nMake a note of the AIC. Perform a similar commands to obtain estimates for the northern region. What is the total AIC?\nAlso make a note of the abundance in each region. What is the total abundance in the study region?\n\n\nFitting a pooled detection function\nWe want to compare the total AIC found previously with the AIC from fitting a detection function to all data combined. This is easy to obtain:\n\nminke.df.all <- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\nGiven the AIC value for the detection function from the pooled data, would you fit a separate detection function in each strata or not?\n\n\nStratification options using dht2\nThe command summary(minke.df.all) will provide the abundance estimates for each region and the total and for this simple example, this is sufficient. However, if we want to consider different stratification options, then the dht2 function is useful.\nAfter fitting a detection function, the dht2 function, allows abundance estimates to be computed over some specified regions. In the command below, the pooled detection function is used to obtain estimates in each strata and over all (like the summary function previously used).\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~Region.Label, stratification=\"geographical\")\n\nThe arguments are:\n\nddf\n\nthe detection function (fitted by ds)\n\nflatfile\n\nthe data object containing all the necessary information\n\nData is referred to as being in a flatfile format if it contains information on region, transects and observations. An alternative is to use a hierarchical structure and have region, transect and observation information in separate data files with links between them to ensure that transects are mapped to the relevant region and observations to the relevant transect. We’ve not used the hierarchical structure during this workshop.\n\n\nstrat_formula=~Region.Label\n\nformula (hence the ~) giving the stratification structure\n\nstratification=\"geographical\"\n\nin this example, we specify that each strata (specified in strat_formula) represents a geographical region.\n\nconvert_units\n\ngetting units conversion correct, same purpose as the convert_units argument in ds. For the minke data all measurements are in the same units, so the argument is not needed in this case.\n\n\nMake a note the total abundance in the study region.\n\n\n\n\n\n\nFailure to respect design during analysis\n\n\n\nWhat happens if we were to ignore the regions and treat the data as though it came from one large study region? This can (dangerously) be done by changing the stratification formula, as shown below.\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~1, stratification=\"geographical\")\n\nHas this changed the abundance estimate? Of course it has; the question is why has this changed the abundance estimate; which estimate is proper?"
  },
  {
    "objectID": "Pr7/strata.html",
    "href": "Pr7/strata.html",
    "title": "Stratification practical 7",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function.\nPhoto by Nick Fewings on Unsplash"
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html",
    "href": "Pr8/alternative-selection-metrics.html",
    "title": "Other model selection metrics",
    "section": "",
    "text": "Demonstration\n\n\n\nAlternative model selection metrics"
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#aicc",
    "href": "Pr8/alternative-selection-metrics.html#aicc",
    "title": "Other model selection metrics",
    "section": "AICc",
    "text": "AICc\nDefined as:\n\\[AICc = -2ln(\\mathscr{L}) - 2k + \\frac{2k(k+1)}{n-k-1}\\]\nwhere k is the number of parameters in the model and n is the number of observations. In general, if \\(n\\) is many times larger than \\(k^2\\), then the extra term will be negligible. What do we know, in general, about the magnitude of k and n in typical distance sampling situations? We encourage the collection of 60 to 80 detections (\\(n\\)). A hazard rate key function with a four-level factor covariate has \\(k=5\\) parameters. The value of the the \\(c\\) term added to the regular AIC ranges from 1.11 with \\(n=60\\) to 0.81 with \\(n=80\\).\nFor a two-parameter model (hazard rate or half normal with a single continuous covariate) the value of the additional term would be 0.21 with \\(n=60\\) to 0.16 with \\(n=80\\). Recognise the magnitude of the other terms in the AIC are in the hundreds or thousands (see below)."
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#bic",
    "href": "Pr8/alternative-selection-metrics.html#bic",
    "title": "Other model selection metrics",
    "section": "BIC",
    "text": "BIC\nDefined as:\n\\[BIC = -2ln(\\mathscr{L}) - k \\cdot ln(n)\\]\nThe penalty term changes as a function of sample size; the larger the number of detections, the greater the penalty term. With a 2 parameter model using AIC, the penalty term would be 4 (\\(2 \\times 2\\)). For a model with the the same number of parameters and 80 detections, the penalty term would be 8.8."
  },
  {
    "objectID": "Pr8/covariates.html",
    "href": "Pr8/covariates.html",
    "title": "Covariates practical 8",
    "section": "",
    "text": "It is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata–we examine whether the geographic stratum effect can be modelled as a covariate.\nPhoto by Roman Mager on Unsplash"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html",
    "href": "Pr8/Group-size-covariate.html",
    "title": "Size bias—how large is the problem?",
    "section": "",
    "text": "Supplement\n\n\n\nCorrecting size bias via size as a covariate. When does it matter?"
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis including group size covariate",
    "text": "Analysis including group size covariate\n\n\n\n\n\nThe distribution of computed average group size centred on the true size of 10 and there was no problem with fitting a detection function. The average over the simulations estimated number of individuals was 2008.39."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without group size covariate",
    "text": "Analysis without group size covariate\nAs a comparison, what happens if we don’t include size as a covariate in our detection function?\n\n\n\n\n\nThe distribution of computed average groups sizes is shown above. We would expect an overestimate of mean group size because small groups at large distances are missing from our sample; but that effect is small in this instance. As a consequence, the average \\(\\hat{N}_{indiv}\\) across all simulations is 1999.46."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis with covariate",
    "text": "Analysis with covariate\n\n\n\n\n\nWhen including size as a covariate, estimates of average group size are not affected (figure above). Likewise, mean \\(\\hat{N}_{indiv}\\) is effectively unbiased: 4503.91."
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without the covariate",
    "text": "Analysis without the covariate\n\n\n\n\n\nNow mean \\(\\hat{N}_{indiv}\\) is considerably biased: 5323.59, 21.3 percent larger than the true number of individuals in the population, 4388."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html",
    "href": "Pr8/Pr8-instructions.html",
    "title": "Covariates in detection function model",
    "section": "",
    "text": "This exercise consists of three data sets of increasing difficulty. The first problem, MCDS with point transects, is complicated and (using the functionality available in R) also includes some basic exploratory analysis of the covariates. Section 2 and 3 are optional but will take you deeper into the heart of understanding multiple covariates."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "href": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "title": "Covariates in detection function model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIt is important to gain an understanding of the data prior to fitting detection functions (Buckland et al., 2015). With this in mind, preliminary analysis of distance sampling data involves:\n\nassessing the shape of the collected data,\nconsidering the level of truncation of distances, and\nexploring patterns in potential covariates.\n\nWe begin by assessing the distribution of distances to decide on a truncation distance.\n\nhist(amakihi$distance)\n\nTo see if there are differences in the distribution of distances recorded by the different observers and in each hour after sunrise, boxplots can be used. Note how the ~ symbol is used to define the discrete groupings (i.e. observer and hour).\n\n# Boxplots by obs\nboxplot(amakihi$distance~amakihi$OBs, xlab=\"Observer\", ylab=\"Distance (m)\")\n# Boxplots by hour after sunrise\nboxplot(amakihi$distance~amakihi$HAS, xlab=\"Hour\", ylab=\"Distance (m)\")\n\nThe components of the boxplot are:\n\nthe thick black line indicates the median\nthe lower limit of the box is the first quartile (25th percentile) and the upper limit is the third quartile (75th percentile)\nthe height of the box is the interquartile range (75th - 25th quartiles)\nthe whiskers extend to the most extreme points which are no more than 1.5 times the interquartile range.\ndots indicate ‘outliers’ if there are any, i.e. points beyond the range of the whiskers.\n\nFor minutes after sunrise (a continuous variable), we create a scatterplot of MAS (on the \\(x\\)-axis) against distances (on the \\(y\\)-axis). The plotting symbol (or character) is selected with the argument pch:\n\n# Plot of MAS vs distance (using dots)\nplot(x=amakihi$MAS, y=amakihi$distance, xlab=\"Minutes after sunrise\",\n     ylab=\"Distance (m)\", pch=20)\n\nYou may also want to think about potential collinerity (linear relationship) between the covariates - if collinear variables are included in the detection function, they will be explaining some of the same variation in the distances and this will reduce their importance as a potential covariate. How might you investigate the relationship between HAS and MAS?\nFrom these plots can you tell if any of the covariates will be useful in explaining the distribution of distances?"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "href": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "title": "Covariates in detection function model",
    "section": "Adjusting the raw covariates",
    "text": "Adjusting the raw covariates\nWe would like to treat OBs and HAS as factor variables as in the original analysis; OBs is, by default, treated as a factor variable because it consists of characters rather than numbers. HAS, on the other hand, consists of numbers and so by default would be treated as a continuous variable (i.e. non-factor). That is fine if we want the effect of HAS to be monotonic (i.e. detectability either increases or decreases as a function of HAS). If we want HAS to have a non-linear effect on detectability, then we need to indicate to R to treat it as a factor as shown below.\n\n# Convert HAS to a factor\namakihi$HAS <- factor(amakihi$HAS)\n\nThe next adjustment is to change the reference level of the observer and hour factor covariates - the only reason to do this is to get the estimated parameters in the detection function to match the parameters estimated in T. A. Marques et al. (2007). You would not carry out this step on your own data. By default R uses the first factor level but by using the relevel function, this can be changed:\n\n# Set the reference level \namakihi$OBs <- relevel(amakihi$OBs, ref=\"TKP\")\namakihi$HAS <- relevel(amakihi$HAS, ref=\"5\")"
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#candidate-models",
    "href": "Pr8/Pr8-instructions.html#candidate-models",
    "title": "Covariates in detection function model",
    "section": "Candidate models",
    "text": "Candidate models\nWith three potential covariates, there are 8 possible models for the detection function:\n\nNo covariates\nOBs\nHAS\nMAS\nOBs + HAS\nOBs + MAS\nHAS + MAS\nOBs + HAS + MAS\n\nEven without considering covariates there are also several possible key function/adjustment term combinations available: if all key function/covariate combinations are considered the number of potential models is large. Note that covariates are not allowed if a uniform key function is chosen and if covariate terms are included, adjustment terms are not allowed. Even with these restrictions, it is not best practice to take a scatter gun approach to detection function model fitting. Buckland et al. (2015) considered 13 combinations of key function/covariates. Here, we look at a subset of these.\nFit a hazard rate model with no covariates or adjustment terms and make a note of the AIC. Note, that 10% of the largest distances are truncated - you may have decided on a different truncation distance.\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr <- ds(amakihi, transect=\"point\", key=\"hr\", truncation=\"10%\",\n              adjustment=NULL, convert_units = conversion.factor)\n\nMake a note of the AIC for this model.\nNow fit a hazard rate model with OBs as a covariate in the detection function and make a note of the AIC. Has the AIC reduced by including a covariate?\n\nconversion.factor <- convert_units(\"meter\", NULL, \"hectare\")\namak.hr.obs <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs,\n                  truncation=\"10%\", convert_units = conversion.factor)\n\nFit a hazard rate model with OBs and HAS in the detection function:\n\namak.hr.obs.has <- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs+HAS,\n                      truncation=\"10%\", convert_units = conversion.factor)\n\nTry fitting other possible formula and decide which model is best in terms of AIC. To quickly compare AIC values from different models, use the AIC command as follows (note only models with the same truncation distance can be compared):\n\n# AIC values\nAIC(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nAnother useful function is summarize_ds_models - this has the advantage of ordering the models by AIC (smallest to largest).\n\n# Compare models\nsummarize_ds_models(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nOnce you have decided on a model, plot your selected detection function."
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#analysis",
    "href": "Pr8/Pr8-instructions.html#analysis",
    "title": "Covariates in detection function model",
    "section": "Analysis",
    "text": "Analysis\nThe data are available in the Distance package:\n\ndata(ETP_Dolphin)\nhead(ETP_Dolphin, n=3)\n\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint - try dividing the histogram of distances into a large number of intervals.)\nAs there are a number of potential covariates to be used in this example (i.e. search method, cue, Beaufort class and month), try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except group size and because they have numeric codes, use the factor function to let R know to treat them as factors.\nNote that both distances and transect lengths were recorded in nautical miles and area in nautical miles squared and so the argument convert_units does not need to be specified.\nKeep in mind that this is a large dataset (> 1000 observations), and hence estimation may take a while. You will likely end up with quite a few models as there are several potential covariates and no ‘right’ answers. Discuss your choice of final model (or models) with your colleagues - did you make the same choices?"
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "It is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\nPhoto by Jamshaid Mughal on Unsplash"
  }
]