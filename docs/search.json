[
  {
    "objectID": "Pr9/multi-analysis.html",
    "href": "Pr9/multi-analysis.html",
    "title": "Multispecies and multisession distance sampling analysis",
    "section": "",
    "text": "A multispecies data set with multiple visits\nIt is increasingly common for investigators to conduct surveys in which multiple species are detected and density estimates for several species are of interest. There are many ways of analysing such data sets, but care must be taken. Not all approaches will produce correct density estimates. To demonstrate one of the ways to produce incorrect estimates, we will use the line transect survey data reported in Buckland (2006). This survey (and data file) recorded detections of four species of songbirds. We conduct an analysis of chaffinch (Fringilla coelebs) (coded c in the data file), but similar results would arise with the other species.\nBegin by reading the flat file in a comma delimited format. Note the URL for the data file is very long, double check that you can read the URL including the Github token.\n\nbirds &lt;- read.csv(file=\"https://raw.githubusercontent.com/DistanceDevelopment/Distance/refs/heads/master/vignettes/montrave-line.csv\")\n\n\n\nSurvey design\nBuckland’s design consisted of visiting each of the 19 transects in his study twice. To examine some of the errors that can arise from improper analysis, I choose to treat the two visits as strata for the express purpose of generating stratum (visit) -specific density estimates. Density estimates reported in Buckland (2006) are in units of birds \\(\\cdot hectare^{-1}\\).\n\nbirds$Region.Label &lt;- birds$visit\ncu &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n\n\n\nAnalysis of only one species (incorrectly)\nThe direct approach to producing a density estimate for the chaffinch would be to subset the original data frame and use the species-specific data frame for analysis. Begin by performing the subset operation.\n\nchaf &lt;- birds[birds$species==\"c\", ]\n\nWhen the data are subset, the integrity of the survey design is not preserved. A simple frequency table of the species-specific data frame flags up a number of transect/visit combinations where no chaffinches were detected. The result is that the subset data frame suggests 3 of the 19 transects lacked chaffinch detections on the first visit and one of the 19 transects lacked chaffinch detections on the second visit. This revelation, in itself, causes no problems for our estimate of density of chaffinches.\n\ndetects &lt;- table(chaf$Sample.Label, chaf$visit)\ndetects &lt;- as.data.frame(detects)\nnames(detects) &lt;- c(\"Transect\", \"Visit\", \"Detections\")\ndetects$Detections &lt;- cell_spec(detects$Detections, \n                          background = ifelse(detects$Detections==0, \"red\", \"white\"))\nknitr::kable(detects)\n\n\n\n\n\n\n\n\n\nTransect\nVisit\nDetections\n\n\n\n\n1\n1\n3\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n1\n3\n\n\n5\n1\n5\n\n\n6\n1\n4\n\n\n7\n1\n2\n\n\n8\n1\n0\n\n\n9\n1\n1\n\n\n10\n1\n1\n\n\n11\n1\n0\n\n\n13\n1\n1\n\n\n14\n1\n1\n\n\n15\n1\n3\n\n\n16\n1\n2\n\n\n17\n1\n3\n\n\n18\n1\n3\n\n\n19\n1\n0\n\n\n1\n2\n1\n\n\n2\n2\n4\n\n\n3\n2\n3\n\n\n4\n2\n2\n\n\n5\n2\n4\n\n\n6\n2\n3\n\n\n7\n2\n3\n\n\n8\n2\n1\n\n\n9\n2\n0\n\n\n10\n2\n2\n\n\n11\n2\n1\n\n\n13\n2\n1\n\n\n14\n2\n1\n\n\n15\n2\n1\n\n\n16\n2\n1\n\n\n17\n2\n1\n\n\n18\n2\n4\n\n\n19\n2\n1\n\n\n\n\n\nHowever, there is a problem hidden within the table above. Transect 12 does not appear in the table because there were no detections of chaffinches on either visit. Consequently, there were 4 transects without chaffinches on the first visit and 2 transects without chaffinches on the second visit, rather than the 3 transects and 1 transect you might mistakenly conclude do not have chaffinch detections if you relied completely upon the table.\nLet’s see what the ds() function thinks about the survey effort using information from the species-specific data frame.\n\nchaf.wrong &lt;- ds(chaf, key=\"hn\", convert_units = cu, truncation=95, formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(chaf.wrong$dht$individuals$summary) %&gt;%\n  kable_paper(full_width=FALSE) %&gt;%\n  column_spec(6, background=\"salmon\") %&gt;%\n  column_spec(7, background=\"steelblue\")\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\n\n\n\n\n1\n33.2\n82.061\n4.319\n39\n15\n9.029868\n1.1159303\n0.1235821\n\n\n2\n33.2\n83.562\n4.398\n34\n17\n7.730787\n0.9798153\n0.1267420\n\n\nTotal\n66.4\n165.623\n8.717\n73\n32\n8.380327\n0.7425191\n0.0886026\n\n\n\n\n\n\n\nExamine the column labelled k (the number of transects) for each of the visits. Rather than the 19 transects that were surveyed on each visit, the ds() function erroneously believes there were only 15 transects surveyed on the first visit and 17 transects surveyed on the second visit.\nNote also the number of detections per kilometer; roughly 9 on the first visit and 7.7 on the second visit. These encounter rates exclude kilometers of effort on transects where there were no detections. We will return to this comparison later.\n\n\nUse explicit data hierarchy\n\n\n\n\n\n\nDescribing the survey design to ds\n\n\n\nAdditional arguments can be passed to ds() to resolve this problem. Consulting the ds() documentation\n\nregion_table data.frame with two columns:\n\nRegion.Label label for the region\nArea area of the region\nregion_table has one row for each stratum. If there is no stratification then region_table has one entry with Area corresponding to the total survey area. If Area is omitted density estimates only are produced.\n\nsample_table data.frame mapping the regions to the samples (i.e. transects). There are three columns:\n\nSample.Label label for the sample\nRegion.Label label for the region that the sample belongs to.\nEffort the effort expended in that sample (e.g. transect length).\n\n\n\n\nThis analysis that produces erroneous results can be remedied by explicitly letting the ds() function know about the study design; specifically, how many strata and the number of transects within each stratum (and associated transect lengths).\nConstruct the region table and sample table showing the two strata with equal areas and each labelled transect (of given length) is repeated two times.\n\nbirds.regiontable &lt;- data.frame(Region.Label=as.factor(c(1,2)), Area=c(33.2,33.2))\nbirds.sampletable &lt;- data.frame(Region.Label=as.factor(rep(c(1,2), each=19)),\n                                Sample.Label=rep(1:19, times=2),\n                                Effort=c(0.208, 0.401, 0.401, 0.299, 0.350,\n                                         0.401, 0.393, 0.405, 0.385, 0.204,\n                                         0.039, 0.047, 0.204, 0.271, 0.236,\n                                         0.189, 0.177, 0.200, 0.020))\n\n\n\nSimple detection function model\nThe chaffinch analysis is performed again, this time supplying the region_table and sample_table information to ds(). The correct number of transects (19) sampled on both visits (even though chaffinch was not detected on 4 transects on visit 1 and 2 transects on visit 2) is now recognised. Hence, the use of region table and sample table solves the problem of effort miscalculation if a species is not detected on all transects.\n\ntr &lt;- 95   # as per Buckland (2006)\nonlycf &lt;- ds(data=birds[birds$species==\"c\", ], \n             region_table = birds.regiontable,\n             sample_table = birds.sampletable,\n             trunc=tr, convert_units=cu, key=\"hn\", formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(onlycf$dht$individuals$summary) %&gt;%\n  kable_paper(full_width=FALSE) %&gt;%\n  column_spec(6, background=\"salmon\") %&gt;%\n  column_spec(7, background=\"steelblue\")\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\n\n\n\n\n1\n33.2\n91.77\n4.83\n39\n19\n8.074534\n1.2196305\n0.1510465\n\n\n2\n33.2\n91.77\n4.83\n34\n19\n7.039338\n1.0612781\n0.1507639\n\n\nTotal\n66.4\n183.54\n9.66\n73\n38\n7.556936\n0.8083641\n0.1069698\n\n\n\n\n\n\n\n\n\nConsequence of incorrect analysis\nTo drive home the consequence of failing to properly specify the survey effort, contrast the encounter rate for the two visits from the incorrect calculations above (9.0 and 7.7 respectively), with the correct calculation (8.1 and 7.0 respectively). The number of transects is incorrect with the knock-on effect of effort being incorrect. If effort is incorrect then so too is covered area.\nThe ripple effect from incomplete information about the survey design results in positively biased estimates of density.\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2",
    "crumbs": [
      "Multipliers",
      "Multispecies and multisession distance sampling analysis"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 64 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n there is a 95% probability that the true mean lies within this range 95% of the data fall within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "Pr9/Pr9-instructions.html",
    "href": "Pr9/Pr9-instructions.html",
    "title": "Analyses using multipliers 💻",
    "section": "",
    "text": "We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets 2 and 3 deal with instantaneous cues and so only cue rate needs to be taken into account.",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#getting-started",
    "href": "Pr9/Pr9-instructions.html#getting-started",
    "title": "Analyses using multipliers 💻",
    "section": "Getting started",
    "text": "Getting started\nThese data (called sikadeer) are available in the Distance package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor &lt;- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "href": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "title": "Analyses using multipliers 💻",
    "section": "Fit detection function to dung pellets",
    "text": "Fit detection function to dung pellets\nFit the usual series of models (i.e. half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don’t spend too long on this). Call your model deer.df. This detection function will be used to obtain \\(\\hat D_{\\textrm{pellet groups}}\\).\nHave a look at the Summary statistics for this model - what do you notice about the allocation of search effort in each woodland?",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#multipliers",
    "href": "Pr9/Pr9-instructions.html#multipliers",
    "title": "Analyses using multipliers 💻",
    "section": "Multipliers",
    "text": "Multipliers\nThe next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.\nData to calculate this has been collected in the file IntroDS_9.1.csv that can be read from the Github internet repository. Following code comes from Meredith (2017).\n\nMIKE.persistence &lt;- function(DATA) {\n  \n#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data \n#  Input: data frame with at least two columns:\n#         DAYS - calendar day on which dung status was observed\n#         STATE - dung status: 1-intact, 0-decayed\n#  Output: point estimate, standard error and CV of mean persistence time\n#\n#  Attribution: code from Mike Meredith website: \n#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm\n#   Citing: CITES elephant protocol\n#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf\n  \n  ##   Fit logistic regression model to STATE on DAYS, extract coefficients\n  dung.glm &lt;- glm(STATE ~ DAYS, data=DATA, family=binomial(link = \"logit\"))\n  betas &lt;- coefficients(dung.glm)\n  ##   Calculate mean persistence time\n  mean.decay &lt;- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]\n  ## Calculate the variance of the estimate\n  vcovar &lt;- vcov(dung.glm)\n  var0 &lt;- vcovar[1,1]  # variance of beta0\n  var1 &lt;- vcovar[2,2]  # variance of beta1\n  covar &lt;- vcovar[2,1] # covariance\n  deriv0 &lt;- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]\n  deriv1 &lt;- -mean.decay/betas[2]\n  var.mean &lt;- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2\n  ## Calculate the SE and CV and return\n  se.mean &lt;- sqrt(var.mean)\n  cv.mean &lt;- se.mean/mean.decay\n  out &lt;- c(mean.decay, se.mean, 100*cv.mean)\n  names(out) &lt;- c(\"Mean persistence time\", \"SE\", \"%CV\")\n  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab=\"Days since initiation\",\n       ylab=\"Dung persists (yes=1)\",\n       main=\"Eight dung piles revisited over time\")\n  curve(predict(dung.glm, data.frame(DAYS=x), type=\"resp\"), add=TRUE)\n  abline(v=mean.decay, lwd=2, lty=3)\n  return(out)\n}\ndecay &lt;- read.csv(\"https://raw.githubusercontent.com/erex/Oct-Quarto/main/Pr9/IntroDS_9.1.csv\")\npersistence.time &lt;- MIKE.persistence(decay)\nprint(persistence.time)\n\nRunning the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been ‘jittered’ to avoid over-plotting.\nAn estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.\nAs stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:\n\ncreation contains estimates of the dung production rate and associated standard error\ndecay contains the dung decay rate and associated standard error where XX and YY are the estimates you obtained from the dung decay rate analysis.\n\n\n# Create list of multipliers\nmult &lt;- list(creation = data.frame(rate=25, SE=0),\n#             decay    = data.frame(rate=XX, SE=YY))\nprint(mult)\n\nThe final step is to use these multipliers to convert \\(\\hat D_{\\textrm{pellet groups}}\\) to \\(\\hat D_{\\textrm{deer}}\\) (as in the equations above) - for this we need to employ the dht2 function. In the command below the multipliers= argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:\n\nstrat_formula=~Region.Label is specified to take into account the design (i.e. different woodlands or blocks).\nstratification=\"effort_sum\" is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.\ndeer.df is the detection function you have fitted.\n\n\n# Weight by effort because we have repeats\ndeer.ests &lt;- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                 convert_units=conversion.factor, multipliers=mult, \n                 stratification=\"effort_sum\", total_area=13.9)\nprint(deer.ests)\n\nThe function dht2 also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata.",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/countmodel-points.html",
    "href": "Pr9/countmodel-points.html",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-points.html#data-organisation",
    "href": "Pr9/countmodel-points.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Data organisation",
    "text": "Data organisation\nNote there is no Effort field for these point count data.\n\nnewthrasher &lt;- merge(thrasherDetectionData, thrasherSiteData, by=\"siteID\", all=TRUE)\nnames(newthrasher) &lt;- sub(\"observer\", \"obs\", names(newthrasher))\nnames(newthrasher) &lt;- sub(\"dist\", \"distance\", names(newthrasher))\nnewthrasher$distance &lt;- as.numeric(newthrasher$distance)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nSpecification of run-specific parameters to analyse the Sage thrasher data set. Particularly note the logical value assigned to pointtransect. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height. The same predictors, with the exception of observer could be used to model the thrasher counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect &lt;- TRUE        # survey conducted using lines or points\ndettrunc &lt;- 170  # truncation for detection function\nmyglmmodel &lt;- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot &lt;- 10      # number of bootstrap replicates\nset.seed(7079)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-points.html#fit-detection-function",
    "href": "Pr9/countmodel-points.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Sage thrasher data set.\n\nsurveytype &lt;- ifelse(pointtransect, \"point\", \"line\")\nwoo.o &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~obs,\n            transect=surveytype, quiet=TRUE)\nwoo.shrub &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(shrub),\n           transect=surveytype, quiet=TRUE)\nwoo.herb &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(herb),\n           transect=surveytype, quiet=TRUE)\nwoo.height &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(height),\n           transect=surveytype, quiet=TRUE)\nwoo &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~1,\n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo, woo.o, woo.shrub, woo.herb, woo.height),\n             digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\",\n             row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~obs\n0.009\n0.292\n0.039\n0.000\n\n\n\nHalf-normal with cosine adjustment term of order 2\n~1\n0.096\n0.394\n0.124\n12.868\n\n\n\nHalf-normal\n~scale(herb)\n0.032\n0.331\n0.034\n17.893\n\n\n\nHalf-normal\n~scale(shrub)\n0.035\n0.331\n0.034\n17.901\n\n\n\nHalf-normal\n~scale(height)\n0.037\n0.333\n0.034\n19.168\n\n\n\n\n\nNone of the covariates contribute to fit of the detection function models for thrashers. Inference should be based upon the no covariate model (that passes the goodness of fit test), but for testing purposes, we will use the model with the observer covariate. Use of the observer covariate model will retain between-point variability in effective area; useful for testing purposes.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Sage thrasher\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nadd_df_covar_line(woo.o, data.frame(obs=\"obs6\"), col=\"coral\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:6,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\", \"coral\"))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. Division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn &lt;- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa &lt;- NA\n  k &lt;- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] &lt;= truncation & !is.na(newdata[i,\"distance\"])) {\n      k &lt;- k+1\n      newdata$Pa[i] &lt;- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea &lt;- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea &lt;- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result &lt;- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea &lt;- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea &lt;- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nThe function has an argument pointflag used to indicate whether point transect sampling was used. If so, the correct effective area calculations are performed.\n\nsitesadj &lt;- effAreafn(woo.o, newthrasher, 10000, dettrunc, pointflag = pointtransect)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nAs for the line transect example, fit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function.\n\nunivarpredictor &lt;- all.vars(myglmmodel)[2]\nglmmodel &lt;- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum &lt;- summary(glmmodel)\ntablecaption &lt;- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nknitr::kable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\n\nGLM coefficients from counts as function of shrub with log(effective area) offset.\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-2.3977\n0.8794\n-2.7266\n0.0064\n\n\nshrub\n0.0818\n0.0419\n1.9538\n0.0507"
  },
  {
    "objectID": "Pr9/countmodel-points.html#visualise",
    "href": "Pr9/countmodel-points.html#visualise",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment."
  },
  {
    "objectID": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est &lt;- vector(\"numeric\", length=nboot)\nslope.est &lt;- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame thrasherSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Sage thrasher density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects &lt;- data.frame() \n  bob &lt;- sample(thrasherSiteData$siteID, replace=TRUE, size=length(unique(thrasherSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite &lt;- bob[bootsite] \n    glob &lt;- newthrasher[newthrasher$siteID==thissite, ]\n    glob$siteID &lt;- sprintf(\"rep%02d\", bootsite)\n    newdetects &lt;- rbind(newdetects, glob)  \n    }\n  newdetects &lt;- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod &lt;- ds(data=newdetects, truncation=dettrunc, formula=~obs, transect=surveytype, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj &lt;- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = pointtransect)\n#   refit the GLM for this bootstrap replicate\n  glmresult &lt;- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] &lt;- coef(glmresult)[1]\n  slope.est[theboot] &lt;- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and bird density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData &lt;- data.frame(predictor=seq(min(thrasherSiteData[ , univarpredictor]),\n                                 max(thrasherSiteData[, univarpredictor]), length.out=50)) \norig.fit &lt;- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform &lt;- NULL\nfor (i in 1:nboot) {\n  mypredict &lt;- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform &lt;- c(longform, mypredict)\n}\nbig.df &lt;- data.frame(predict=longform)\nbig.df$shrub &lt;- predData$predictor\nbig.df$group &lt;- rep(1:nboot, each=length(predData$predictor))\nalpha &lt;- 0.05\n# point-wise confidence intervals\nquants &lt;- big.df %&gt;% \n  group_by(shrub) %&gt;% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %&gt;% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label &lt;- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label &lt;- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 &lt;- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=24, y=1.5, label= b0label, size=5) +\n  annotate(geom=\"text\", x=24, y=1.4, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Sage thrasher density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds &lt;- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 &lt;- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.18, y=11, label=round(bounds[2],3)) \ncomplete &lt;- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Sage Thrasher density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Sage Thrasher."
  },
  {
    "objectID": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. In the case of the Sage Thrasher, the species appears to exhibit little response to the presence of shrub cover during this study. Note that the range of shrub cover is not extensive (~16% to ~26%). We can make no inference regarding the density:habitat relationship outside this range in shrub cover."
  },
  {
    "objectID": "Pr9/countmodel-lines.html",
    "href": "Pr9/countmodel-lines.html",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable \\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#data-organisation",
    "href": "Pr9/countmodel-lines.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Data organisation",
    "text": "Data organisation\nIn Carlisle’s data set, sightings information is kept separate from information about each site. For our purposes, we will merge those together. In addition, some field names are changed for consistency with functions in the Distance package.\n\nnewsparrow &lt;- merge(sparrowDetectionData, sparrowSiteData, by=\"siteID\", all=TRUE)\nnames(newsparrow) &lt;- sub(\"observer\", \"obs\", names(newsparrow))\nnames(newsparrow) &lt;- sub(\"dist\", \"distance\", names(newsparrow))\nnames(newsparrow) &lt;- sub(\"length\", \"Effort\", names(newsparrow))\nnewsparrow$distance &lt;- as.numeric(newsparrow$distance)\nnewsparrow$Effort &lt;- as.numeric(newsparrow$Effort)",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nAlthough not formally written as a set of functions, we bring to the front of the code arguments the user will need to change to alter to suite their needs. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height, shrubclass. The same predictors, with the exception of observer could be used to model the sparrow counts.\n\n\n\n\n\n\nImportant\n\n\n\n\nalpha &lt;- 0.05          # type I error rate for confidence intervals\npointtransect &lt;- FALSE        # survey conducted using lines or points\ndettrunc &lt;- 100  # truncation for detection function\ntransect.length &lt;- newsparrow$Effort[1] # each transect the same length\nmeterstohectares &lt;- 10000\nmyglmmodel &lt;- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot &lt;- 50       # number of bootstrap replicates\nset.seed(255992)   # random number seed",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#fit-detection-function",
    "href": "Pr9/countmodel-lines.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Brewer’s sparrow data set.\n\nsurveytype &lt;- ifelse(pointtransect, \"point\", \"line\")\nwoo.o &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.bare &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~bare,\n                transect=surveytype, quiet=TRUE)\nwoo.ht &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~height, \n             transect=surveytype, quiet=TRUE)\nwoo.shrc &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~shrubclass, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~shrub, \n            transect=surveytype, quiet=TRUE)\nwoo &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo.o,  woo.bare,\n                                 woo.ht, woo.shrc, woo.shrub, woo), digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\", row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~bare\n0.526\n0.556\n0.025\n0.000\n\n\n\nHalf-normal\n~shrubclass\n0.511\n0.559\n0.025\n1.689\n\n\n\nHalf-normal\n~height\n0.492\n0.558\n0.025\n2.014\n\n\n\nHalf-normal\n~shrub\n0.476\n0.559\n0.025\n2.691\n\n\n\nHalf-normal\n~1\n0.469\n0.563\n0.025\n4.843\n\n\n\nHalf-normal\n~obs\n0.587\n0.559\n0.025\n8.837\n\n\n\n\nwinner &lt;- woo.bare\n\nAll of the candidate models fit the Brewer’s sparrow line transect data. Also note that the estimate detection probability of all six models is the same to the third decimal. The detection function model using bare ground as a predictor is slightly favoured by AIC; we will base our inference on the detection function that includes bare as a covariate. There is likely little effect of this model selection choice upon the ecological question of interest.\n\nPlot of detection function\n\nif(winner$ddf$ds$aux$ddfobj$scale$formula == \"~obs\") {\n  plot(woo.o, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with observer covariate\", pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Observer\", legend=1:5,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"darkgreen\", \"blue\", \"purple\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~bare\") {\n  plot(woo.bare, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with bare ground covariate\", pdf=pointtransect) \n  quantvals &lt;- quantile(sparrowSiteData$bare, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Bare ground\", legend=quantvals,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"blue\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~height\") {\n  plot(woo.ht, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with shrub height covariate\", pdf=pointtransect) \n  threeheights &lt;- quantile(sparrowSiteData$height, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Shrub height\", legend=threeheights,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"blue\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~shrubclass\") {\n  plot(woo.shrc, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with shrub height class covariate\", pdf=pointtransect) \n  add_df_covar_line(woo.shrc, data.frame(shrubclass=\"Low\"), col=\"red\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.shrc, data.frame(shrubclass=\"High\"), col=\"green\", lwd=2, lty=1, pdf=pointtransect) \n  legend(\"topright\", title=\"Shrub height class\", legend=c(\"Low\", \"High\"),\n         lwd=2, lty=1, col=c(\"red\", \"green\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~shrub\") {\n  plot(woo.shrc, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with percent shrub covariate\", pdf=pointtransect) \n  quantvals &lt;- quantile(sparrowSiteData$shrub, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect) \n  legend(\"topright\", title=\"Shrub cover\", legend=quantvals,\n         lwd=2, lty=1, col=c(\"red\", \"green\"))\n}",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. For line transect example of Brewers Sparrows, Effort was measured in meters, but we wish to produce our estimates of density in numbers per hectare. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. In both cases, division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\nsparrowSiteData$effArea &lt;- 2*predict(winner,newdata=data.frame(obs=sparrowSiteData$obs, \n                                                               bare=sparrowSiteData$bare,\n                                                               herb=sparrowSiteData$herb, \n                                                               shrub=sparrowSiteData$shrub,\n                                                               height=sparrowSiteData$height, \n                                                               shrubclass=sparrowSiteData$shrubclass),\n                            esw=TRUE)$fitted*transect.length/meterstohectares\nsitesadj &lt;- sparrowSiteData",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#digression-for-computing-effective-area",
    "href": "Pr9/countmodel-lines.html#digression-for-computing-effective-area",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Digression for computing Effective Area",
    "text": "Digression for computing Effective Area\nI could not convince predict.ds() to behave, so I reverted to first principles. In the function below, I “manually” compute the detection function in the presence of a covariate, demonstrated in the code as shrub as if shrub was the covariate in the detection function. Effective strip width is computed for each transect (below) by integrating this function at the specified level of the covariate for each transect.\n\ngz&lt;-function(z,\n             beta, sigintercept, sigcoef, DistWin,\n             key=\"HR\", w=max(z), predictor){\n#this is a generic detection function that returns the probability of detecting an animal\n#    z               generic distance (perpendicular) - can be scalar or vector\n#    beta            shape coefficient\n#    sigintercept  intercept coefficient for sigma\n#    sigcoef         coefficient for specific factor level\n#    key             the detection function key, works for hazard rate and half normal\n#    w               truncation distance, by default the max of the distances\n#    predictor     the covariate within the detection function influencing sigma\n#\n#RETURNS: a probability\n  \n  if(key != \"HN\" & key != \"HR\") {\n    stop(\"Argument 'key' must be either HN or HR\")\n  }\n  sigma &lt;- exp(sigintercept + sigcoef*predictor)\n  exponent &lt;- exp(beta)\n  if(key==\"HR\") {\n    scale.dist &lt;- z/sigma\n    inside &lt;- -(scale.dist)^(-exponent)\n    gx &lt;- 1 - exp(inside)\n  } else {\n    scale.dist &lt;- z  # debatably don't scale for half normal\n    inside &lt;- -(scale.dist^2/(2*sigma^2))\n    gx &lt;- exp(inside)\n  }\n  return(gx)\n}",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nFit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function. Consequently, the offset must also use the log transform.\nTo generalise the code, and recognising the same call to glm() will need to be made elsewhere in this analysis, we specify the GLM model we wish to fit as an object of type formula. This was specified in the Analysis parameters specification section above.\n\nunivarpredictor &lt;- all.vars(myglmmodel)[2]\nglmmodel &lt;- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum &lt;- summary(glmmodel)\ntablecaption &lt;- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nkable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM coefficients from counts as function of shrub with log(effective area) offset.\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-1.2375\n0.1431\n-8.6477\n0\n\n\nshrub\n0.0886\n0.0101\n8.8100\n0",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#visualise",
    "href": "Pr9/countmodel-lines.html#visualise",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment.\nWe plot the estimated density against the continuous univariate predictor.\n\nsitesadj$density &lt;- sitesadj$myCount / sitesadj$effArea",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est &lt;- vector(\"numeric\", length=nboot)\nslope.est &lt;- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame sparrowSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Brewer’s sparrow density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nnum.transects &lt;- dim(sparrowSiteData)[1]\nfor (theboot in 1:nboot) {\n  newdetects &lt;- data.frame() \n  bob &lt;- sample(sparrowSiteData$siteID, replace=TRUE, size=length(unique(sparrowSiteData$siteID)))\n#  print(unique(bob))\n  for (bootsite in 1:length(bob)) { \n    thissite &lt;- bob[bootsite] \n    glob &lt;- newsparrow[newsparrow$siteID==thissite, ]\n    glob$siteID &lt;- sprintf(\"rep%02d\", bootsite)\n    newdetects &lt;- rbind(newdetects, glob)  \n  }\n  newdetects &lt;- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detectcovar &lt;- as.formula(winner$ddf$ds$aux$ddfobj$scale$formula)\n  detfnmod &lt;- ds(data=newdetects, truncation=dettrunc, formula=detectcovar, quiet=TRUE, optimizer = \"R\")\n#  Compute effective area offset for each transect\n#  09Mar24 simplify to just transect-level data\n  newtransects &lt;- newdetects[!duplicated(newdetects$siteID), ]\n  esw &lt;- vector(mode=\"numeric\", length=num.transects)\n  hold &lt;- detfnmod$ddf$par\n  for (transect in 1:num.transects) { \n    esw[transect] &lt;- integrate(gz, lower=0, upper=dettrunc, key=\"HN\", \n                               beta=0, sigintercept = hold[1], \n                               sigcoef = hold[2], DistWin=FALSE, \n                               predictor=newtransects[transect, substr(winner$ddf$ds$aux$ddfobj$scale$formula, 2,10)])$value\n  }\n#   If point transects, use effArea =  2 * esw * pi * truncation^2  \n  newtransects$effArea &lt;- 2 * esw * transect.length/meterstohectares\n#   refit the GLM for this bootstrap replicate\n  glmresult &lt;- glm(formula= myglmmodel, family=\"poisson\", data=newtransects)\n  intercept.est[theboot] &lt;- coef(glmresult)[1]\n  slope.est[theboot] &lt;- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and Brewer’s sparrow density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData &lt;- data.frame(predictor=seq(min(sparrowSiteData[ , univarpredictor]),\n                                 max(sparrowSiteData[, univarpredictor]), length.out=100)) \norig.fit &lt;- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform &lt;- NULL\nfor (i in 1:nboot) {\n  mypredict &lt;- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform &lt;- c(longform, mypredict)\n}\nbig.df &lt;- data.frame(predict=longform)\nbig.df$shrub &lt;- predData$predictor\nbig.df$group &lt;- rep(1:nboot, each=length(predData$predictor))\n# point-wise confidence intervals\nquants &lt;- big.df %&gt;% \n  group_by(shrub) %&gt;% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %&gt;% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label &lt;- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label &lt;- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 &lt;- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=20, y=0.6, label= b0label, size=5) +\n  annotate(geom=\"text\", x=20, y=0.3, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Brewer's sparrow density as function of shrub cover\",\n       subtitle=paste0(\"Detection function model includes \", as.character(detectcovar)[2])) +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds &lt;- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 &lt;- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=bounds[1]-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=bounds[2]+.01, y=11, label=round(bounds[2],3)) \ncomplete &lt;- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Brewer’s sparrow density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Brewer’s sparrow.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. For the habitat characteristic shrub, there is a positive response of Brewer’s sparrow to increasing shrub cover.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "Lecture slides for 15 May 2025\n\n\n\n\n\n\n\n\nIt is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\n\n\n\n\n\nPhoto by Jamshaid Mughal on Unsplash",
    "crumbs": [
      "Multipliers",
      "Multipliers practical 9"
    ]
  },
  {
    "objectID": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "href": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "It is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.",
    "crumbs": [
      "Multipliers",
      "Multipliers practical 9"
    ]
  },
  {
    "objectID": "Pr9/Prac9_solution.html",
    "href": "Pr9/Prac9_solution.html",
    "title": "Analysis with multipliers solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nAnalysis with multipliers\n\n\n\nDung survey of deer\nReturning to the data described in (Marques et al., 2001), the following code loads the relevant packages and data. The perpendicular distances are measured in centimetres, effort along the transects measured in kilometres and areas in square kilometres.\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor &lt;- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")\n\nHere we did not perform a comprehensive examination of fitting a detection function to the detected pellet groups, however, as a general guideline, we truncated the longest 10% perpendicular distances.\n\ndeer.df &lt;- ds(sikadeer, key=\"hn\", truncation=\"10%\", convert_units = conversion.factor)\nplot(deer.df)\n\n\n\n\n\n\n\nprint(deer.df$dht$individuals$summary)\n\n  Region Area CoveredArea Effort    n  k        ER      se.ER     cv.ER\n1      A 13.9    0.005950   1.70 1217 13 715.88234 119.918872 0.1675120\n2      B 10.3    0.003850   1.10  396 10 359.99999  86.859289 0.2412758\n3      C  8.6    0.001575   0.45   17  3  37.77778   8.521202 0.2255612\n4      E  8.0    0.002975   0.85   30  5  35.29412  16.568939 0.4694533\n5      F 14.0    0.000700   0.20   29  1 145.00000   0.000000 0.0000000\n6      G 15.2    0.001400   0.40   32  3  80.00000  39.686269 0.4960784\n7      H 11.3    0.000700   0.20    3  1  15.00000   0.000000 0.0000000\n8      J  9.6    0.000350   0.10    7  1  70.00000   0.000000 0.0000000\n9  Total 90.9    0.017500   5.00 1731 37 201.90876   0.000000 0.0000000\n\n\nThe summary above shows that in blocks F, H and J there was only one transect and, as a consequence, it is not possible to calculate a variance empirically for the encounter rate in those blocks.\n\n\nEstimating decay rate from data\nA paper by Laing et al. (2003) describes field protocol for collecting data to estimate the mean persistence time of dung or nests to be used as multipliers. The code segment shown earlier analyses a file of such data via logistic regression to produce an estimate of mean persistence time and its associated uncertainty.\n\n\n\n\n\n\n\n\n\nMean persistence time                    SE                   %CV \n           163.396748             14.226998              8.707026 \n\n\nUsing the output from calling the MIKE.persistence function, the multipliers can be specified:\n\n# Create list of multipliers\nmult &lt;- list(creation = data.frame(rate=25, SE=0),\n             decay    = data.frame(rate=163, SE=14))\nprint(mult)\n\n$creation\n  rate SE\n1   25  0\n\n$decay\n  rate SE\n1  163 14\n\ndeer_ests &lt;- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                  convert_units=conversion.factor, multipliers=mult, \n                  stratification=\"effort_sum\", total_area = 100)\nprint(deer_ests, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : effort_sum \nVariance       : R2, n/L \nMultipliers    : creation, decay \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label  Area CoveredArea Effort    n  k      ER   se.ER cv.ER\n            A  13.9    0.005950   1.70 1217 13 715.882 119.919 0.168\n            B  10.3    0.003850   1.10  396 10 360.000  86.859 0.241\n            C   8.6    0.001575   0.45   17  3  37.778   8.521 0.226\n            E   8.0    0.002975   0.85   30  5  35.294  16.569 0.469\n            F  14.0    0.000700   0.20   29  1 145.000   0.000 0.000\n            G  15.2    0.001400   0.40   32  3  80.000  39.686 0.496\n            H  11.3    0.000700   0.20    3  1  15.000   0.000 0.000\n            J   9.6    0.000350   0.10    7  1  70.000   0.000 0.000\n        Total 100.0    0.017500   5.00 1731 37 346.200  45.234 0.131\n\nAbundance estimates:\n Region.Label Estimate      se    cv  LCI  UCI        df\n            A     1027 197.466 0.192  691 1527    20.797\n            B      383  99.167 0.259  220  667    11.955\n            C       34   8.200 0.244   15   75     2.759\n            E       29  13.959 0.479    9   99     4.329\n            F      210  19.752 0.094  174  252 60310.077\n            G      126  63.396 0.505   18  858     2.147\n            H       18   1.649 0.094   15   21 60310.077\n            J       69   6.538 0.094   58   83 60310.077\n        Total     3574 575.837 0.161 2560 4990    20.215\n\nComponent percentages of variance:\n Region.Label Detection    ER Multipliers\n            A      4.07 75.96       19.97\n            B      2.24 86.76       10.99\n            C      2.52 85.14       12.34\n            E      0.66 96.13        3.22\n            F     16.93  0.00       83.07\n            G      0.59 96.52        2.89\n            H     16.93  0.00       83.07\n            J     16.93  0.00       83.07\n        Total      8.09 91.91        0.00\n\n\nThere are a few things to notice:\n\noverall estimate of density\n\nmost effort took place in woodland A where deer density was high. Therefore, the overall estimate is between the estimated density in woodland A and the lower densities in the other woodlands.\n\ncomponents of variance\n\nwe now have uncertainty associated with the encounter rate, detection function and decay rate (note there was no uncertainty associated with the production rate) and so the components of variation for all three components are provided.\n\n\nIn woodland A, there were 13 transects on which over 1,200 pellet groups were detected: uncertainty in the estimated density was 19% and the variance components were apportioned as detection probability 4%, encounter rate 76% and multipliers 20%.\nIn woodland E, there were 5 transects and 30 pellet groups resulting in a coefficient of variation (CV) of 48%: the variance components were apportioned as detection probability 0.7%, encounter rate 96% and multipliers 3%.\nIn woodland F only a single transect was placed and the CV of density of 9% was apportioned as detection probability 17% and multipliers 83%. Do you trust this assessment of uncertainty in the density of deer in this woodland? We are missing a component of variation because we were negligent in placing only a single transect in this woodland and so are left to ‘assume’ there is no variability in encounter rate in this woodland.\nBy the same token, we are left to assume there is no variability in production rates between deer because we have not included a measure of uncertainty in this facet of our analysis.\n\n\nCue counting survey of whales\n\ndata(CueCountingExample)\nhead(CueCountingExample, n=3)\n\n  Region.Label  Area Sample.Label Cue.rate Cue.rate.SE Cue.rate.df object distance\n1            B 85000         2.18       25           5           1     NA       NA\n2            B 85000         2.19       25           5           1     NA       NA\n3            B 85000         2.20       25           5           1     NA       NA\n  Sample.Fraction Sample.Fraction.SE Search.time bss   sp size Study.Area\n1             0.5                  0   0.1333333  NA &lt;NA&gt;   NA     whales\n2             0.5                  0   0.1000000  NA &lt;NA&gt;   NA     whales\n3             0.5                  0   0.4333333  NA &lt;NA&gt;   NA     whales\n\nCueCountingExample$Effort &lt;- CueCountingExample$Search.time\ncuerates &lt;- CueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\")]\ncuerates &lt;- unique(cuerates)\nnames(cuerates) &lt;- c(\"rate\", \"SE\", \"df\")\nmult &lt;- list(creation=cuerates)\nprint(mult)\n\n$creation\n  rate SE df\n1   25  5  1\n\n\nThe estimated cue rate, \\(\\hat \\nu\\), is 25 cues per unit time (per hour in this case). Its standard error is 5, therefore the CV of cue rate is \\(5/25 = 0.2\\) (20%).\n\n# Tidy up data by removing surplus columns\nCueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\", \"Sample.Fraction\", \n                       \"Sample.Fraction.SE\")] &lt;- list(NULL)\ntrunc &lt;- 1.2\nwhale.df.hn &lt;- ds(CueCountingExample, key=\"hn\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nwhale.df.hr &lt;- ds(CueCountingExample, key=\"hr\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nknitr::kable(summarize_ds_models(whale.df.hn, whale.df.hr), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~1\n0.810\n0.239\n0.043\n0.00\n\n\n\nHazard-rate\n~1\n0.747\n0.281\n0.065\n3.07\n\n\n\n\n\nHalf the circle (point transect) was searched and so the sampling fraction \\(\\phi /2\\pi = 0.5\\). Therefore, \\(\\phi = \\pi\\) (\\(\\phi\\) must be in radians).\nThe following commands obtain density estimates assuming no stratification (strat_formula=~1).\n\nwhale.est.hn &lt;- dht2(whale.df.hn, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hn, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    13654 5263.071 0.385 6112 30500 13.058\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      21.7 51.38       26.92\n\nwhale.est.hr &lt;- dht2(whale.df.hr, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hr, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    11590 4777.034 0.412 5017 26774 16.593\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total     31.52 44.93       23.54\n\n\nA half normal detection function was chosen and whale abundance was estimated to be 13,654 whales with a 95% confidence interval (6,112: 30,500).\nNote the large difference between the half normal estimate and the estimate from the hazard rate model, which is 11,590 whales, with 95% confidence interval (5,017; 26773). Remember that the key parameter in a cue counting analysis is \\(h(0)\\), the slope of the fitted pdf to the observed data at distance zero. The difference between the estimates for the different key function is the difference between these slopes for the two models (Fig. 2):\nplot(whale.df.hn, pdf=TRUE, main=\"Half normal\")\nplot(whale.df.hr, pdf=TRUE, main=\"Hazard rate\")\n\n\n\n\n\n\nHalf normal PDF\n\n\n\n\n\n\n\nHazard rate PDF\n\n\n\n\n\nCue counting estimates of detection probability are more volatile than those from line transect surveys, because on a cue counting survey you have few data where you need it most to estimate \\(h(0)\\) - namely at distances close to zero. As a consequence, cue-counting surveys require higher cue sample size for reliable estimation than samples of animals for line transect surveys.\nDon’t worry too much about the apparent lack of fit in the first interval, or two, in Figure 2 - remember the sample size is very small in these intervals. Use the plot above and the goodness-of-fit statistics to guide you about the fit of your model.\n\n\nCue counting survey of songbirds (optional)\nAnalysis of the cue count data of winter wrens described by Buckland (2006).\n\ndata(wren_cuecount)\ncuerate &lt;- unique(wren_cuecount[ , c(\"Cue.rate\",\"Cue.rate.SE\")])\nnames(cuerate) &lt;- c(\"rate\", \"SE\")\nmult &lt;- list(creation=cuerate)\nprint(mult)\n\n$creation\n    rate     SE\n1 1.4558 0.2428\n\n# Search time is the effort - this is 2 * 5min visits\nwren_cuecount$Effort &lt;- wren_cuecount$Search.time\nw3.hr &lt;- ds(wren_cuecount, transect=\"point\", key=\"hr\", adjustment=NULL, truncation=92.5)\n\nThe sampling fraction for these data will be 1 because the full circle around the observer was searched.\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\nw3.est &lt;- dht2(w3.hr, flatfile=wren_cuecount, strat_formula=~1,\n               multipliers=mult, convert_units=conversion.factor)\n# NB \"Effort\" here is sum(Search.time) in minutes\n# NB \"CoveredArea\" here is pi * w^2 * sum(Search.time)\nprint(w3.est, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total 33.2    860.1681    320 765 32 2.391 0.236 0.099\n\nDensity estimates:\n .Label Estimate    se  cv    LCI    UCI      df\n  Total   1.2092 0.242 0.2 0.8195 1.7843 522.541\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      6.14 24.33       69.54\n\n\nNote the large proportion of the uncertainty in winter wren density stems from variability in cue (song) rate. Analyses of the cue count data are necessarily rather subjective as the data show substantial over-dispersion (a single bird may give many song bursts all from the same location during a five minute count). In this circumstance, goodness-of-fit tests are misleading and care must be taken not to over-fit the data (i.e. fit a complicated detection function).\nplot(w3.hr, pdf=TRUE, main=\"Cue distances of winter wren.\")\ngof_ds(w3.hr)\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 1.70062 p-value = 6.04794e-05\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2\n\n\nLaing, S. E., Buckland, S. T., Burn, R. W., Lambie, D., & Amphlett, A. (2003). Dung and nest surveys: Estimating decay rate. Journal of Applied Ecology, 40, 1102–1111. https://doi.org/10.1111/j.1365-2664.2003.00861.x\n\n\nMarques, F. F. C., Buckland, S. T., Goffin, D., Dixon, C. E., Borchers, D. L., Mayle, B. A., & Peace, A. J. (2001). Estimating deer abundance from line transect surveys of dung: sika deer in southern Scotland. Journal of Applied Ecology, 38(2), 349–363. https://doi.org/10.1046/j.1365-2664.2001.00584.x",
    "crumbs": [
      "Multipliers",
      "Analysis with multipliers **solution** 💡"
    ]
  },
  {
    "objectID": "Pr1/sampling.html",
    "href": "Pr1/sampling.html",
    "title": "Sampling practical 1",
    "section": "",
    "text": "Lecture slides for 06 October 2025\n\n\n\n\n\n\n\n\nThis exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\n\n\n\n\n\nPhoto by Shelby Cohron on Unsplash\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region.",
    "crumbs": [
      "Sampling",
      "Sampling practical 1"
    ]
  },
  {
    "objectID": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "href": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "title": "Sampling practical 1",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.",
    "crumbs": [
      "Sampling",
      "Sampling practical 1"
    ]
  },
  {
    "objectID": "Pr2/detnfns.html",
    "href": "Pr2/detnfns.html",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "Lecture slides for 07 October 2025\n\n\n\n\n\n\n\n\nThe data are the famous “ducknest” data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\n\n\n\n\n\nPhoto by Freysteinn G. Jonsson on Unsplash\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit.",
    "crumbs": [
      "Detection functions",
      "Detection functions practical 2"
    ]
  },
  {
    "objectID": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "href": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.",
    "crumbs": [
      "Detection functions",
      "Detection functions practical 2"
    ]
  },
  {
    "objectID": "Pr3/criticism.html",
    "href": "Pr3/criticism.html",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "Lecture slides for 08 October 2025\n\n\n\n\n\n\n\n\nThis practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Quarto (.qmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.\n\n\n\n\n\nPhoto by Blake Cheek on Unsplash",
    "crumbs": [
      "Criticism",
      "Model criticism practical 3"
    ]
  },
  {
    "objectID": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "href": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Quarto (.qmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.",
    "crumbs": [
      "Criticism",
      "Model criticism practical 3"
    ]
  },
  {
    "objectID": "Pr4/precision.html",
    "href": "Pr4/precision.html",
    "title": "Precision practical 4",
    "section": "",
    "text": "Lecture slides for 09 October 2025\n\n\n\n\n\n\n\n\nThis exercise examines the ability to precisely estimate density from line transect data.\n\n\n\n\n\nPhoto by Andrea Sonda on Unsplash\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set.",
    "crumbs": [
      "Precision",
      "Precision practical 4"
    ]
  },
  {
    "objectID": "Pr4/precision.html#precision-of-density-estimates",
    "href": "Pr4/precision.html#precision-of-density-estimates",
    "title": "Precision practical 4",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.",
    "crumbs": [
      "Precision",
      "Precision practical 4"
    ]
  },
  {
    "objectID": "Pr5/points.html",
    "href": "Pr5/points.html",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Lecture slides for 10 October 2025\n\n\n\n\n\n\n\n\nHaving mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\n\n\n\n\n\nPhoto by Vincent van Zalinge on Unsplash\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method.",
    "crumbs": [
      "Points",
      "Point transects practical 5"
    ]
  },
  {
    "objectID": "Pr5/points.html#analysis-of-point-transect-data",
    "href": "Pr5/points.html#analysis-of-point-transect-data",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.",
    "crumbs": [
      "Points",
      "Point transects practical 5"
    ]
  }
]