[
  {
    "objectID": "Pr9/Prac9_solution.html",
    "href": "Pr9/Prac9_solution.html",
    "title": "Analysis with multipliers solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nAnalysis with multipliers\n\n\n\nDung survey of deer\nReturning to the data described in (Marques et al., 2001), the following code loads the relevant packages and data. The perpendicular distances are measured in centimetres, effort along the transects measured in kilometres and areas in square kilometres.\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor &lt;- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")\n\nHere we did not perform a comprehensive examination of fitting a detection function to the detected pellet groups, however, as a general guideline, we truncated the longest 10% perpendicular distances.\n\ndeer.df &lt;- ds(sikadeer, key=\"hn\", truncation=\"10%\", convert_units = conversion.factor)\nplot(deer.df)\n\n\n\n\n\n\n\nprint(deer.df$dht$individuals$summary)\n\n  Region Area CoveredArea Effort    n  k        ER      se.ER     cv.ER\n1      A 13.9    0.005950   1.70 1217 13 715.88234 119.918872 0.1675120\n2      B 10.3    0.003850   1.10  396 10 359.99999  86.859289 0.2412758\n3      C  8.6    0.001575   0.45   17  3  37.77778   8.521202 0.2255612\n4      E  8.0    0.002975   0.85   30  5  35.29412  16.568939 0.4694533\n5      F 14.0    0.000700   0.20   29  1 145.00000   0.000000 0.0000000\n6      G 15.2    0.001400   0.40   32  3  80.00000  39.686269 0.4960784\n7      H 11.3    0.000700   0.20    3  1  15.00000   0.000000 0.0000000\n8      J  9.6    0.000350   0.10    7  1  70.00000   0.000000 0.0000000\n9  Total 90.9    0.017500   5.00 1731 37 201.90876   0.000000 0.0000000\n\n\nThe summary above shows that in blocks F, H and J there was only one transect and, as a consequence, it is not possible to calculate a variance empirically for the encounter rate in those blocks.\n\n\nEstimating decay rate from data\nA paper by Laing et al. (2003) describes field protocol for collecting data to estimate the mean persistence time of dung or nests to be used as multipliers. The code segment shown earlier analyses a file of such data via logistic regression to produce an estimate of mean persistence time and its associated uncertainty.\n\n\n\n\n\n\n\n\n\nMean persistence time                    SE                   %CV \n           163.396748             14.226998              8.707026 \n\n\nUsing the output from calling the MIKE.persistence function, the multipliers can be specified:\n\n# Create list of multipliers\nmult &lt;- list(creation = data.frame(rate=25, SE=0),\n             decay    = data.frame(rate=163, SE=14))\nprint(mult)\n\n$creation\n  rate SE\n1   25  0\n\n$decay\n  rate SE\n1  163 14\n\ndeer_ests &lt;- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                  convert_units=conversion.factor, multipliers=mult, \n                  stratification=\"effort_sum\", total_area = 100)\nprint(deer_ests, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : effort_sum \nVariance       : R2, n/L \nMultipliers    : creation, decay \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label  Area CoveredArea Effort    n  k      ER   se.ER cv.ER\n            A  13.9    0.005950   1.70 1217 13 715.882 119.919 0.168\n            B  10.3    0.003850   1.10  396 10 360.000  86.859 0.241\n            C   8.6    0.001575   0.45   17  3  37.778   8.521 0.226\n            E   8.0    0.002975   0.85   30  5  35.294  16.569 0.469\n            F  14.0    0.000700   0.20   29  1 145.000   0.000 0.000\n            G  15.2    0.001400   0.40   32  3  80.000  39.686 0.496\n            H  11.3    0.000700   0.20    3  1  15.000   0.000 0.000\n            J   9.6    0.000350   0.10    7  1  70.000   0.000 0.000\n        Total 100.0    0.017500   5.00 1731 37 346.200  45.234 0.131\n\nAbundance estimates:\n Region.Label Estimate      se    cv  LCI  UCI        df\n            A     1027 197.474 0.192  691 1527    20.797\n            B      383  99.171 0.259  220  667    11.955\n            C       34   8.200 0.244   15   75     2.759\n            E       29  13.959 0.479    9   99     4.329\n            F      210  19.752 0.094  174  252 60314.199\n            G      126  63.399 0.505   18  858     2.147\n            H       18   1.649 0.094   15   21 60314.199\n            J       69   6.539 0.094   58   83 60314.199\n        Total     3575 575.860 0.161 2560 4990    20.215\n\nComponent percentages of variance:\n Region.Label Detection    ER Multipliers\n            A      4.07 75.96       19.97\n            B      2.24 86.76       10.99\n            C      2.52 85.14       12.34\n            E      0.66 96.13        3.22\n            F     16.93  0.00       83.07\n            G      0.59 96.52        2.89\n            H     16.93  0.00       83.07\n            J     16.93  0.00       83.07\n        Total      8.09 91.91        0.00\n\n\nThere are a few things to notice:\n\noverall estimate of density\n\nmost effort took place in woodland A where deer density was high. Therefore, the overall estimate is between the estimated density in woodland A and the lower densities in the other woodlands.\n\ncomponents of variance\n\nwe now have uncertainty associated with the encounter rate, detection function and decay rate (note there was no uncertainty associated with the production rate) and so the components of variation for all three components are provided.\n\n\nIn woodland A, there were 13 transects on which over 1,200 pellet groups were detected: uncertainty in the estimated density was 19% and the variance components were apportioned as detection probability 4%, encounter rate 76% and multipliers 20%.\nIn woodland E, there were 5 transects and 30 pellet groups resulting in a coefficient of variation (CV) of 48%: the variance components were apportioned as detection probability 0.7%, encounter rate 96% and multipliers 3%.\nIn woodland F only a single transect was placed and the CV of density of 9% was apportioned as detection probability 17% and multipliers 83%. Do you trust this assessment of uncertainty in the density of deer in this woodland? We are missing a component of variation because we were negligent in placing only a single transect in this woodland and so are left to ‘assume’ there is no variability in encounter rate in this woodland.\nBy the same token, we are left to assume there is no variability in production rates between deer because we have not included a measure of uncertainty in this facet of our analysis.\n\n\nCue counting survey of whales\n\ndata(CueCountingExample)\nhead(CueCountingExample, n=3)\n\n  Region.Label  Area Sample.Label Cue.rate Cue.rate.SE Cue.rate.df object distance\n1            B 85000         2.18       25           5           1     NA       NA\n2            B 85000         2.19       25           5           1     NA       NA\n3            B 85000         2.20       25           5           1     NA       NA\n  Sample.Fraction Sample.Fraction.SE Search.time bss   sp size Study.Area\n1             0.5                  0   0.1333333  NA &lt;NA&gt;   NA     whales\n2             0.5                  0   0.1000000  NA &lt;NA&gt;   NA     whales\n3             0.5                  0   0.4333333  NA &lt;NA&gt;   NA     whales\n\nCueCountingExample$Effort &lt;- CueCountingExample$Search.time\ncuerates &lt;- CueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\")]\ncuerates &lt;- unique(cuerates)\nnames(cuerates) &lt;- c(\"rate\", \"SE\", \"df\")\nmult &lt;- list(creation=cuerates)\nprint(mult)\n\n$creation\n  rate SE df\n1   25  5  1\n\n\nThe estimated cue rate, \\(\\hat \\nu\\), is 25 cues per unit time (per hour in this case). Its standard error is 5, therefore the CV of cue rate is \\(5/25 = 0.2\\) (20%).\n\n# Tidy up data by removing surplus columns\nCueCountingExample[ ,c(\"Cue.rate\", \"Cue.rate.SE\", \"Cue.rate.df\", \"Sample.Fraction\", \n                       \"Sample.Fraction.SE\")] &lt;- list(NULL)\ntrunc &lt;- 1.2\nwhale.df.hn &lt;- ds(CueCountingExample, key=\"hn\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nwhale.df.hr &lt;- ds(CueCountingExample, key=\"hr\", transect=\"point\", adjustment=NULL,\n                  truncation=trunc)\nknitr::kable(summarize_ds_models(whale.df.hn, whale.df.hr), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~1\n0.810\n0.239\n0.043\n0.00\n\n\n\nHazard-rate\n~1\n0.747\n0.281\n0.065\n3.07\n\n\n\n\n\nHalf the circle (point transect) was searched and so the sampling fraction \\(\\phi /2\\pi = 0.5\\). Therefore, \\(\\phi = \\pi\\) (\\(\\phi\\) must be in radians).\nThe following commands obtain density estimates assuming no stratification (strat_formula=~1).\n\nwhale.est.hn &lt;- dht2(whale.df.hn, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hn, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    13653 5262.964 0.385 6112 30500 13.058\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      21.7 51.38       26.92\n\nwhale.est.hr &lt;- dht2(whale.df.hr, flatfile=CueCountingExample, strat_formula=~1, \n                     multipliers=mult, sample_fraction=0.5)\nprint(whale.est.hr, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 0.5 \n\n\nSummary statistics:\n .Label   Area CoveredArea Effort  n  k    ER se.ER cv.ER\n  Total 168000     82.4932  36.47 40 92 1.097 0.303 0.276\n\nAbundance estimates:\n .Label Estimate       se    cv  LCI   UCI     df\n  Total    11589 4776.542 0.412 5017 26771 16.591\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total     31.52 44.94       23.55\n\n\nA half normal detection function was chosen and whale abundance was estimated to be 13,654 whales with a 95% confidence interval (6,112: 30,500).\nNote the large difference between the half normal estimate and the estimate from the hazard rate model, which is 11,590 whales, with 95% confidence interval (5,017; 26773). Remember that the key parameter in a cue counting analysis is \\(h(0)\\), the slope of the fitted pdf to the observed data at distance zero. The difference between the estimates for the different key function is the difference between these slopes for the two models (Fig. 2):\nplot(whale.df.hn, pdf=TRUE, main=\"Half normal\")\nplot(whale.df.hr, pdf=TRUE, main=\"Hazard rate\")\n\n\n\n\n\n\nHalf normal PDF\n\n\n\n\n\n\n\nHazard rate PDF\n\n\n\n\n\nCue counting estimates of detection probability are more volatile than those from line transect surveys, because on a cue counting survey you have few data where you need it most to estimate \\(h(0)\\) - namely at distances close to zero. As a consequence, cue-counting surveys require higher cue sample size for reliable estimation than samples of animals for line transect surveys.\nDon’t worry too much about the apparent lack of fit in the first interval, or two, in Figure 2 - remember the sample size is very small in these intervals. Use the plot above and the goodness-of-fit statistics to guide you about the fit of your model.\n\n\nCue counting survey of songbirds (optional)\nAnalysis of the cue count data of winter wrens described by Buckland (2006).\n\ndata(wren_cuecount)\ncuerate &lt;- unique(wren_cuecount[ , c(\"Cue.rate\",\"Cue.rate.SE\")])\nnames(cuerate) &lt;- c(\"rate\", \"SE\")\nmult &lt;- list(creation=cuerate)\nprint(mult)\n\n$creation\n    rate     SE\n1 1.4558 0.2428\n\n# Search time is the effort - this is 2 * 5min visits\nwren_cuecount$Effort &lt;- wren_cuecount$Search.time\nw3.hr &lt;- ds(wren_cuecount, transect=\"point\", key=\"hr\", adjustment=NULL, truncation=92.5)\n\nThe sampling fraction for these data will be 1 because the full circle around the observer was searched.\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\nw3.est &lt;- dht2(w3.hr, flatfile=wren_cuecount, strat_formula=~1,\n               multipliers=mult, convert_units=conversion.factor)\n# NB \"Effort\" here is sum(Search.time) in minutes\n# NB \"CoveredArea\" here is pi * w^2 * sum(Search.time)\nprint(w3.est, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : P2, n/L \nMultipliers    : creation \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total 33.2    860.1681    320 765 32 2.391 0.236 0.099\n\nDensity estimates:\n .Label Estimate    se  cv    LCI    UCI      df\n  Total   1.2092 0.242 0.2 0.8195 1.7843 522.541\n\nComponent percentages of variance:\n .Label Detection    ER Multipliers\n  Total      6.14 24.33       69.54\n\n\nNote the large proportion of the uncertainty in winter wren density stems from variability in cue (song) rate. Analyses of the cue count data are necessarily rather subjective as the data show substantial over-dispersion (a single bird may give many song bursts all from the same location during a five minute count). In this circumstance, goodness-of-fit tests are misleading and care must be taken not to over-fit the data (i.e. fit a complicated detection function).\nplot(w3.hr, pdf=TRUE, main=\"Cue distances of winter wren.\")\ngof_ds(w3.hr)\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 1.70062 p-value = 6.04794e-05\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2\n\n\nLaing, S. E., Buckland, S. T., Burn, R. W., Lambie, D., & Amphlett, A. (2003). Dung and nest surveys: Estimating decay rate. Journal of Applied Ecology, 40, 1102–1111. https://doi.org/10.1111/j.1365-2664.2003.00861.x\n\n\nMarques, F. F. C., Buckland, S. T., Goffin, D., Dixon, C. E., Borchers, D. L., Mayle, B. A., & Peace, A. J. (2001). Estimating deer abundance from line transect surveys of dung: sika deer in southern Scotland. Journal of Applied Ecology, 38(2), 349–363. https://doi.org/10.1046/j.1365-2664.2001.00584.x",
    "crumbs": [
      "Multipliers",
      "Analysis with multipliers **solution** 💡"
    ]
  },
  {
    "objectID": "Pr9/multipliers.html",
    "href": "Pr9/multipliers.html",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "Lecture slides for 17 October 2024\n\n\n\n\n\n\n\n\nIt is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.\n\n\n\n\n\nPhoto by Jamshaid Mughal on Unsplash",
    "crumbs": [
      "Multipliers",
      "Multipliers practical 9"
    ]
  },
  {
    "objectID": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "href": "Pr9/multipliers.html#multipliers-and-indirect-surveys",
    "title": "Multipliers practical 9",
    "section": "",
    "text": "It is potentially easier to estimate the density of things produced by animals (songs, dung, nests) than to estimate density of animals. In this situation, the calculated density of animal cues needs to be transformed into an estimate of animal density. This requires companion field investigations to estimate cue production rate and possibly, cue disappearance rate. Those rates are termed multipliers.\nThis exercise presents data on a Sika deer dung study in southern Scotland, where data on dung disappearance needs to be analysed via logistic regression to produce the disappearance rate multiplier. There is a study of whale blows from a moving ship–effort is measured in units of time rather than distance. Finally, we return to the Montrave Estate study of Prof Buckland to estimate density of winter wrens via detections of song; necessitated because of evasive movement of winter wrens in the field.",
    "crumbs": [
      "Multipliers",
      "Multipliers practical 9"
    ]
  },
  {
    "objectID": "Pr9/countmodel-points.html",
    "href": "Pr9/countmodel-points.html",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable\n\\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different."
  },
  {
    "objectID": "Pr9/countmodel-points.html#data-organisation",
    "href": "Pr9/countmodel-points.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Data organisation",
    "text": "Data organisation\nNote there is no Effort field for these point count data.\n\nnewthrasher &lt;- merge(thrasherDetectionData, thrasherSiteData, by=\"siteID\", all=TRUE)\nnames(newthrasher) &lt;- sub(\"observer\", \"obs\", names(newthrasher))\nnames(newthrasher) &lt;- sub(\"dist\", \"distance\", names(newthrasher))\nnewthrasher$distance &lt;- as.numeric(newthrasher$distance)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-points.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nSpecification of run-specific parameters to analyse the Sage thrasher data set. Particularly note the logical value assigned to pointtransect. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height. The same predictors, with the exception of observer could be used to model the thrasher counts.\n\n\n\n\n\n\nWarning\n\n\n\n\npointtransect &lt;- TRUE        # survey conducted using lines or points\ndettrunc &lt;- 170  # truncation for detection function\nmyglmmodel &lt;- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot &lt;- 10      # number of bootstrap replicates\nset.seed(7079)   # random number seed"
  },
  {
    "objectID": "Pr9/countmodel-points.html#fit-detection-function",
    "href": "Pr9/countmodel-points.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Sage thrasher data set.\n\nsurveytype &lt;- ifelse(pointtransect, \"point\", \"line\")\nwoo.o &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~obs,\n            transect=surveytype, quiet=TRUE)\nwoo.shrub &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(shrub),\n           transect=surveytype, quiet=TRUE)\nwoo.herb &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(herb),\n           transect=surveytype, quiet=TRUE)\nwoo.height &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~scale(height),\n           transect=surveytype, quiet=TRUE)\nwoo &lt;- ds(data=newthrasher, truncation=dettrunc, formula=~1,\n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo, woo.o, woo.shrub, woo.herb, woo.height),\n             digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\",\n             row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~obs\n0.009\n0.292\n0.039\n0.000\n\n\n\nHalf-normal with cosine adjustment term of order 2\n~1\n0.096\n0.394\n0.124\n12.868\n\n\n\nHalf-normal\n~scale(herb)\n0.032\n0.331\n0.034\n17.893\n\n\n\nHalf-normal\n~scale(shrub)\n0.035\n0.331\n0.034\n17.901\n\n\n\nHalf-normal\n~scale(height)\n0.037\n0.333\n0.034\n19.168\n\n\n\n\n\nNone of the covariates contribute to fit of the detection function models for thrashers. Inference should be based upon the no covariate model (that passes the goodness of fit test), but for testing purposes, we will use the model with the observer covariate. Use of the observer covariate model will retain between-point variability in effective area; useful for testing purposes.\n\nPlot of detection function\n\nplot(woo.o, showpoints=FALSE, main=\"Sage thrasher\\nDetection with observer covariate\",\n     pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"dark green\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, pdf=pointtransect) \nadd_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, pdf=pointtransect)\nadd_df_covar_line(woo.o, data.frame(obs=\"obs6\"), col=\"coral\", lwd=2, pdf=pointtransect)\nlegend(\"topright\", title=\"Observer\", legend=1:6,\n       lwd=2, lty=2, col=c(\"red\", \"green\", \"dark green\", \"blue\", \"purple\", \"coral\"))"
  },
  {
    "objectID": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-points.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. Division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\neffAreafn &lt;- function(detfnobj, newdata, areaconv, truncation, pointflag) {\n#\n# Input: detfnobj - model object created by `ds()`\n#        newdata - data frame of detections and site-level covariates\n#        areaconv - conversion of transect length units to area units (m2 to ha for example)\n#        truncation - truncation distance for detection function fit\n#        pointflag - TRUE if point transect survey, FALSE for line transects\n#\n# Output: data frame of transects with effective area appended for each transect (to be used as offset)\n#\n  newdata$Pa &lt;- NA\n  k &lt;- 0\n  for (i in 1:dim(newdata)[1]) {\n    if(newdata[i, \"distance\"] &lt;= truncation & !is.na(newdata[i,\"distance\"])) {\n      k &lt;- k+1\n      newdata$Pa[i] &lt;- detfnobj$ddf$fitted[k]\n    }\n  }\n  if(!pointflag) {\n    newdata$effArea &lt;- ifelse(is.na(newdata$Pa), NA, 2 * newdata$Pa * truncation * newdata$Effort / areaconv)\n  }else{\n    newdata$effArea &lt;- ifelse(is.na(newdata$Pa), NA, newdata$Pa * truncation^2 * pi / areaconv)\n  }\n  result &lt;- newdata[!duplicated(newdata$siteID),]\n#    newdata must contain all covariates used in the detection function!  \n  if(!pointflag) {\n#     predict for line transects when esw=TRUE returns the ESW that needs to be converted to effective area    \n    result$effArea &lt;- ifelse(result$myCount==0,\n                              2 * predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted * result$Effort / areaconv,\n                              result$effArea)\n  }else{\n#     predict for point transects when esw=TRUE returns the effective area    \n    result$effArea &lt;- ifelse(result$myCount==0,\n                              predict(detfnobj, \n                                          newdata=data.frame(obs=result$obs, bare=result$bare,\n                                                             herb=result$herb, shrub=result$shrub,\n                                                             height=result$height),\n                                          esw = TRUE)$fitted / areaconv,\n                              result$effArea)\n    \n  }\n  return(result)\n}\n\nThe function has an argument pointflag used to indicate whether point transect sampling was used. If so, the correct effective area calculations are performed.\n\nsitesadj &lt;- effAreafn(woo.o, newthrasher, 10000, dettrunc, pointflag = pointtransect)"
  },
  {
    "objectID": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-points.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nAs for the line transect example, fit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function.\n\nunivarpredictor &lt;- all.vars(myglmmodel)[2]\nglmmodel &lt;- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum &lt;- summary(glmmodel)\ntablecaption &lt;- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nknitr::kable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM model output\n\n\n\n\nGLM coefficients from counts as function of shrub with log(effective area) offset.\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-2.3977\n0.8794\n-2.7266\n0.0064\n\n\nshrub\n0.0818\n0.0419\n1.9538\n0.0507"
  },
  {
    "objectID": "Pr9/countmodel-points.html#visualise",
    "href": "Pr9/countmodel-points.html#visualise",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment."
  },
  {
    "objectID": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-points.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est &lt;- vector(\"numeric\", length=nboot)\nslope.est &lt;- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame thrasherSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Sage thrasher density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nfor (theboot in 1:nboot) {\n  newdetects &lt;- data.frame() \n  bob &lt;- sample(thrasherSiteData$siteID, replace=TRUE, size=length(unique(thrasherSiteData$siteID)))\n  for (bootsite in 1:length(bob)) { \n    thissite &lt;- bob[bootsite] \n    glob &lt;- newthrasher[newthrasher$siteID==thissite, ]\n    glob$siteID &lt;- sprintf(\"rep%02d\", bootsite)\n    newdetects &lt;- rbind(newdetects, glob)  \n    }\n  newdetects &lt;- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detfnmod &lt;- ds(data=newdetects, truncation=dettrunc, formula=~obs, transect=surveytype, quiet=TRUE)\n#  Compute effective area offset for each transect\n  bootsitesadj &lt;- effAreafn(detfnmod, newdetects, 10000, dettrunc, pointflag = pointtransect)\n#   refit the GLM for this bootstrap replicate\n  glmresult &lt;- glm(formula= myglmmodel, family=\"poisson\", data=bootsitesadj)\n  intercept.est[theboot] &lt;- coef(glmresult)[1]\n  slope.est[theboot] &lt;- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and bird density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData &lt;- data.frame(predictor=seq(min(thrasherSiteData[ , univarpredictor]),\n                                 max(thrasherSiteData[, univarpredictor]), length.out=50)) \norig.fit &lt;- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform &lt;- NULL\nfor (i in 1:nboot) {\n  mypredict &lt;- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform &lt;- c(longform, mypredict)\n}\nbig.df &lt;- data.frame(predict=longform)\nbig.df$shrub &lt;- predData$predictor\nbig.df$group &lt;- rep(1:nboot, each=length(predData$predictor))\nalpha &lt;- 0.05\n# point-wise confidence intervals\nquants &lt;- big.df %&gt;% \n  group_by(shrub) %&gt;% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %&gt;% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label &lt;- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label &lt;- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 &lt;- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=24, y=1.5, label= b0label, size=5) +\n  annotate(geom=\"text\", x=24, y=1.4, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Sage thrasher density as function of shrub cover\") +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds &lt;- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 &lt;- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=.18, y=11, label=round(bounds[2],3)) \ncomplete &lt;- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Sage Thrasher density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Sage Thrasher."
  },
  {
    "objectID": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-points.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density–points",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. In the case of the Sage Thrasher, the species appears to exhibit little response to the presence of shrub cover during this study. Note that the range of shrub cover is not extensive (~16% to ~26%). We can make no inference regarding the density:habitat relationship outside this range in shrub cover."
  },
  {
    "objectID": "Pr8/Prac8_solution.html",
    "href": "Pr8/Prac8_solution.html",
    "title": "Covariates in the detection function solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nCovariates in the detection function",
    "crumbs": [
      "Covariates",
      "Covariates in the detection function **solution** 💡"
    ]
  },
  {
    "objectID": "Pr8/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "href": "Pr8/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "title": "Covariates in the detection function solution 💡",
    "section": "Colourful plot noting effect of cue type",
    "text": "Colourful plot noting effect of cue type\nJust an example of using the function add_df_covar_line to visually explore consequences of covariates on the detection function. A regular call to plot() is first used to produce the histogram and average detection function line; subsequent calls to the new function with different values of the covariate of interest completes the plot.\n\nplot(etp.hr.cue, main=\"ETP dolphin survey\", showpoints=FALSE)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=1), col='red', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=2), col='blue', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=3), col='green', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=4), col='purple', lwd=2, lty=1)\nlegend(\"topright\", legend=c(\"Birds\",\"Splashes\",\"Unspecified\",\"Floating objects\"),\n       col=c(\"red\", \"blue\", \"green\", \"purple\"), lwd=2, title = \"Cue type\")\n\n\n\n\nDetection function with cue type as covariate.",
    "crumbs": [
      "Covariates",
      "Covariates in the detection function **solution** 💡"
    ]
  },
  {
    "objectID": "Pr8/Group-size-covariate.html",
    "href": "Pr8/Group-size-covariate.html",
    "title": "Size bias—how large is the problem?",
    "section": "",
    "text": "Supplement\n\n\n\nCorrecting size bias via size as a covariate. When does it matter?",
    "crumbs": [
      "Covariates",
      "Size bias---how large is the problem?"
    ]
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-including-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis including group size covariate",
    "text": "Analysis including group size covariate\n\n\n\n\n\n\n\n\n\nThe distribution of computed average group size centred on the true size of 10 and there was no problem with fitting a detection function. The average over the simulations estimated number of individuals was 2003.05, a bias of 0.2%.",
    "crumbs": [
      "Covariates",
      "Size bias---how large is the problem?"
    ]
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-group-size-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without group size covariate",
    "text": "Analysis without group size covariate\nAs a comparison, what happens if we don’t include size as a covariate in our detection function?\n\n\n\n\n\n\n\n\n\nThe distribution of computed average groups sizes is shown above. We would expect an overestimate of mean group size because small groups at large distances are missing from our sample; but that effect is small in this instance. As a consequence, the average \\(\\hat{N}_{indiv}\\) across all simulations is 2002.71, a bias of 0.1%.",
    "crumbs": [
      "Covariates",
      "Size bias---how large is the problem?"
    ]
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-with-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis with covariate",
    "text": "Analysis with covariate\n\n\n\n\n\n\n\n\n\nWhen including size as a covariate, estimates of average group size are not affected (figure above). Likewise, mean \\(\\hat{N}_{indiv}\\) is effectively unbiased: 4440.5, a bias of -1.2%. .",
    "crumbs": [
      "Covariates",
      "Size bias---how large is the problem?"
    ]
  },
  {
    "objectID": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "href": "Pr8/Group-size-covariate.html#analysis-without-the-covariate",
    "title": "Size bias—how large is the problem?",
    "section": "Analysis without the covariate",
    "text": "Analysis without the covariate\n\n\n\n\n\n\n\n\n\nNow mean \\(\\hat{N}_{indiv}\\) is considerably biased: 5357.69, a bias of 19.3%.",
    "crumbs": [
      "Covariates",
      "Size bias---how large is the problem?"
    ]
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html",
    "href": "Pr8/alternative-selection-metrics.html",
    "title": "Other model selection metrics",
    "section": "",
    "text": "Demonstration\n\n\n\nAlternative model selection metrics",
    "crumbs": [
      "Covariates",
      "Other model selection metrics"
    ]
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#aicc",
    "href": "Pr8/alternative-selection-metrics.html#aicc",
    "title": "Other model selection metrics",
    "section": "AICc",
    "text": "AICc\nDefined as:\n\\[AICc = -2ln(\\mathscr{L}) - 2k + \\frac{2k(k+1)}{n-k-1}\\]\nwhere k is the number of parameters in the model and n is the number of observations. In general, if \\(n\\) is many times larger than \\(k^2\\), then the extra term will be negligible. What do we know, in general, about the magnitude of k and n in typical distance sampling situations? We encourage the collection of 60 to 80 detections (\\(n\\)). A hazard rate key function with a four-level factor covariate has \\(k=5\\) parameters. The value of the the \\(c\\) term added to the regular AIC ranges from 1.11 with \\(n=60\\) to 0.81 with \\(n=80\\).\nFor a two-parameter model (hazard rate or half normal with a single continuous covariate) the value of the additional term would be 0.21 with \\(n=60\\) to 0.16 with \\(n=80\\). Recognise the magnitude of the other terms in the AIC are in the hundreds or thousands (see below).",
    "crumbs": [
      "Covariates",
      "Other model selection metrics"
    ]
  },
  {
    "objectID": "Pr8/alternative-selection-metrics.html#bic",
    "href": "Pr8/alternative-selection-metrics.html#bic",
    "title": "Other model selection metrics",
    "section": "BIC",
    "text": "BIC\nDefined as:\n\\[BIC = -2ln(\\mathscr{L}) - k \\cdot ln(n)\\]\nThe penalty term changes as a function of sample size; the larger the number of detections, the greater the penalty term. With a 2 parameter model using AIC, the penalty term would be 4 (\\(2 \\times 2\\)). For a model with the the same number of parameters and 80 detections, the penalty term would be 8.8.",
    "crumbs": [
      "Covariates",
      "Other model selection metrics"
    ]
  },
  {
    "objectID": "Pr7/strata.html",
    "href": "Pr7/strata.html",
    "title": "Stratification practical 7",
    "section": "",
    "text": "Lecture slides for 15 October 2024\n\n\n\n\n\n\n\n\nCarrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function.\n\n\n\n\n\nPhoto by Nick Fewings on Unsplash",
    "crumbs": [
      "Strata",
      "Stratification practical 7"
    ]
  },
  {
    "objectID": "Pr7/strata.html#analysis-of-stratified-surveys-1",
    "href": "Pr7/strata.html#analysis-of-stratified-surveys-1",
    "title": "Stratification practical 7",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function.",
    "crumbs": [
      "Strata",
      "Stratification practical 7"
    ]
  },
  {
    "objectID": "Pr7/Pr7-instructions.html",
    "href": "Pr7/Pr7-instructions.html",
    "title": "Analysis of stratified survey data 💻",
    "section": "",
    "text": "For this exercise, we use data from a survey of Antarctic minke whales. The study region was divided into two strata (North and South) the two strata were surveyed by different vessels at the same time. The minkes tend to be found in high densities against the ice edge, where they feed, and so the density in the southern stratum is typically higher than in the northern stratum (Figure 1). This is the primary reason for using a stratified survey design. It is also the reason for covering the southern stratum more intensely: in the southern stratum the transect length per unit area is more than 2.5 times that of the northern stratum.\n\nFigure 1. An example of the sort of survey design used and a typical minke density gradient. The irregular bottom border is the ice-edge. The ‘stepped’ black line defines the boundary between the strata; dotted lines are transects and dots are detections.\n\nObjectives\nThe objectives of this exercise are to:\n\nCreate subsets of the data\nDecide whether to fit separate detection functions or a pooled detection function\nSpecify different stratification options using the dht2 function.\n\n\n\nGetting started\nBegin by reading in the data. Distances are in kilometers and a truncation distance of 1.5km is specified and used in the following detection function fitting. Perpendicular distances, transect lengths and study area size are all measured in kilometers; hence convert_units argument to ds is 1 and has been omitted. To keep things simple, a hazard rate detection function with no adjustments is used for all detection functions.\n\nlibrary(Distance)\ndata(minke)\nhead(minke)\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\nYou will see that these data contain a column called ‘Region.Label’: this contains values ‘North’ or ‘South’.\n\n\nFull geographical stratification\nFirst, we want to fit encounter rate and detection function separately in each strata. This is easily performed by splitting the data by region and using ds on each subset. The commands below do this for the southern region (note, there are alternative ways to select a subset of data).\n\n# Create dataset for South \nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\n# Fit df to south\nminke.df.S.strat &lt;- ds(minke.S, key=\"hr\", adjustment=NULL, truncation=minke.trunc)\nsummary(minke.df.S.strat)\n\nMake a note of the AIC. Perform a similar commands to obtain estimates for the northern region. What is the total AIC?\nAlso make a note of the abundance in each region. What is the total abundance in the study region?\n\n\nFitting a pooled detection function\nWe want to compare the total AIC found previously with the AIC from fitting a detection function to all data combined. This is easy to obtain:\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\nGiven the AIC value for the detection function from the pooled data, would you fit a separate detection function in each strata or not?\n\n\nStratification options using dht2\nThe command summary(minke.df.all) will provide the abundance estimates for each region and the total and for this simple example, this is sufficient. However, if we want to consider different stratification options, then the dht2 function is useful.\nAfter fitting a detection function, the dht2 function, allows abundance estimates to be computed over some specified regions. In the command below, the pooled detection function is used to obtain estimates in each strata and over all (like the summary function previously used).\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~Region.Label, stratification=\"geographical\")\n\nThe arguments are:\n\nddf\n\nthe detection function (fitted by ds)\n\nflatfile\n\nthe data object containing all the necessary information\n\nData is referred to as being in a flatfile format if it contains information on region, transects and observations. An alternative is to use a hierarchical structure and have region, transect and observation information in separate data files with links between them to ensure that transects are mapped to the relevant region and observations to the relevant transect. We’ve not used the hierarchical structure during this workshop.\n\n\nstrat_formula=~Region.Label\n\nformula (hence the ~) giving the stratification structure\n\nstratification=\"geographical\"\n\nin this example, we specify that each strata (specified in strat_formula) represents a geographical region.\n\nconvert_units\n\ngetting units conversion correct, same purpose as the convert_units argument in ds. For the minke data all measurements are in the same units, so the argument is not needed in this case.\n\n\nMake a note the total abundance in the study region.\n\n\n\n\n\n\nFailure to respect design during analysis\n\n\n\nWhat happens if we were to ignore the regions and treat the data as though it came from one large study region? This can (dangerously) be done by changing the stratification formula, as shown below.\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~1, stratification=\"geographical\")\n\nHas this changed the abundance estimate? Of course it has; the question is why has this changed the abundance estimate; which estimate is proper?",
    "crumbs": [
      "Strata",
      "Analysis of stratified survey data 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-solution.html",
    "href": "Pr6/Pr6-solution.html",
    "title": "Distance sampling survey design solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nDistance sampling survey design\nlibrary(dssd)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE",
    "crumbs": [
      "Design",
      "Distance sampling survey design **solution** 💡"
    ]
  },
  {
    "objectID": "Pr6/Pr6-solution.html#systematic-parallel-line-design",
    "href": "Pr6/Pr6-solution.html#systematic-parallel-line-design",
    "title": "Distance sampling survey design solution 💡",
    "section": "Systematic Parallel Line Design",
    "text": "Systematic Parallel Line Design\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing would you select for this design? What is the maximum trackline length for the design you have selected? What on-effort line length are we likely to achieve?\n\nThe spacing chosen by dssd of 4937.5m to generate a line length of 200km resulted in a maximum trackline length of around 261km (each exact answer will vary due to the random generate of surveys). If we choose this design then it is possible that when we randomly generate our survey we may not be able to complete it with the effort we have available.\n\n\nWe should therefore increase the spacing between the transects and re-run the coverage simulations. A spacing of 5000m gave a maximum trackline length of around 249km (see summary table of Trackline length in the output below) so we can be fairly confident that we will be able to complete any survey which we randomly generate from this design. This spacing should allow us to achieve an on-effort line length of 199km (see Line length section of design summary below). The minimum line length we would expect to achieve is 184km and the maximum is 206km. [Note your values might differ to those below]\n\n\n\n\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\nregion.sab &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\ncover.sabay &lt;- make.coverage(region.sab, n.grid.points = 5000)\ndesign.spacing5km &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.spacing5km &lt;- run.coverage(design.spacing5km, reps = 250, quiet=TRUE)\nplot(design.spacing5km)\n\n\n\n\nCoverage grid plot for parallel design of St Andrews Bay.\n\n\n\n\n\nprint(design.spacing5km)\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced transects\nSpacing:  5000\nNumber of samplers:  NA\nLine length: NA\nDesign angle:  90\nEdge protocol:  minus\n\nStrata areas:  987500079\nRegion and effort units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        St Andrews Bay Total\nMinimum            7.0   7.0\nMean               7.9   7.9\nMedian             8.0   8.0\nMaximum            8.0   8.0\nsd                 0.3   0.3\n\n    Covered area:\n    \n        St Andrews Bay     Total\nMinimum      725931799 725931799\nMean         761646034 761646034\nMedian       767142920 767142920\nMaximum      778645897 778645897\nsd            15163214  15163214\n\n    % of region covered:\n    \n        St Andrews Bay Total\nMinimum          73.51 73.51\nMean             77.13 77.13\nMedian           77.69 77.69\nMaximum          78.85 78.85\nsd                1.54  1.54\n\n    Line length:\n    \n        St Andrews Bay     Total\nMinimum      184435.97 184435.97\nMean         197460.99 197460.99\nMedian       198832.98 198832.98\nMaximum      205738.36 205738.36\nsd             5577.34   5577.34\n\n    Trackline length:\n    \n        St Andrews Bay     Total\nMinimum      220529.62 220529.62\nMean         242826.93 242826.93\nMedian       246323.21 246323.21\nMaximum      248784.14 248784.14\nsd             7706.15   7706.15\n\n    Cyclic trackline length:\n    \n        St Andrews Bay     Total\nMinimum      251941.50 251941.50\nMean         279701.04 279701.04\nMedian       283809.44 283809.44\nMaximum      285970.54 285970.54\nsd             8884.56   8884.56\n\n    Coverage Score Summary:\n    \n        St Andrews Bay      Total\nMinimum     0.34400000 0.34400000\nMean        0.77130932 0.77130932\nMedian      0.79200000 0.79200000\nMaximum     0.85200000 0.85200000\nsd          0.08668743 0.08668743",
    "crumbs": [
      "Design",
      "Distance sampling survey design **solution** 💡"
    ]
  },
  {
    "objectID": "Pr6/Pr6-solution.html#equal-spaced-zigzag-design",
    "href": "Pr6/Pr6-solution.html#equal-spaced-zigzag-design",
    "title": "Distance sampling survey design solution 💡",
    "section": "Equal Spaced Zigzag Design",
    "text": "Equal Spaced Zigzag Design\n\n\n\n\n\n\nAnswers\n\n\n\nDoes this design meet our survey effort constraint? What is the maximum total trackline length for this design? What line length are we likely to achieve with this design? Is this higher or lower than the systematic parallel design?\n\nYou were asked to then run a coverage simulation and check if the trackline length was within our effort constraints. I found the maximum trackline length to be 242km (see Trackline length summary table in the output below) so within our constraint of 250km. I then got a mean line length of 221km and minimum and maximum line lengths of 212km and 227km, respectively (see Line length summary table in the output below). We can therefore expect to achieve just over 20km more on-effort survey line length with the zigzag design than the systematic parallel line design - 10% gain. [Note your values may differ]\n\n\n\n\ndesign.zz.4500 &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 250, quiet=TRUE)\n# Plot coverage\nplot(design.zz.4500)\n\n\n\n\nCoverage grid plot for zigzag design of St Andrews Bay.\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDo you think the coverage scores look uniform across the study region? Where are they higher/lower? Why do you think this is?\n\nYou were finally asked to look at the coverage scores across the survey region to see if this design has even coverage. There are some points with lower coverage around the survey region boundary. This is actually down to the fact we are using a minus sampling strategy. If we plotted coverage scores from a systematic parallel design we would see a similar pattern. Usually edge effects from minus sampling are minor unless we have a very long survey region boundary containing a small study area. If the fact that we are using a zigzag design was causing us issues with coverage we would expect to see higher coverage at the very top or very bottom of the survey region (as our design angle is 0). We do not see this. The survey region boundaries at the top and bottom are both quite wide and perpendicular to the design angle, in this situation zigzag designs perform well with regard to even coverage.",
    "crumbs": [
      "Design",
      "Distance sampling survey design **solution** 💡"
    ]
  },
  {
    "objectID": "Pr6/Pr6-solution.html#coverage",
    "href": "Pr6/Pr6-solution.html#coverage",
    "title": "Distance sampling survey design solution 💡",
    "section": "Coverage",
    "text": "Coverage\nOrganise the study area shape file.\n\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", \n                              package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n\nCreate the coverage grid.\n\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 5000)\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tentsmuir &lt;- generate.transects(design.tm)\n\n\nprint(survey.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  751.2295\nNumber of samplers:  26\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  769310.2\nStrata coverage: 5.45%\nStrata area:  14108643\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  218.3674\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  408405.1\nStrata coverage: 57.1%\nStrata area:  715264.9\n\n   Study Area Totals:\n   _________________\nNumber of samplers:  41\nCovered area:  1177715\nAverage coverage: 7.94%\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers? Did your survey achieve exactly the number of samplers you requested? How much does coverage differ between the two strata for this realisation?\n\nA spacing of 751m was used in the main stratum and 218m in the Morton Lochs stratum - these values are calculated based on the stratum areas and should not vary between surveys generated from the same design. You may or may not have achieved the number of transects you requested, this will depend on the random start point calculated for your particular survey. There will also be some variability in coverage, my survey achieved a coverage of 5.7% in the main strata and 64.8% in the Morton Loch strata.\n\n\n\n\ncoverage.tentsmuir &lt;- run.coverage(design.tm, reps=250, quiet=TRUE)\nprint(coverage.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum        22         11.0  36.0\nMean           25         15.0  39.9\nMedian         25         15.0  40.0\nMaximum        27         18.0  44.0\nsd              1          1.2   1.5\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 687235.83    331670.39 1070982.36\nMean    763967.75    415039.04 1179006.79\nMedian  765282.29    413348.27 1180116.41\nMaximum 820570.12    468935.12 1262069.20\nsd       29169.97     27014.44   38418.57\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.87        46.37  7.22\nMean         5.41        58.03  7.95\nMedian       5.42        57.79  7.96\nMaximum      5.82        65.56  8.51\nsd           0.21         3.78  0.26\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00400000    0.2440000 0.00400000\nMean    0.05414208    0.5766833 0.07923409\nMedian  0.05600000    0.6280000 0.05600000\nMaximum 0.09200000    0.7120000 0.71200000\nsd      0.01209221    0.1123968 0.11501271\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nView the design statistics. What is the minimum number of samplers you will achieve in each strata? Is this sufficient to complete separate analyses in each stratum?\n\nMy design statistics indicated I should achieve between 22 and 27 transects in the main stratum and between 12 and 18 in the Morton Lochs stratum. I might be a bit concerned about the possibility of only achieving 12 transects in the Morton Lochs stratum (remember I cannot just discard a survey due to the number of transects and generate another as it will affect my coverage properties) but whether this is sufficient will depend on a number of things… what are the objectives of the study? how many detections are you likely to get from each transect? etc. Information from a pilot study would be useful to help decide how many transects are required as a minimum.\n\n\n\nplot(coverage.tentsmuir, strata=1)\nplot(coverage.tentsmuir, strata=2)\n\n\n\n\n\n\nCoverage scores main stratum Tentsmuir Forest.\n\n\n\n\n\n\n\nCoverage scores Morton Lochs stratum Tentsmuir Forest.\n\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDoes it appear that you that there is even coverage within strata?\n\nThe main strata looks to have fairly uniform coverage. The values appear to have such small levels of variability that the variability that is seen will be down to stochasticity as it is seen across the entire strata. The Morton Lochs strata we can see has areas of lower coverage around the edge of the study region. This grid is a bit too coarse to allow us to properly judge how much of an issue edge effects will be in this strata. It may be wise to re-run the coverage simulation with a finer coverage grid and more repetitions too. Edge effects could potentially be problematic in such small areas.",
    "crumbs": [
      "Design",
      "Distance sampling survey design **solution** 💡"
    ]
  },
  {
    "objectID": "Pr6/leaflet-demo.html",
    "href": "Pr6/leaflet-demo.html",
    "title": "Distance sampling survey design supplement",
    "section": "",
    "text": "Supplement to survey design\n\n\n\nCombining output of dssd with leaflet\n\n\n\nSurvey design with dssd\nThis exercise demonstrated how to examine the properties of various distance sampling survey designs. The exercise showed how to write survey locations to a GPX file, then import into Google Earth, but there’s a way to make visualisations all within R. I present here visualisations of the Tentsmuir point transect survey and a line transect survey of the coast of Ireland. The leaflet R package is used to show the placement of the survey effort.\n\n\nTentsmuir survey\nDesign of the survey begins with reading the unprojected shape file (coordinates likely degrees) and converting to a shape with distances measured in meters. A design is examined with allocation of point transects disproportionately between the two strata, with the smallest stratum receiving the highest allocation of effort. The final line of code in this chunk generates the coordinates of sampling stations for a realisation of this design.\n\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 100)\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tm &lt;- generate.transects(design.tm)\n\nUse either the +/- tools or mouse wheel to zoom and move around the map. The resulting map depicts the sampling stations as markers denoted with a binoculars icon. Hovering over the marker shows the latitude/longitude of each station. The red circles centred on each station is a circle of 100m radius, indicating the truncation distance specified in the make.design argument above. Use the measurement tool (upper right) to confirm the radius of the circles is 100m.\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\n\nTentsmuir study design, measuring tool top right.\n\n\n\n\nLine transect survey design with strata\nA different base map is used with leaflet to depict this marine survey. The basemap here shows some features of ocean bathymetry.\nIn contrast with the Tentsmuir survey, this begins with a projected study area map, with units of measure already in metres. Each of the six strata are given a different design.angle so as to approximate transects roughly perpendicular to the shore. Design specification is to have 15km spacing between lines within a stratum. Final line of code in this chunk produces coordinates of transects for a single realisation of this design.\n\nireland.name &lt;- system.file(\"extdata\", \"AreaRProjStrata.shp\", package = \"dssd\")\nireland &lt;- read_sf(ireland.name)\nst_crs(ireland)\n\nCoordinate Reference System:\n  User input: Albers-9 \n  wkt:\nPROJCRS[\"Albers-9\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",35,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-9,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nregion &lt;- make.region(region.name = \"Area R Ireland coast\",\n                      units = \"m\",\n                      shape = ireland.name)\ncover &lt;- make.coverage(region, n.grid.points = 100)\ndesign.space15k &lt;- make.design(region = region,\n                               transect.type = \"line\",\n                               design = \"systematic\",\n                               spacing = 15000,\n                               design.angle = c(0, 160, 85, 90, 85, 160),\n                               edge.protocol = \"minus\",\n                               truncation = 2000,\n                               coverage.grid = cover)\nireland.trans &lt;- generate.transects(object = design.space15k)\n\nUse the measurement tool (lower left corner) to check that line spacing is indeed 15km.\n\n\n\n\nMultiple strata for Irish survey.",
    "crumbs": [
      "Design",
      "Distance sampling survey design supplement"
    ]
  },
  {
    "objectID": "Pr6/design.html",
    "href": "Pr6/design.html",
    "title": "Survey design practical 6",
    "section": "",
    "text": "Lecture slides for 14 October 2024\n\n\n\n\n\n\n\n\nFundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\n\n\n\n\n\nPhoto by ray rui on Unsplash\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with this R package, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area.",
    "crumbs": [
      "Design",
      "Survey design practical 6"
    ]
  },
  {
    "objectID": "Pr6/design.html#designing-surveys-and-determining-effort-to-achieve-objectives",
    "href": "Pr6/design.html#designing-surveys-and-determining-effort-to-achieve-objectives",
    "title": "Survey design practical 6",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.",
    "crumbs": [
      "Design",
      "Survey design practical 6"
    ]
  },
  {
    "objectID": "Pr5/Pr5-instructions.html",
    "href": "Pr5/Pr5-instructions.html",
    "title": "Point transect sampling 💻",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds.",
    "crumbs": [
      "Points",
      "Point transect sampling 💻"
    ]
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#probability-density-function",
    "href": "Pr5/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling 💻",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)",
    "crumbs": [
      "Points",
      "Point transect sampling 💻"
    ]
  },
  {
    "objectID": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "Pr5/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling 💻",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method.",
    "crumbs": [
      "Points",
      "Point transect sampling 💻"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html",
    "href": "Pr5/animal_distribution.html",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "",
    "text": "Simulation of line transect survey, I won’t show the simulation details, but here is the distribution of animals (3000) and placement of 40 transects in the study area.\nDistribution of animals and survey effort.\nFrom this survey, sample pairs of transects to visually examine the uniformity of animal distances.",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html#two-transects",
    "href": "Pr5/animal_distribution.html#two-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Two transects",
    "text": "Two transects",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html#five-transects",
    "href": "Pr5/animal_distribution.html#five-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Five transects",
    "text": "Five transects",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html#ten-transects",
    "href": "Pr5/animal_distribution.html#ten-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Ten transects",
    "text": "Ten transects",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html#fifteen-transects",
    "href": "Pr5/animal_distribution.html#fifteen-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Fifteen transects",
    "text": "Fifteen transects",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr5/animal_distribution.html#twenty-transects",
    "href": "Pr5/animal_distribution.html#twenty-transects",
    "title": "Uniform distribution of animals with respect to transects",
    "section": "Twenty transects",
    "text": "Twenty transects",
    "crumbs": [
      "Points",
      "Uniform distribution of animals with respect to transects"
    ]
  },
  {
    "objectID": "Pr4/Prac4_solution.html",
    "href": "Pr4/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimation for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n     median    mean     se    lcl     ucl   cv\nNhat 978.02 1013.82 322.53 556.63 1830.16 0.33\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x",
    "crumbs": [
      "Precision",
      "Variance estimation for systematic survey designs **solution** 💡"
    ]
  },
  {
    "objectID": "Pr4/left-truncation.html",
    "href": "Pr4/left-truncation.html",
    "title": "Supplement Analysis options for left truncation",
    "section": "",
    "text": "Situations arise where detectability on/near the transect line is obscured, either because of inability to look under a survey aircraft, or because travel along the transect is difficult. One solution to such situations is to perform left truncation upon the distances resulting from the survey. Recognise however, detections at small distances have high information content. But these are the data that are lost through left truncation; approach such an analysis with caution. Such cautions are expressed in Marques (2016).\n\n\n\n\n\nDownward view of aerial survey, with distance bands emanating from the transect. Note area directly underneath plane where detections are not possible.\n\n\n\n\nThe following are extracts from Buckland et al. (2001), with my emphasis added describe analyses with data that are left truncated.\n\n\nIn some surveys, such as aerial surveys with an inadequate view of the line, probability of detection on the line may be uncertain. In this instance, a histogram of the distance data will tend to show too few detections near the line. A simple solution is to offset the line, to a distance at which detection is believed to be certain. Any observations closer to the observer are then truncated. This is termed left-truncation because it is observations from the left end of a conventional plot of the detection function that are truncated. Another method of left-truncation is to retain the line at its original location, but to truncate data within a given distance of the line. The detection function is fitted to the remaining data, and extrapolated back to zero distance (Alldredge & Gates, 1985). An example of both approaches is given in Section 8.4.3 (below).\nLeft-truncation is sometimes used to alleviate other problems with data on or near the line. If there are either too many or too few detections close to the line, left-truncation might be considered. However, the reasons for the problem should be understood before a decision on whether to left truncate is made. If there are too few detections near the line because objects move away from the observer before they are detected, then there will be too many observations further away, and left-truncation would lead to overestimation of density. Similarly, if there are too many observations close to the line because of movement towards the observer, or because perpendicular distances have been rounded to zero, then left-truncation would cause us to underestimate abundance. On the other hand, if transects are conducted along tracks (Section 7.8.5), a surfeit or deficit of detections near the line may simply reflect a higher or lower density of animals along the tracks, in which case left-truncation may yield estimates of density that are more representative of the survey region.\n\n\n\nThis section refers to the Monte Verde duck nest data you analysed previously. However this is for nests of a different species than the data you analysed.\nDespite large sample sizes, well distributed in time and space, the ability to examine differences in detectability by vegetation type was limited by the fact that baltic rush and greasewood made up approximately 68% and 15% of the vegetation on the refuge, respectively. Initially it was hypothesized that nest detectability would decline more rapidly with distance from the centreline in the tall, but often sparse, stands of greasewood when compared to the lower, more dense areas of rush. Instead, it became clear that the histogram of grouped distance data for nests found in greasewood indicated a mode well away from the transect centreline. It was hypothesized that observers would avoid the thorny greasewood (see Fig. 8.7) by walking off line and around these shrubs. Thus, nests at the base of these shrubs tended to go undetected near the transect centreline. Nests detected at the edge of greasewood clumps would be detected with near certainty while the observer was temporarily off the centreline (and thus avoiding the greasewood). Once such a nest was found its distance to the centreline was measured and recorded. Such temporary departures from the transect centreline could explain the odd distance data for the pintail nests (Fig. 8.5). Perhaps pintail were common nesters in greasewood types and, thus, many were missed near the centreline. Indeed, 24.2% of the pintail nests were found in greasewood; surely this percentage would be still higher if nests near the centreline in greasewood were all detected. Other species nested in greasewood types less frequently: mallard 15.6%, gadwall 19.5% teal 6.9% and shoveler 2.6%. We tentatively conclude that observers were reluctant to enter the thorny greasewood type, and this resulted in nests being missed near the centreline.\n\n\n\n\n\n\n\n\n\nAn alternative explanation is that the observer measured the distance from his or her position to the nest and that pintail tended to nest at least 2 feet into the greasewood type. Then Fig. 8.5b would arise without missing any nests near the centreline; instead, the data would arise because the observer’s path would go through habitat with a low pintail nest density. In any event, the presence of obstacles such as greasewood on the line must be dealt with effectively in the field survey or the analysis of the data can be problematic. We do not always advocate that the observer plunge through such cover types; instead, extra care in searching must be taken when an easier path is temporarily followed. For example, the observer could go around clumps of such vegetation both to the left and then to the right, searching the centreline more carefully. In any event, the measurements must be taken from the transect centreline, not to the observer who may be away from the centreline.\nA definitive analysis of data such as those for the pintail nests is not possible. Approximate analyses that might be useful could be considered. First, one could fit a monotonically constrained function for \\(g(x),\\) as is shown in Fig. 8.5b for the half-normal key function with Hermite polynomial adjustments. This is likely to result in an underestimate of density if a substantial number of nests near the centreline was undetected. However, in this particular case, one knows from several other, similar species in this survey that the shape of \\(g(x)\\) has a broad shoulder, so that the procedure might be acceptable.\nSecond, one could use some arbitrary left-truncation and then estimate \\(f(0)\\) and \\(D\\) using, for example, the uniform + cosine or half-normal + Hermite model. First, one could decide on a truncation point; 3 feet might be reasonable for the pintail nest data. Here the grouped distance data less than 3 feet could be discarded, the remaining data rescaled as if the fourth interval was actually the first interval, and proceed to estimate density in the usual way (Fig. 8.8a). This is likely to be similar to the first procedure because we have reason to suspect that the detection function for pintail nests is fairly flat. Still, in this case, some underestimation might be expected (unless \\(g(0) \\simeq 1.0\\) but nests close to zero tended to be recorded at around 3 feet; then overestimation might result).\nThird, the left-truncation procedure of Alldredge and Gates (1985) could be employed, using the same truncation point. The result of this procedure is very dependent upon the model chosen and is often imprecise (Fig. 8.8b). In this example, where something is known about the distribution of distances of nests of other species of ducks, it seems likely that density of pintail nests is overestimated using this approach. Of course any left truncation decreases sample size. The results of using the three approaches for the pintail nest data are summarized in Table 8.6 for the half-normal key function and Hermite polynomial adjustments (see figure below).\nThe three estimates seem fairly reasonable for the pintail nest data, although one might prefer a density estimate near 30-32, rather than 35, unless the observer’s path around greasewood types tended to sample areas of low pintail nest density. Considerable precision is lost in efforts to alleviate this problem; this is to be expected given the uncertainty introduced.",
    "crumbs": [
      "Precision",
      "Supplement Analysis options for left truncation"
    ]
  },
  {
    "objectID": "Pr4/left-truncation.html#left-truncation-1",
    "href": "Pr4/left-truncation.html#left-truncation-1",
    "title": "Supplement Analysis options for left truncation",
    "section": "",
    "text": "In some surveys, such as aerial surveys with an inadequate view of the line, probability of detection on the line may be uncertain. In this instance, a histogram of the distance data will tend to show too few detections near the line. A simple solution is to offset the line, to a distance at which detection is believed to be certain. Any observations closer to the observer are then truncated. This is termed left-truncation because it is observations from the left end of a conventional plot of the detection function that are truncated. Another method of left-truncation is to retain the line at its original location, but to truncate data within a given distance of the line. The detection function is fitted to the remaining data, and extrapolated back to zero distance (Alldredge & Gates, 1985). An example of both approaches is given in Section 8.4.3 (below).\nLeft-truncation is sometimes used to alleviate other problems with data on or near the line. If there are either too many or too few detections close to the line, left-truncation might be considered. However, the reasons for the problem should be understood before a decision on whether to left truncate is made. If there are too few detections near the line because objects move away from the observer before they are detected, then there will be too many observations further away, and left-truncation would lead to overestimation of density. Similarly, if there are too many observations close to the line because of movement towards the observer, or because perpendicular distances have been rounded to zero, then left-truncation would cause us to underestimate abundance. On the other hand, if transects are conducted along tracks (Section 7.8.5), a surfeit or deficit of detections near the line may simply reflect a higher or lower density of animals along the tracks, in which case left-truncation may yield estimates of density that are more representative of the survey region.",
    "crumbs": [
      "Precision",
      "Supplement Analysis options for left truncation"
    ]
  },
  {
    "objectID": "Pr4/left-truncation.html#nest-detection-in-differing-habitat",
    "href": "Pr4/left-truncation.html#nest-detection-in-differing-habitat",
    "title": "Supplement Analysis options for left truncation",
    "section": "",
    "text": "This section refers to the Monte Verde duck nest data you analysed previously. However this is for nests of a different species than the data you analysed.\nDespite large sample sizes, well distributed in time and space, the ability to examine differences in detectability by vegetation type was limited by the fact that baltic rush and greasewood made up approximately 68% and 15% of the vegetation on the refuge, respectively. Initially it was hypothesized that nest detectability would decline more rapidly with distance from the centreline in the tall, but often sparse, stands of greasewood when compared to the lower, more dense areas of rush. Instead, it became clear that the histogram of grouped distance data for nests found in greasewood indicated a mode well away from the transect centreline. It was hypothesized that observers would avoid the thorny greasewood (see Fig. 8.7) by walking off line and around these shrubs. Thus, nests at the base of these shrubs tended to go undetected near the transect centreline. Nests detected at the edge of greasewood clumps would be detected with near certainty while the observer was temporarily off the centreline (and thus avoiding the greasewood). Once such a nest was found its distance to the centreline was measured and recorded. Such temporary departures from the transect centreline could explain the odd distance data for the pintail nests (Fig. 8.5). Perhaps pintail were common nesters in greasewood types and, thus, many were missed near the centreline. Indeed, 24.2% of the pintail nests were found in greasewood; surely this percentage would be still higher if nests near the centreline in greasewood were all detected. Other species nested in greasewood types less frequently: mallard 15.6%, gadwall 19.5% teal 6.9% and shoveler 2.6%. We tentatively conclude that observers were reluctant to enter the thorny greasewood type, and this resulted in nests being missed near the centreline.\n\n\n\n\n\n\n\n\n\nAn alternative explanation is that the observer measured the distance from his or her position to the nest and that pintail tended to nest at least 2 feet into the greasewood type. Then Fig. 8.5b would arise without missing any nests near the centreline; instead, the data would arise because the observer’s path would go through habitat with a low pintail nest density. In any event, the presence of obstacles such as greasewood on the line must be dealt with effectively in the field survey or the analysis of the data can be problematic. We do not always advocate that the observer plunge through such cover types; instead, extra care in searching must be taken when an easier path is temporarily followed. For example, the observer could go around clumps of such vegetation both to the left and then to the right, searching the centreline more carefully. In any event, the measurements must be taken from the transect centreline, not to the observer who may be away from the centreline.\nA definitive analysis of data such as those for the pintail nests is not possible. Approximate analyses that might be useful could be considered. First, one could fit a monotonically constrained function for \\(g(x),\\) as is shown in Fig. 8.5b for the half-normal key function with Hermite polynomial adjustments. This is likely to result in an underestimate of density if a substantial number of nests near the centreline was undetected. However, in this particular case, one knows from several other, similar species in this survey that the shape of \\(g(x)\\) has a broad shoulder, so that the procedure might be acceptable.\nSecond, one could use some arbitrary left-truncation and then estimate \\(f(0)\\) and \\(D\\) using, for example, the uniform + cosine or half-normal + Hermite model. First, one could decide on a truncation point; 3 feet might be reasonable for the pintail nest data. Here the grouped distance data less than 3 feet could be discarded, the remaining data rescaled as if the fourth interval was actually the first interval, and proceed to estimate density in the usual way (Fig. 8.8a). This is likely to be similar to the first procedure because we have reason to suspect that the detection function for pintail nests is fairly flat. Still, in this case, some underestimation might be expected (unless \\(g(0) \\simeq 1.0\\) but nests close to zero tended to be recorded at around 3 feet; then overestimation might result).\nThird, the left-truncation procedure of Alldredge and Gates (1985) could be employed, using the same truncation point. The result of this procedure is very dependent upon the model chosen and is often imprecise (Fig. 8.8b). In this example, where something is known about the distribution of distances of nests of other species of ducks, it seems likely that density of pintail nests is overestimated using this approach. Of course any left truncation decreases sample size. The results of using the three approaches for the pintail nest data are summarized in Table 8.6 for the half-normal key function and Hermite polynomial adjustments (see figure below).\nThe three estimates seem fairly reasonable for the pintail nest data, although one might prefer a density estimate near 30-32, rather than 35, unless the observer’s path around greasewood types tended to sample areas of low pintail nest density. Considerable precision is lost in efforts to alleviate this problem; this is to be expected given the uncertainty introduced.",
    "crumbs": [
      "Precision",
      "Supplement Analysis options for left truncation"
    ]
  },
  {
    "objectID": "Pr3/Pr3-instructions.html",
    "href": "Pr3/Pr3-instructions.html",
    "title": "Assessing line transect detection functions 💻",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package.",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions 💻"
    ]
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#accessing-the-data",
    "href": "Pr3/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions 💻",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect.",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions 💻"
    ]
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#truncation",
    "href": "Pr3/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions 💻",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions 💻"
    ]
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#exploring-different-models",
    "href": "Pr3/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions 💻",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos &lt;- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\).",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions 💻"
    ]
  },
  {
    "objectID": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions 💻",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins &lt;- seq(from=0, to=80, by=10)\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin &lt;- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions 💻"
    ]
  },
  {
    "objectID": "Pr3/criticism.html",
    "href": "Pr3/criticism.html",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "Lecture slides for 09 October 2024\n\n\n\n\n\n\n\n\nThis practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Quarto (.qmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.\n\n\n\n\n\nPhoto by Blake Cheek on Unsplash",
    "crumbs": [
      "Criticism",
      "Model criticism practical 3"
    ]
  },
  {
    "objectID": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "href": "Pr3/criticism.html#goodness-of-fit-and-model-selection",
    "title": "Model criticism practical 3",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the Quarto (.qmd) file found in the project.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird.",
    "crumbs": [
      "Criticism",
      "Model criticism practical 3"
    ]
  },
  {
    "objectID": "Pr2/Prac2_solution.html",
    "href": "Pr2/Prac2_solution.html",
    "title": "Line transect estimation using R solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example.",
    "crumbs": [
      "Detection functions",
      "Line transect estimation using R **solution** 💡"
    ]
  },
  {
    "objectID": "Pr2/detnfns.html",
    "href": "Pr2/detnfns.html",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "Lecture slides for 08 October 2024\n\n\n\n\n\n\n\n\nThe data are the famous “ducknest” data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\n\n\n\n\n\nPhoto by Freysteinn G. Jonsson on Unsplash\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit.",
    "crumbs": [
      "Detection functions",
      "Detection functions practical 2"
    ]
  },
  {
    "objectID": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "href": "Pr2/detnfns.html#estimating-density-of-duck-nests",
    "title": "Detection functions practical 2",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described by Anderson and Pospahala (1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.",
    "crumbs": [
      "Detection functions",
      "Detection functions practical 2"
    ]
  },
  {
    "objectID": "Pr1/Prac1_solution.html",
    "href": "Pr1/Prac1_solution.html",
    "title": "Detection function fitting for lines solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nEstimation of duck nest density by hand\n\n\nIn this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nFrequency of duck nests detected in perpendicular distance bands (metres)\n\n\nDistance.band\nFrequency\n\n\n\n\n0.0-0.3\n74\n\n\n0.3-0.6\n73\n\n\n0.6-0.9\n79\n\n\n0.9-1.2\n66\n\n\n1.2-1.5\n78\n\n\n1.5-1.8\n58\n\n\n1.8-2.1\n52\n\n\n2.1-2.4\n54\n\n\n\n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g. counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]",
    "crumbs": [
      "Sampling",
      "Detection function fitting for lines **solution** 💡"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling workshop",
    "section": "",
    "text": "Material contained herein will acquaint you with design, collection and analysis of distance sampling data.\nThe materials are intended to be experiential rather than used as a reference. Exercises are paced to bring you up to speed with the fundamentals of distance sampling; how this method of population assessment differs from other forms of population sampling. This leads to the introduction of a detection function and ways to model it.\nFurther topics of model selection, assessing precision of population estimates and collection and analysis of point transect data round out the first half of the materials. These topics are depicted by the clothesline diagram below. At this point, you will have acquired sufficient experience to analyse basic distance sampling data.\n\n\n\nThe second half of the material (second clothesline diagram) exposes you to slightly more advanced concepts: design of distance sampling surveys, including the use of stratification. Also discussed are analytical methods associated with stratified surveys. This gives way to including predictors other than distance in modelling the detection process. Finally multipliers and methods associated with indirect animal surveys are introduced.\n\n\n\nAs well as the exercises (green) and their solutions (yellow), there are numerous supplements (grey), touching upon topics or demonstrating issues related to the analysis of distance sampling data. The supplements are not fundamental to successfully employing distance sampling methods, however, the more you understand tools such as distance sampling (via these supplements), the better you will be able to employ such methods.\n\nEach set of exercises is accompanied by a discussion of distance sampling principles. PDFs of those discussion materials are linked in the table below (as well as linked from the landing page of each exercise).\n\n\n\nTopic\nDate\n\n\n\n\nFundamental principles\n07 October\n\n\nDetection functions\n08 October\n\n\nModel criticism (selection and fit assessment)\n09 October\n\n\nMeasuring precision and controlling variance\n10 October\n\n\nPoint transect analysis and detailed example\n11 October\n\n\nDesign of distance sampling surveys\n14 October\n\n\nAnalysis of stratified surveys\n15 October\n\n\nCovariates in the detection function\n16 October\n\n\nIncluding multipliers in distance sampling analysis\n17 October\n\n\nField methods and summary\n18 October"
  },
  {
    "objectID": "extras/extras.html",
    "href": "extras/extras.html",
    "title": "Development of additional skills",
    "section": "",
    "text": "The practicals you have seen thus far have been targeted on specific pedagogical tasks: fitting detection functions, assessing precision, designing surveys, etc. The exercises presented as “extras” take a broader view.\n\nEven if you never conduct a distance sampling analysis, you will have the opportunity to “consume” the results of distance sampling surveys. In that consumption, you will have to determine the credibility of published findings. Given your skills understanding how to design surveys and analyse data from those surveys, you can apply those skills in assessing the defensibility of work presented by others. This is the intent of the literature critique exercise. Examine one of the three published papers as a critical consumer and make determinations about the credibility of the findings.\nA “mystery” data set is provided to take an analysis from start to finish. With this data set, you will make the series of decisions associated with distance sampling analysis. More importantly, you should emphasize the evidence used to support the decisions made.",
    "crumbs": [
      "Extras",
      "Development of additional skills"
    ]
  },
  {
    "objectID": "announce.html",
    "href": "announce.html",
    "title": "Announcements",
    "section": "",
    "text": "More resources\n\n\nNot all concepts can be discussed in this 10-session training workshop. Where to go if you want more information?\n\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution to mystery analysis\n\n\nRemember, the most important aspect of this exercise is the workflow that you followed, the decisions you made and the support (evidence) used to support those decisions.\n\n\n\nOct 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs this analysis sound?\n\n\nHow would you respond to this query, if somebody asked your opinion?\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs this analysis sound?\n\n\nHow would you respond to this query, if somebody asked your opinion?\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild your own detection function\n\n\nSee the effect of adding adjustment terms with this interactive app.\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotated ds output\n\n\nWhat do all those numbers in the summary mean?\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork to do prior to the workshop beginning\n\n\nA pencil and paper exercise fitting a detection function to a histogram of perpendicular detection distances.\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Quarto files\n\n\nQuarto files are provided for all practicals on the Posit/cloud workspace.\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Extras",
      "Announcements"
    ]
  },
  {
    "objectID": "announce/2022-12-29-quarto/index.html",
    "href": "announce/2022-12-29-quarto/index.html",
    "title": "Working with Quarto files",
    "section": "",
    "text": "Learning materials\nThese exist in two places:\n\nthis website you are currently viewing and\nthe Posit/cloud space I created that you all share.\n\nDistance sampling analysis is best learned by performing analyses. Take the opportunity to experiment and play. With Posit/cloud you are free to make mistakes, if things go completely wrong, simply delete your copy of a practical workspace and begin again.\n\n\nPossible workflow\nI envision your workflow to have the instructions for a practical open in one browser window or tab, and be working with Posit/cloud in another window or tab.\n\n\n\n\nA Quarto file (file with .qmd extension) consists both of R code and “human” language describing the purpose of the code. These files are the “raw material” I used to produce the pages of this website.\nIn performing a practical, you can take either of two approaches, or a hybrid of both.\n\nbuild code in Posix/cloud from scratch, typing all code yourself\nuse copy/paste of code from the instructions to move code from the instructions webpage into your Posit/cloud session\n\nyou will note a small clipboard appear in the upper right corner of code blocks as you hover your mouse above; that lets you copy the content of the block to your clipboard\n\nas the workshop progresses, I expect you will make the transition from using the former to using the latter\nremember to experiment as well. Use the code I provide as a starting point; from which you can investigate alternative analyses.\n\n\nAlternative workflow\n\n\n\n\nYou can also open a .qmd file in Posit/cloud. Pressing the Render arrow in the editor should recreate the document, for example a solution to a practical. To take a more studied approach to working with the code, use the green triangle at the top right of each chunk of R code to execute each bit of code individually.\nWhatever way you interact with the code within these files, pay special attention to the output produced and experiment with additional analyses."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Once upon a time, Eric wanted to begin creating a new distance sampling website, using Quarto.\nThis is his first step in this adventure."
  },
  {
    "objectID": "announce/2022-11-30-announce1/index.html",
    "href": "announce/2022-11-30-announce1/index.html",
    "title": "Work to do prior to the workshop beginning",
    "section": "",
    "text": "This exercise is not computer-based. Therefore, you won’t find anything in the RStudio/Cloud account related to Practical 1. Instead, below find links to an instruction file and a sketch of the data to complete. Print the sketch and use it to fit a detection function to the data and complete the exercise. The HTML file contains the instructions explaining how to estimate duck nest density from your figure.."
  },
  {
    "objectID": "extras/critique.html",
    "href": "extras/critique.html",
    "title": "Literature critique of distance sampling papers",
    "section": "",
    "text": "In addition to understanding how to conduct distance sampling surveys that are defensible and robust, I want you also to detect reinforcement and threats to robust inference in published distance sampling studies. I provide you with examples of recently published distance sampling studies.\nI ask that you select one of these papers and invest 30 minutes in scanning the methods and results sections, understanding what they have done and asking yourself if there is anything you would disagree with in their approach or conclusions. Do they provide evidence to support the conclusions they present?\nI’ve found a few papers representing three taxonomic groups: terrestrial mammals, terrestrial birds and marine mammals. Select a paper of interest to you and create a list of positive and negative aspects of the way in which the design and analysis are reported. Do you feel the results are defensible in light of the methodology employed and described?\n\nBuuveibaatar, B., Strindberg, S., Kaczensky, P., Payne, J., Chimeddorj, B., Naranbaatar, G., Amarsaikhan, S., Dashnyam, B., Munkhzul, T., Purevsuren, T., Hosack, D. A., & Fuller, T. K. (2017). Mongolian Gobi supports the world’s largest populations of khulan Equus hemionus and goitered gazelles Gazella subgutturosa. Oryx 51(4):639–647. https://doi.org/10.1017/S0030605316000417\nOmifolaji, J. K., Adedoyin, S. O., Ikyaagba, E. T., Khan, T. U., Ojo, V. A., Hu, Y., Alarape, A. A., Jimoh, S. O., & Hu, H. (2024). Population abundance and density estimates of poorly documented near-threatened Calabar angwantibo (Arctocebus calabarensis) in Oban Hills region. Animals, 14(9):1374-1385 https://doi.org/10.3390/ani14091374\nÖzsandıkçı, U., & Özdemir, S. (2024). Seasonal abundance estimates of cetaceans in the southern Black Sea (Sinop), Türkiye. Marine Mammal Science, 40(2), e13092. https://doi.org/10.1111/mms.13092\nStrindberg, S., Ersts, P. J., Collins, T., Sounguet, G.-P., & Rosenbaum, H. C. (2020). Line transect estimates of humpback whale abundance and distribution on their wintering grounds in the coastal waters of Gabon. J. Cetacean Res. Manage. 153–160. https://doi.org/10.47536/jcrm.vi3.324\nSyahrullah, F. N., Maddus, U., Mustari, A. H., Gursky, S., & Indrawan, M. (2023). Distribution and abundance of Peleng Tarsier (Tarsius pelengensis) in Banggai Island group, Indonesia. Scientific Reports, 13(1), Article 1. https://doi.org/10.1038/s41598-023-30049-5\nZelelew, S. A., Bekele, A., & Archibald, G. (2020). Detection function, cluster size, density, and population size of Black Crowned Crane Balearica pavonina ceciliae in the upper Blue Nile River, Lake Tana area. Scientific African 10:e00557. https://doi.org/10.1016/j.sciaf.2020.e00557",
    "crumbs": [
      "Extras",
      "Literature critique of distance sampling papers"
    ]
  },
  {
    "objectID": "extras/mystery.html",
    "href": "extras/mystery.html",
    "title": "Your skills as a distance sampling analyst",
    "section": "",
    "text": "If you wish to perform a distance sampling analysis upon a data set, I have one for you to download. It is a bit of a scrappy data set, not very many line transects (12), not very many detections (43). As such, it might present a couple of challenges to you, but hopefully not too many. 🕵\n\nThere was one covariate recorded ✍ with each detection–whether the animal was male or female .\nUnits of measure 📏 are: perpendicular distances in meters, effort (transect lengths) in kilometers and area of study region in square kilometers.\n\nDon’t forget to set the units properly\n\nlibrary(Distance)\nconversion &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\nRemember some of the hints I have provided (in decision sequence above) along with decisions you will need to make regarding truncation.\nDo a competent job with your analysis and we will share our findings (preferred model, point estimate of abundance and precision of abundance estimate) on Thursday during the practical session.\nThe data set (in CSV format) is available in the file space of this project, shown in the File panel, named mystery.csv.\nThis will be your first adventure looking at data that is not contained within the Distance package. Therefore, your first analysis task will be to read the data from the .csv file into R for subsequent analysis. Example code below:\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/erex/Oct-Quarto/main/extras/mystery.csv\")\n\nWhat happens after this is up to you. 🥴 🤔\n\n\n\n\nworkflow",
    "crumbs": [
      "Extras",
      "Your skills as a distance sampling analyst"
    ]
  },
  {
    "objectID": "Pr1/Pr1-instructions.html",
    "href": "Pr1/Pr1-instructions.html",
    "title": "Detection function fitting for lines 💻",
    "section": "",
    "text": "In this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nFrequency of duck nests detected in perpendicular distance bands (metres)\n\n\nDistance.band\nFrequency\n\n\n\n\n0.0-0.3\n74\n\n\n0.3-0.6\n73\n\n\n0.6-0.9\n79\n\n\n0.9-1.2\n66\n\n\n1.2-1.5\n78\n\n\n1.5-1.8\n58\n\n\n1.8-2.1\n52\n\n\n2.1-2.4\n54\n\n\n\n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e. the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF found at this link, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e. the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\] \\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]",
    "crumbs": [
      "Sampling",
      "Detection function fitting for lines 💻"
    ]
  },
  {
    "objectID": "Pr1/sampling.html",
    "href": "Pr1/sampling.html",
    "title": "Sampling practical 1",
    "section": "",
    "text": "Lecture slides for 07 October 2024\n\n\n\n\n\n\n\n\nThis exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\n\n\n\n\n\nPhoto by Shelby Cohron on Unsplash\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region.",
    "crumbs": [
      "Sampling",
      "Sampling practical 1"
    ]
  },
  {
    "objectID": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "href": "Pr1/sampling.html#estimating-widehatp_a-with-a-pencil",
    "title": "Sampling practical 1",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.",
    "crumbs": [
      "Sampling",
      "Sampling practical 1"
    ]
  },
  {
    "objectID": "Pr2/Pr2-instructions.html",
    "href": "Pr2/Pr2-instructions.html",
    "title": "Line transect estimation using R 💻",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org",
    "crumbs": [
      "Detection functions",
      "Line transect estimation using R 💻"
    ]
  },
  {
    "objectID": "Pr2/truncation-decisions.html",
    "href": "Pr2/truncation-decisions.html",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let’s see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}",
    "crumbs": [
      "Detection functions",
      "Effect of truncation upon density estimates"
    ]
  },
  {
    "objectID": "Pr2/truncation-decisions.html#the-function",
    "href": "Pr2/truncation-decisions.html#the-function",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "Is presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}",
    "crumbs": [
      "Detection functions",
      "Effect of truncation upon density estimates"
    ]
  },
  {
    "objectID": "Pr2/truncation-decisions.html#duck-nest-result",
    "href": "Pr2/truncation-decisions.html#duck-nest-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange &lt;- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data.",
    "crumbs": [
      "Detection functions",
      "Effect of truncation upon density estimates"
    ]
  },
  {
    "objectID": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "href": "Pr2/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc &lt;- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data.",
    "crumbs": [
      "Detection functions",
      "Effect of truncation upon density estimates"
    ]
  },
  {
    "objectID": "Pr3/modelsel-demo.html",
    "href": "Pr3/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003).",
    "crumbs": [
      "Criticism",
      "Demonstration two stages of model selection"
    ]
  },
  {
    "objectID": "Pr3/modelsel-demo.html#half-normal-cosine",
    "href": "Pr3/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos &lt;- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2805.973\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   D --&gt; ID2\n   ID0(hn0&lt;br&gt;AIC=2817)\n   ID1(hn1&lt;br&gt;AIC=2806)\n   ID2(hn2&lt;br&gt;AIC=2808)\n   FIN(hn1)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN",
    "crumbs": [
      "Criticism",
      "Demonstration two stages of model selection"
    ]
  },
  {
    "objectID": "Pr3/modelsel-demo.html#uniform-cosine",
    "href": "Pr3/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos &lt;- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --&gt; ID1\n   D --&gt; ID2\n   D --&gt; ID3\n   D --&gt; ID4\n   ID1(unif1&lt;br&gt;AIC=2811)\n   ID2(unif2&lt;br&gt;AIC=2808)\n   ID3(unif3&lt;br&gt;AIC=2807)\n   ID4(unif4&lt;br&gt;AIC=2808)\n   FIN(unif3)\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN\n   ID3 --&gt; FIN\n   ID4 --&gt; FIN",
    "crumbs": [
      "Criticism",
      "Demonstration two stages of model selection"
    ]
  },
  {
    "objectID": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "href": "Pr3/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos &lt;- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.469\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   ID0(hr0&lt;br&gt;AIC=2805)\n   ID1(hr1&lt;br&gt;AIC=2807)\n   FIN(hr0)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms",
    "crumbs": [
      "Criticism",
      "Demonstration two stages of model selection"
    ]
  },
  {
    "objectID": "Pr3/Prac3_solution.html",
    "href": "Pr3/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions **solution** 💡"
    ]
  },
  {
    "objectID": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "Pr3/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution 💡",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos &lt;- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos &lt;- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and QQ plots are shown below:\nplot(caper.hn.cos, main=\"Half normal\")\nx &lt;- gof_ds(caper.hn.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.hr.cos, main=\"Hazard rate\")\nx &lt;- gof_ds(caper.hr.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.uf.cos, main=\"Uniform\")\nx &lt;- gof_ds(caper.uf.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nQQ plot half normal\n\n\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\n\nQQ plot hazard rate\n\n\n\n\n\n\n\n\n\nUniform with adjustment\n\n\n\n\n\n\n\nQQ plot uniform adj\n\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.614\n0.682\n0.098\n0.279\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.153\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results.",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions **solution** 💡"
    ]
  },
  {
    "objectID": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution 💡",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‘correct’ perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, …, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins &lt;- c(0, seq(from=7.5, to=67.5, by=10), 80)\nprint(bins)\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\ncaper.hn.bin &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n\n\n\n\n# See a portion of the results\nknitr::kable(caper.hn.bin$dht$individuals$summary, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\nMonaughty Forest\n1472\n3840\n240\n112\n1\n0.4666667\n0\n0\n1\n0\n\n\n\n\nknitr::kable(caper.hn.bin$dht$individuals$D[1:6], row.names = FALSE, digits=3)\n\n\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\n\n\n\n\nTotal\n0.045\n0.007\n0.152\n0.026\n0.079\n\n\n\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data.",
    "crumbs": [
      "Criticism",
      "Assessing line transect detection functions **solution** 💡"
    ]
  },
  {
    "objectID": "Pr4/Pr4-instructions.html",
    "href": "Pr4/Pr4-instructions.html",
    "title": "Variance estimation for systematic surveys 💻",
    "section": "",
    "text": "In the lecture describing measures of precision, we explained that systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise, we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design. This means that the usual variance estimators used in the ds function, which are based on random transect placement, are far too high. The true variance is low, but the estimated variance is high.\nWe will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g. the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strong trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon’t forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nsysvar2.hn$dht$individuals$D\nsysvar2.hn$dht$individuals$N\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e. 1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‘O2’ (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e. transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibit a trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‘O2’ estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nsysvar1.hn$dht$individuals$D\nsysvar1.hn$dht$individuals$N\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x",
    "crumbs": [
      "Precision",
      "Variance estimation for systematic surveys 💻"
    ]
  },
  {
    "objectID": "Pr4/precision.html",
    "href": "Pr4/precision.html",
    "title": "Precision practical 4",
    "section": "",
    "text": "Lecture slides for 10 October 2024\n\n\n\n\n\n\n\n\nThis exercise examines the ability to precisely estimate density from line transect data.\n\n\n\n\n\nPhoto by Andrea Sonda on Unsplash\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set.",
    "crumbs": [
      "Precision",
      "Precision practical 4"
    ]
  },
  {
    "objectID": "Pr4/precision.html#precision-of-density-estimates",
    "href": "Pr4/precision.html#precision-of-density-estimates",
    "title": "Precision practical 4",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.",
    "crumbs": [
      "Precision",
      "Precision practical 4"
    ]
  },
  {
    "objectID": "Pr5/points.html",
    "href": "Pr5/points.html",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Lecture slides for 11 October 2024\n\n\n\n\n\n\n\n\nHaving mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\n\n\n\n\n\nPhoto by Vincent van Zalinge on Unsplash\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method.",
    "crumbs": [
      "Points",
      "Point transects practical 5"
    ]
  },
  {
    "objectID": "Pr5/points.html#analysis-of-point-transect-data",
    "href": "Pr5/points.html#analysis-of-point-transect-data",
    "title": "Point transects practical 5",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.",
    "crumbs": [
      "Points",
      "Point transects practical 5"
    ]
  },
  {
    "objectID": "Pr5/Prac5_solution.html",
    "href": "Pr5/Prac5_solution.html",
    "title": "Point transect sampling solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise",
    "crumbs": [
      "Points",
      "Point transect sampling **solution** 💡"
    ]
  },
  {
    "objectID": "Pr5/Prac5_solution.html#truncation-of-20m",
    "href": "Pr5/Prac5_solution.html#truncation-of-20m",
    "title": "Point transect sampling solution 💡",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648",
    "crumbs": [
      "Points",
      "Point transect sampling **solution** 💡"
    ]
  },
  {
    "objectID": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "Pr5/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Point transect sampling solution 💡",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\n\n\nHalf normal without truncation\n\n\n\n\n\n\n\nHalf normal 20m truncation\n\n\n\n\n\n\n\n\n\nHazard rate 20m truncation\n\n\n\n\n\n\n\nUniform cosine 20m truncation\n\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates.",
    "crumbs": [
      "Points",
      "Point transect sampling **solution** 💡"
    ]
  },
  {
    "objectID": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "Pr5/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Point transect sampling solution 💡",
    "section": "Probability density functions for Buckland’s winter wren point transects",
    "text": "Probability density functions for Buckland’s winter wren point transects\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\n\n\nDetection function 5 minute count\n\n\n\n\n\n\n\nDetection function snapshot\n\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data.",
    "crumbs": [
      "Points",
      "Point transect sampling **solution** 💡"
    ]
  },
  {
    "objectID": "Pr6/effort-and-power.html",
    "href": "Pr6/effort-and-power.html",
    "title": "Effort and power calculations for line transect surveys",
    "section": "",
    "text": "Demonstration\n\n\n\nEffort needed to achieve objective",
    "crumbs": [
      "Design",
      "Effort and power calculations for line transect surveys"
    ]
  },
  {
    "objectID": "Pr6/effort-and-power.html#cv-graph",
    "href": "Pr6/effort-and-power.html#cv-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "CV graph",
    "text": "CV graph\nThe result tab showing the relationship between cumulative population change and necessary CV is shown in the CV graph tab. There is a positive relationship between these: greater population loss requires less precision to detect; or small population change requires more precise density estimates. The red dot and horizontal dotted line indicate the precision of annual estimates to detect the specified cumulative change over the specified number of annual surveys with the desired power. For these calculations, the encounter rate from the pilot survey plays no role.",
    "crumbs": [
      "Design",
      "Effort and power calculations for line transect surveys"
    ]
  },
  {
    "objectID": "Pr6/effort-and-power.html#effort-graph",
    "href": "Pr6/effort-and-power.html#effort-graph",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Effort graph",
    "text": "Effort graph\nThe result tab labelled Effort graph brings information from the pilot survey into the calculations. The CV graph indicates the necessary precision to achieve the desired results, this CV is fed into the formula from Buckland et al. (2015) to estimate the amount of survey effort to be expended annually to achieve the precision derived from the power calculations.\nThe red ball and horizontal dotted line now indicates the amount of effort needed to achieve the specified objectives. Small changes in abundance require exponentially larger amounts of annual survey effort to detect. The steepness of that exponential curve is less extreme when encounter rates are large.",
    "crumbs": [
      "Design",
      "Effort and power calculations for line transect surveys"
    ]
  },
  {
    "objectID": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "href": "Pr6/effort-and-power.html#numerical-result-of-power-calculation",
    "title": "Effort and power calculations for line transect surveys",
    "section": "Numerical result of power calculation",
    "text": "Numerical result of power calculation\nThe final result tab simply provides a single numerical solution to the required CV and effort necessary to achieve that CV for the specified combination of cumulative change, number of annual surveys, power and pilot study encounter rate.",
    "crumbs": [
      "Design",
      "Effort and power calculations for line transect surveys"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html",
    "href": "Pr6/Pr6-instructions.html",
    "title": "Distance sampling survey design 💻",
    "section": "",
    "text": "We provide two exercises in survey design so you can choose the one you feel is most useful to you.\n\nThe first example involves designing a line transect survey to estimate the abundance of porpoise, common dolphins and seals in and around St Andrews Bay.\n\nIt considers how you choose your design based on effort limitations.\nIt also compares an aerial survey based on systematic parallel lines with a boat based survey using zigzags.\n\nThe second example involves designing a point transect bird survey in Tentsmuir Forest.\n\nThis looks at how to project your study area from latitude and longitude on to a flat plane using R.\nIt also involves defining a design for multiple strata with different coverage in each strata.",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#study-region",
    "href": "Pr6/Pr6-instructions.html#study-region",
    "title": "Distance sampling survey design 💻",
    "section": "Study Region",
    "text": "Study Region\nSet up the study region and plot it. The shapefile for this study area is contained within the dssd R library. This shapefile has already been projected from latitude and longitude on to a flat plane and its units are in metres. The first line of code below returns the path for the shapefile within the R library and may vary on different computers. You will then pass this shapefile pathway to the make.region function to set up the survey region. As this shapefile does not have a projection (.prj) file associated with it we should tell dssd the units (m) when we create the survey region.\n\n#Find the pathway to the file\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\n#Create the region using this shapefile\nregion &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\n#Plot the region\nplot(region)\n\n\n\n\nStudy Region",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage",
    "href": "Pr6/Pr6-instructions.html#coverage",
    "title": "Distance sampling survey design 💻",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover &lt;- make.coverage(region, n.grid.points = 500)",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "href": "Pr6/Pr6-instructions.html#systematic-parallel-design",
    "title": "Distance sampling survey design 💻",
    "section": "Systematic Parallel Design",
    "text": "Systematic Parallel Design\nThe small survey plane available can complete a total flight time of around 250km (excluding the flight time to and from the landing strip at Fife Ness). Generally, systematic parallel line designs are preferable for aerial surveys as they allow some rest time for observers as the plane travels between transects and avoids the sharp turns associated with zigzag designs.\nFirstly, we will consider the design angle. Often animal density is affected by distance to coast so it is probably wise for this survey to orientate lines approximately perpendicular to the coast. To do this we can select a design angle of 90 degrees. We can therefore expect to spend a little more than 40 km (the height of the survey region) on off-effort transit time and might hope to be able to complete around 200km of transects. dssd lets us specify the desired line length as a design parameter and will then choose an appropriate value for transect spacing. We will choose a minus sampling strategy and set the truncation distance to 2km. Note that as our survey region coordinates are in metres we also need to supply the design parameters in metres.\n\n# Define the design\ndesign.LL200 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      line.length = 200000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nNow we have defined the design we should check it visually by creating a survey (a single set of transects).\n\n# Create a single survey from the design\nsurvey.LL200 &lt;- generate.transects(design.LL200)\n# Plot the region and the survey\nplot(region, survey.LL200)\n\n\n\n\nSingle survey generated from the systematic parallel line design\n\n\n\n\nThe survey consists of parallel systematically spaced transects running horizontally across the survey region roughly perpendicular to the coast as we wanted. We can also view the details of the survey which will tell us what spacing dssd used to try and achieve a line length of 200 km.\n\n# Display the survey details\nsurvey.LL200\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced parallel transects\nSpacing:  4937.5\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nDesign angle:  90\nEdge protocol:  minus\nCovered area:  779476845\nStrata coverage: 78.93%\nStrata area:  987500079\n\n   Study Area Totals:\n   _________________\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nCovered area:  779476845\nAverage coverage: 78.93%\n\n\nThe spacing used by dssd was 4938m which gives us 8 samplers and a coverage of just under 80%. In addition, this example survey has a line length of just under 200km and a trackline length of just over 248 km. However, given the random nature of the design and the fact that the width of the study region is not constant everybody should get slightly different values. Although the trackline length was just under 250km it is not sufficient to only look at one survey, we need to know that all surveys under this design will have a trackline length of &lt; 250km.\nTo assess the design statistics across many surveys we will now run a coverage simulation. This simulation will randomly generate many surveys from our design and record coverage as well as various statistics including line length and trackline length. As we know that coverage for a parallel line design is largely uniform (apart from edge effects due to minus sampling) we do not need to run too many repetitions, 100 should be sufficient to give us an indication of the range of line lengths and trackline lengths for this design.\n\n# Run the coverage simulation\ndesign.LL200 &lt;- run.coverage(design.LL200, reps = 100)\ndesign.LL200\n\nExamine the design statistics. The mean line length should be around 200km (200,000 m). Now look at the maximum trackline length, we need this value to be less than 250km (250,000 m).\nUse the results of this simulation to create some new designs based on various spacings to find the maximum line length that can be achieved without risking exceeding the maximum trackline length of 250km (remember to generate a line length of 200km dssd selected a spacing of 4938m). Try transect spacings of 5000m or 5500m.\n\n# Define the design\ndesign.space500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing would you select for this design?\nWhat is the maximum trackline length for the design you have selected?\nWhat on-effort line length are we likely to achieve?",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#zigzag-design",
    "href": "Pr6/Pr6-instructions.html#zigzag-design",
    "title": "Distance sampling survey design 💻",
    "section": "Zigzag Design",
    "text": "Zigzag Design\nZigzag designs are often more efficient in their use of effort having less off-effort transit time between transects. For this survey another option would be to complete a boat-based survey. The boat survey will have the same total effort available allowing us a trackline length of 250km.\nDefine a zigzag design for the same region. For zigzag designs the design angle has a different definition, it describes the angle across which the zigzags are constructed. Set the vertical design angle to 0. Zigzag designs also require an additional argument as zigzags can only be created inside convex shapes. Specify the bounding shape, choose a convex hull as it is more efficient than a minimum bounding rectangle. A convex hull works as if we were stretching an elastic band around the survey region. The code below shows you how to create the zigzag design, you should then create a single realisation of this design and plot it to check it looks acceptable.\n\n# Define the zigzag design\ndesign.zz.4500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nsurvey.zz &lt;- generate.transects(design.zz.4500)\nplot(region, survey.zz)\n\n\n\n\nSingle survey generated from the equal spaced zigzag design\n\n\n\n\nRun a coverage simulation to verify that we have stayed within the restraints of our survey effort; a total trackline length of &lt; 250km. Run the coverage simulation with more repetitions to also assess the coverage.\nExamine the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes this design meet our survey effort constraint?\nWhat is the maximum total trackline length for this design?\nWhat line length are we likely to achieve with this design?\nIs this higher or lower than the systematic parallel design?\n\n\n\n\n# Run the coverage simulation\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 500)\n# Display the design statistics\ndesign.zz.4500\n\nExamine the coverage. Sometimes with zigzag surveys generated inside convex hulls produce areas of higher coverage in narrower parts of the survey region at either end of the design axis. One of the easiest ways to assess coverage is visually by plotting the coverage grid.\n\n# Plot the coverage grid\nplot(design.zz.4500)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDo you think the coverage scores look uniform across the study region?\nGiven the lack of uniformity, where are they higher/lower?\nWhy do you think this is?\n\nNote, revisit one of the parallel line designs and plot the coverage scores to compare (although there are fewer repetitions you can still get an idea of coverage).",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "href": "Pr6/Pr6-instructions.html#projecting-your-study-region",
    "title": "Distance sampling survey design 💻",
    "section": "Projecting your Study Region",
    "text": "Projecting your Study Region\nThis exercise demonstrates how to deal with unprojected shapefiles. Study areas should always be projected onto a flat plane before you use them to design your survey. This is because in most parts of the world one degree latitude is not the same in distance as one degree longitude. If we didn’t project, our study region and any surveys generated in it, would be distorted possibly leading to non-uniform coverage.\nLoad the study region and project it onto a flat plane using an Albers Equal Area Conical projection. As we have to project the shapefile we load the shape object separately instead of directly into a region object.\n\n#Load the unprojected shapefile\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\n# Check current coordinate reference system\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n# Define a European Albers Equal Area projection\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\n# Project the study area on to a flat plane\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\n\nCreate the region object for dssd using the projected shape and plot it to check what it looks like.\n\n# Create the survey region in dssd\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n# Plot the survey region\nplot(region.tm)\n\n\n\n\nTentsmuir Forest: showing the main stratum and the Morton Loch stratum.",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#coverage-1",
    "href": "Pr6/Pr6-instructions.html#coverage-1",
    "title": "Distance sampling survey design 💻",
    "section": "Coverage",
    "text": "Coverage\nSet up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 1000)",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#design",
    "href": "Pr6/Pr6-instructions.html#design",
    "title": "Distance sampling survey design 💻",
    "section": "Design",
    "text": "Design\nSet up a systematic point transect design. We will assume that we have sufficient resources to survey 40 point transects. As the Morton Lochs stratum is of special interest we will give it higher coverage. We will therefore explicitly allocate 25 samplers to the main stratum and 15 to the Morton Lochs stratum (note that the area of the Morton Lochs stratum is much small than the main stratum). If we wanted to allocate the same effort to both stratum we could provide the samplers argument with the single value of 40 and it would divide the effort equally between the strata. We will leave the design angle as 0 and set the truncation distance to 100 m. We will use a minus sampling approach at the edges.\n\n\n\n\n\n\nQuestion:\n\n\n\n\nWhat are the analysis implications of a design with unequal coverage?\n\n\n\n\n# Set up a multi strata systematic point transect design\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#generate-a-survey",
    "href": "Pr6/Pr6-instructions.html#generate-a-survey",
    "title": "Distance sampling survey design 💻",
    "section": "Generate a Survey",
    "text": "Generate a Survey\nYou will now generate a single survey from this design and plot it inside the survey region to check what it looks like. If you want to check whether the covered areas of the samplers in the Morton Lochs stratum overlap add the argument covered.area = TRUE to the plot function.\n\n# Create a single survey from the design\nsurvey.tm &lt;- generate.transects(design.tm)\n# Plot the region and the survey\nplot(region.tm, survey.tm, covered.area = TRUE)\n\n\n\n\nSingle survey generated from the systematic parallel line design\n\n\n\n\nExamine the survey information.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers?\nDid your survey achieve exactly the number of samplers you requested?\nHow much does coverage differ between the two strata for this realisation?\n\n\n\n\n# Display survey information\nsurvey.tm\n\n\nSave coordinates to a file\nIf this survey is to be conducted in the field, you will want the coordinates that you can load into a handheld GPS. The function write.transects() can write waypoints of the survey (in this case the point transect stations) to text, comma-separated value or GPX files.\n\nwrite.transects(survey.tm,\n                dsn = \"tentsmuir-points.gpx\",\n                layer = \"points\",\n                dataset.options = \"GPX_USE_EXTENSIONS=yes\",\n                proj4string=sf::st_crs(sf.shape))\n\nThe GPX file can be transferred to a GPS, or viewed using Google Earth.\n\n\n\n\n\nRealised survey with locations written to GPX file and imported into Google Earth.",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "href": "Pr6/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "title": "Distance sampling survey design 💻",
    "section": "Assessing Coverage and Design Statistics",
    "text": "Assessing Coverage and Design Statistics\nWe will now run a coverage simulation to assess how much the number of samplers and average coverage varies between surveys. We will also be able to assess how coverage varies spatially to see if edge effects are of concern.\nView the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is the minimum number of samplers you will achieve in each strata?\nIs this sufficient to complete separate analyses in each stratum?\n\n\n\nPlot the coverage scores.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes it appear that there is even coverage within each strata?\n\nAs there is such a difference in the range of coverage scores between strata you may need to plot each strata individually.\n\n\n\n\n\n# View the design statistics\nprint(design.tm)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  100\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         13.0  36.0\nMean         25.1         15.1  40.2\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            0.9          1.1   1.3\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 683662.63    363258.99 1076099.58\nMean    768186.34    420283.45 1188469.79\nMedian  772479.34    417730.22 1188447.46\nMaximum 816669.52    468461.70 1264365.86\nsd       25071.81     25394.87   32875.01\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.85        50.79  7.26\nMean         5.44        58.76  8.02\nMedian       5.48        58.40  8.02\nMaximum      5.79        65.49  8.53\nsd           0.18         3.55  0.22\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00000000    0.2200000 0.00000000\nMean    0.05483087    0.5750000 0.08094378\nMedian  0.05000000    0.6450000 0.06000000\nMaximum 0.15000000    0.7800000 0.78000000\nsd      0.02997747    0.1456197 0.12170445\n\n# Plot the coverage scores\nplot(design.tm)\n\n\n\n\n\n\n\n# coverage scores for individual strata could be plotted separately\n# plot(design.tm, strata.id = 1)\n# plot(design.tm, strata.id = 2)",
    "crumbs": [
      "Design",
      "Distance sampling survey design 💻"
    ]
  },
  {
    "objectID": "Pr7/detecting-differences.html",
    "href": "Pr7/detecting-differences.html",
    "title": "Detecting differences in density estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nComparing two density estimates",
    "crumbs": [
      "Strata",
      "Detecting differences in density estimates"
    ]
  },
  {
    "objectID": "Pr7/detecting-differences.html#aside",
    "href": "Pr7/detecting-differences.html#aside",
    "title": "Detecting differences in density estimates",
    "section": "Aside",
    "text": "Aside\nWhen I use the term stratum, I could be referring to two geographic strata in your study area. More broadly, strata in an analysis could be two different years of surveys of the same study area, or two species within a study area in a single survey. Depending upon how strata are defined, inferences can be made about differences across space, across time, or between species.",
    "crumbs": [
      "Strata",
      "Detecting differences in density estimates"
    ]
  },
  {
    "objectID": "Pr7/detecting-differences.html#output-interpretation",
    "href": "Pr7/detecting-differences.html#output-interpretation",
    "title": "Detecting differences in density estimates",
    "section": "Output interpretation",
    "text": "Output interpretation\nThe first two lines of output echo much of the input information: number of detections, encounter rate CV, line length, number of transects, average group size, CV of average groups size, number of parameters in the detection function. Also echoed are the stratum-specific density estimates and their CV along with the estimate of the \\(f(0)\\) parameter of the detection function and its CV.\nThe last line of output shows the difference of estimated densities and its standard error. Following this is the test statistic which is distributed as a t-statistic (Equation 2) and its associated degrees of freedom (Equation 3) and an associated significance level for a two-tailed test. The final pair of values are the bounds of the confidence interval on the estimated difference computed from Equation 5.\nNote: When a pooled detection function is used, the difference is relatively small (~0.05) and non-significant. When separate detection functions are used, the magnitude of the estimated difference increases (~0.08), with a standard error of roughly the same magnitude. Quite a few degrees of freedom are lost by having to estimate two additional parameters from separate detection functions. The total width of the confidence interval changes little (0.1662 for separate detection functions versus 0.1614 for pooled detection function). However because of the shift in the estimated difference from 0.0484 to 0.0802, the significance level shifts from 0.234 to 0.058.\nLesson for survey design: The uncertainty in all of the density estimates come from encounter rate variability. If the purpose of the study was to demonstrate a difference in density between these strata, a better survey design, with more than 13 and 12 transects would have been required to produce better estimates of stratum-specific encounter rate uncertainty.\n\n\n\n\n\n\nFootnote\n\n\n\nWhat if I used stratum as a covariate in the detection function?\nAt the time Buckland et al. (2001) was published, the use of covariates in the detection function was in early stages of development; hence that case was not considered when testing for differences between density estimates were described. Consequently, the appendix below does not address this situation.\nBut code exists for addressing assessing differences in density between strata when stratum is a covariate in the detection function. The code along with two examples of its use, are presented in our case study website.",
    "crumbs": [
      "Strata",
      "Detecting differences in density estimates"
    ]
  },
  {
    "objectID": "Pr7/detecting-differences.html#this-is-section-3.6.5-of-buckland2001",
    "href": "Pr7/detecting-differences.html#this-is-section-3.6.5-of-buckland2001",
    "title": "Detecting differences in density estimates",
    "section": "This is Section 3.6.5 of Buckland et al. (2001)",
    "text": "This is Section 3.6.5 of Buckland et al. (2001)\nFrequently, we wish to draw inference on change in density over time, or difference in density between habitats. We consider here simple comparisons between two density estimates.\nConsider two density estimates, \\(\\hat{D}_{1}\\) and \\(\\hat{D}_{2}.\\) Suppose first that they are independently estimated. We then estimate the difference in density by \\(\\hat{D}_{1}-\\hat{D}_{2}\\) with variance \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)=\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}\\right)+\\widehat{\\operatorname{var}}\\left(\\hat{D}_{2}\\right)\n\\tag{1}\\] Distance provides approximate degrees of freedom \\(\\mathrm{df}_{1}\\) for \\(\\hat{D}_{1}\\) and \\(\\mathrm{df}_{2}\\) for \\(\\hat{D}_{2},\\) based on Satterthwaite’s approximation. We can use these to obtain an approximate \\(t\\)-statistic:\n\\[\nT=\\frac{\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)-\\left(D_{1}-D_{2}\\right)}{\\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}} \\sim t_{\\mathrm{df}}\n\\tag{2}\\]\nwhere \\[\n\\mathrm{d} \\mathrm{f} \\simeq \\frac{\\left\\{\\widehat{v a r}\\left(\\hat{D}_{1}\\right)+\\widehat{v a r}\\left(\\hat{D}_{2}\\right)\\right\\}^{2}}{\\left\\{\\widehat{v a r}\\left(\\hat{D}_{1}\\right)\\right\\}^{2} / \\mathrm{df}_{1}+\\left\\{\\widehat{v a r}\\left(\\hat{D}_{2}\\right)\\right\\}^{2} / \\mathrm{df}_{2}}\n\\tag{3}\\]\nProvided df are around 30 or more, the simpler \\(z\\)-statistic provides a good approximation: \\[\nZ=\\frac{\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)-\\left(D_{1}-D_{2}\\right)}{\\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}} \\sim N(0,1)\n\\tag{4}\\] For either statistic, we can test the null hypothesis \\(H_{0}: D_{1}=D_{2}\\) by substituting \\(D_{1}-D_{2}=0\\) in Equation 2 or Equation 4, and looking at the resulting value in \\(t\\)-tables or \\(z\\)-tables. Approximate \\(100(1-2 \\alpha) \\%\\) confidence limits for \\(\\left(D_{1}-D_{2}\\right)\\) are given by \\[\n\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) \\pm t_{\\mathrm{df}}(\\alpha) \\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}\n\\tag{5}\\]\nfor df \\(&lt;30,\\) or \\[\n\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) \\pm z(\\alpha) \\sqrt{\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)}  \n\\tag{6}\\] otherwise. Often, a single detection function is fitted to pooled data, so that \\[\n\\hat{D}_{1}=\\frac{n_{1} \\hat{f}(0) \\hat{E}_{1}(s)}{2 L_{1}} \\quad \\text { and } \\quad \\hat{D}_{2}=\\frac{n_{2} \\hat{f}(0) \\hat{E}_{2}(s)}{2 L_{2}}\n\\tag{7}\\] (where the terms \\(\\hat{E}_{i}(s)\\) are omitted if the objects are not in clusters). Because \\(\\hat{f}(0)\\) appears in both equations, we can no longer assume that \\(\\hat{D}_{1}\\) and \\(\\hat{D}_{2}\\) are independent. Instead, we can write \\[\n\\hat{D}_{i}=\\hat{M}_{i} \\hat{f}(0), \\quad i=1,2\n\\tag{8}\\] where \\[\n\\hat{M}_{i}=\\frac{n_{i} \\hat{E}_{i}(s)}{2 L_{i}}\n\\tag{9}\\] As the \\(M_{i}\\) are independently estimated, and are assumed to be independent of \\(\\hat{f}(0),\\) we can now find the variance of \\(\\hat{D}_{1}-\\hat{D}_{2}\\) using the delta method: \\[\n\\hat{D}_{1}-\\hat{D}_{2}=\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right) \\hat{f}(0)\n\\tag{10}\\] so that \\[\n\\begin{aligned}\n\\widehat{\\operatorname{var}}\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right) &=\\left(\\hat{D}_{1}-\\hat{D}_{2}\\right)^{2}\\left[\\frac{\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)}{\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)^{2}}+\\frac{\\widehat{\\operatorname{var}}\\{\\hat{f}(0)\\}}{\\{\\hat{f}(0)\\}^{2}}\\right] \\\\\n&=\\{\\hat{f}(0)\\}^{2} \\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)+\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)^{2} \\widehat{var}\\{\\hat{f}(0)\\}\n\\end{aligned}\n\\tag{11}\\] where \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)=\\widehat{\\operatorname{var}}\\left(\\hat{M}_{1}\\right)+\\widehat{\\operatorname{var}}\\left(\\hat{M}_{2}\\right)\n\\tag{12}\\] and \\[\n\\widehat{\\operatorname{var}}\\left(\\hat{M}_{i}\\right)=\\hat{M}_{i}^{2}\\left[\\frac{\\widehat{\\operatorname{var}}\\left(n_{i}\\right)}{n_{i}^{2}}+\\frac{\\widehat{\\operatorname{var}}\\left\\{\\hat{E}_{i}(s)\\right\\}}{\\left\\{\\hat{E}_{i}(s)\\right\\}^{2}}\\right], \\quad i=1,2\n\\tag{13}\\] Note that the second form of Equation 11 still applies when \\(\\left(\\hat{M}_{1}-\\hat{M}_{2}\\right)=0,\\) whereas the first form leads to a ratio of zero over zero. Inference can now proceed as before, either with additional applications of Satterthwaite’s approximation in conjunction with Equation 2 if an approximate \\(t\\)-statistic is required, or more usually, by straightforward application of Equation 4.",
    "crumbs": [
      "Strata",
      "Detecting differences in density estimates"
    ]
  },
  {
    "objectID": "Pr7/Prac7_solution.html",
    "href": "Pr7/Prac7_solution.html",
    "title": "Analysis of data from stratified surveys solution 💡",
    "section": "",
    "text": "Solution\n\n\n\nAnalysis of data from stratified surveys\n\n\nReading in the data from the stratified survey in the Southern Ocean:\n\nlibrary(Distance)\nlibrary(kableExtra)\n# Load data\ndata(minke)\nhead(minke, n=3)\n\n  Region.Label  Area Sample.Label Effort distance object\n1        South 84734            1  86.75     0.10      1\n2        South 84734            1  86.75     0.22      2\n3        South 84734            1  86.75     0.16      3\n\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\n\nStrata treated distinctly\nFit detection function and encounter rate separately in each strata.\n\n## Fit to each region separately - full geographical stratification\n# Create data set for South\nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\nminke.df.S.strat &lt;- ds(minke.S, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.S.strat)\n\n\nSummary for distance analysis \nNumber of observations :  39 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  8.617404 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.5102606 0.1921723\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.242147 0.3770239\n\n                      Estimate         SE       CV\nAverage p            0.4956459  0.0662961 0.133757\nN in covered region 78.6852058 13.8143714 0.175565\n\nSummary statistics:\n  Region  Area CoveredArea Effort  n  k         ER      se.ER     cv.ER\n1  South 84734     1453.23 484.41 39 13 0.08051031 0.01809954 0.2248102\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 4587.926 1200.166 0.2615924 2687.497 7832.219 21.14052\n\nDensity:\n  Label   Estimate         se        cv        lcl        ucl       df\n1 Total 0.05414505 0.01416393 0.2615924 0.03171687 0.09243301 21.14052\n\n# Combine selection and detection function fitting for North\nminke.df.N.strat &lt;- ds(minke[minke$Region.Label==\"North\", ],\n                       truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.N.strat)\n\n\nSummary for distance analysis \nNumber of observations :  49 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  37.27825 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.0108104 0.2203526\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.022021 0.6907906\n\n                      Estimate         SE        CV\nAverage p            0.7592309 0.09987673 0.1315499\nN in covered region 64.5389972 9.62021387 0.1490605\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 9986.683 3878.031 0.3883203 4469.865 22312.49 13.98197\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 Total 0.01583725 0.006149924 0.3883203 0.007088475 0.03538397 13.98197\n\n\n\n\nDetections combined across strata\nNext we fitted a pooled detection function.\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\n\nSummary for distance analysis \nNumber of observations :  88 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  48.63688 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.2967912 0.1765812\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 0.964833 0.3605009\n\n                       Estimate         SE        CV\nAverage p             0.6224396  0.0668011 0.1073214\nN in covered region 141.3791725 17.7757788 0.1257312\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n2  South  84734     1453.23  484.41 39 13 0.08051031 0.01809954 0.2248102\n3  Total 715316     5528.37 1842.79 88 25 0.04133635 0.01181436 0.2858103\n\nAbundance:\n  Label  Estimate        se        cv      lcl       ucl       df\n1 North 12181.419 4638.6284 0.3807954 5499.950 26979.692 12.96781\n2 South  3653.345  910.0975 0.2491135 2181.595  6117.971 17.96263\n3 Total 15834.764 4834.2848 0.3052957 8388.423 29891.167 15.25496\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 North 0.01931774 0.007356107 0.3807954 0.008722023 0.04278538 12.96781\n2 South 0.04311546 0.010740641 0.2491135 0.025746391 0.07220207 17.96263\n3 Total 0.02213674 0.006758251 0.3052957 0.011726878 0.04178736 15.25496\n\n\nCompute combined AIC for entire study area.\n\naic.all &lt;- summary(minke.df.all$ddf)$aic\naic.S &lt;- summary(minke.df.S.strat$ddf)$aic\naic.N &lt;- summary(minke.df.N.strat$ddf)$aic\naic.SN &lt;- aic.S + aic.N\n\n\n\nDetermining the correct stratification to use\nThe AIC value for the detection function in the South was 8.617 and the AIC for the North was 37.28. This gives a total AIC of 45.9. The AIC value for the pooled detection function was 48.64. Because 48.64 is greater than 45.9, estimation of separate detection functions in each stratum is preferable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffering abundance estimates from stratification decision\nIn the full geographical stratification, both encounter rate and detection function were estimated separately for each region (or strata). This resulted in the following abundances:\n\n\n\nAbundance estimates using full geographical stratification.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n9987\n\n\nSouth\n4588\n\n\nTotal\n14575\n\n\n\n\n\n\n\nNext, the distances were combined to fit a pooled detection function but encounter rate was obtained for each region. This resulted in the following abundances:\n\n\n\nAbundance estimates calculating encounter rate by strata and a pooled detection function.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n12181\n\n\nSouth\n3653\n\n\nTotal\n15835\n\n\n\n\n\n\n\n\n\nAnother approach to stratification (advanced)\nAn equivalent result for full geographic stratification could be produced using the dht2 function, which does not require the disaggregation of the data set into two data sets.\n\n# Geographical stratification with stratum-specific detection function \nstrat.specific.detfn &lt;- ds(data=minke, truncation=minke.trunc, key=\"hr\", \n                           adjustment=NULL, formula=~Region.Label)\nabund.by.strata &lt;- dht2(ddf=strat.specific.detfn, flatfile=minke, \n                        strat_formula=~Region.Label, stratification=\"geographical\")\nprint(abund.by.strata, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : R2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label   Area CoveredArea  Effort  n  k    ER se.ER cv.ER\n        North 630582     4075.14 1358.38 49 12 0.036 0.013 0.365\n        South  84734     1453.23  484.41 39 13 0.081 0.018 0.225\n        Total 715316     5528.37 1842.79 88 25 0.048 0.011 0.237\n\nAbundance estimates:\n Region.Label Estimate       se    cv  LCI   UCI     df\n        North     9865 3761.296 0.381 4451 21860 13.035\n        South     4651 1224.818 0.263 2719  7955 22.163\n        Total    14515 3970.578 0.274 8215 25648 16.065\n\nComponent percentages of variance:\n Region.Label Detection    ER\n        North      8.18 91.82\n        South     27.13 72.87\n        Total     12.49 87.51\n\n\nI won’t say anything just now about the wrinkle I introduced with the formula argument in the call to ds(). Recognise there is an alternative (easier) way to perform the full geographic stratification analysis without tearing apart the data. The abundance estimates presented in the last output do not identically match the estimates shown earlier for full geographic stratification, but they are close. The added benefit of this latter analysis is that the uncertainty in the total population size is computed within dht2 rather than needing to be calculated manually using the delta method.\n\n\n\n\n\n\nAn aside\n\n\n\nIf geographic stratification were ignored, the abundance estimate of would be 18,293 minkes. This estimate is substantially larger than the estimates above. The reason is that the survey design was geographically stratified with a smaller proportion of the north stratum receiving sampling effort and a greater proportion of the southern stratum receiving survey effort. Ignoring this inequity in the unstratified analysis would lead us to believe that the more heavily sampled southern stratum is indicative of whale density throughout the study area.",
    "crumbs": [
      "Strata",
      "Analysis of data from stratified surveys **solution** 💡"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html",
    "href": "Pr7/stratumspecific-bias.html",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "",
    "text": "Demonstration\n\n\n\nDistance sampling simulation where detection functions differ between strata. When stratum-specific abundance estimates are produced using a pooled detection function, bias arises. The magnitude of the bias depends upon the magnitude of the difference in the detection functions.",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "href": "Pr7/stratumspecific-bias.html#north-sea-study-area",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "North Sea study area",
    "text": "North Sea study area\nInterest is in estimating the density of minke whales in the western portion of the North Sea, off the east coast of Britain. The study area is divided into north and south strata, with the north stratum being roughly 1.9 times the size of the south stratum, as shown in the map below.\n\nm &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.OceanBasemap)\nm &lt;- m %&gt;% \n  setView(1.4, 55.5, zoom=5)\nminkes &lt;- read_sf(myshapefilelocation)\nstudy.area.trans &lt;- st_transform(minkes, '+proj=longlat +datum=WGS84')\nm &lt;- addPolygons(m, data=study.area.trans$geometry, weight=2)\nm",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "href": "Pr7/stratumspecific-bias.html#properties-of-the-design",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Properties of the design",
    "text": "Properties of the design\nTo demonstrate the estimated number of transects in each stratum, the run.coverage function is used to show the number of replicates in each stratum is allocated roughly according to stratum size.\n\ndesign.properties &lt;- run.coverage(equal.cover, reps = 10, quiet=TRUE)\nmine &lt;- data.frame(Num.transects=design.properties@design.statistics$sampler.count[3,],\n                   Proportion.covered=design.properties@design.statistics$p.cov.area[3,])\nkable(mine)\n\n\n\n\n\nNum.transects\nProportion.covered\n\n\n\n\nSouth\n17\n8.10\n\n\nNorth\n23\n8.11\n\n\nTotal\n40\n8.12",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "href": "Pr7/stratumspecific-bias.html#loop-over-delta-difference-in-sigma-between-strata",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata",
    "text": "Loop over \\(\\Delta\\) difference in \\(\\sigma\\) between strata\n\ndelta.multiplier &lt;- c(seq(from=0.5, to=1.1, by=0.1),\n#                      seq(from=0.85, to=1.15, by=0.1),\n                      seq(from=1.2, to=2.4, by=0.2))\nsigma.south &lt;- 0.3\nnorth.sigma &lt;- sigma.south*delta.multiplier\n\nScale parameter (\\(\\sigma\\)) for the southern stratum remains fixed at 0.3, but in the northern stratum, the scale parameter is a multiple of the southern stratum \\(\\sigma\\), ranging from a low of 0.15 to a maximum of 0.72.\n\nhn &lt;- function(sigma, x) {return(exp(-x^2/(2*sigma^2)))}\nfor (i in seq_along(north.sigma)) {\n  curve(hn(north.sigma[i],x),from=0,to=0.8,add=i!=1,  \n        xlab=\"Distance\", ylab=\"Detection probability\", \n        main=\"Range of detection probability disparity\\nSouth function in blue\")\n}\ncurve(hn(sigma.south,x),from=0,to=0.8, lwd=2, col='blue', add=TRUE)\n\n\n\n\n\n\n\n\n\nequalcover &lt;- list()\nwhichmodel &lt;- list()\nnum.sims &lt;- 10\nfor (i in seq_along(delta.multiplier)) {\n  sigma.strata &lt;- c(sigma.south, sigma.south*delta.multiplier[i])\n  detect &lt;- make.detectability(key.function = \"hn\",\n                               scale.param = sigma.strata,\n                               truncation = 0.8)\n  equalcover.sim &lt;- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = pooled.hn)\n  whichmodel.sim &lt;- make.simulation(reps = num.sims,\n                                    design = equal.cover,\n                                    population.description = minkepop,\n                                    detectability = detect,\n                                    ds.analysis = strat.specific.or.not)\n  equalcover[[i]] &lt;- run.simulation(equalcover.sim, run.parallel = TRUE, max.cores=7)\n  whichmodel[[i]] &lt;- run.simulation(whichmodel.sim, run.parallel = TRUE, max.cores=7)\n}",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "href": "Pr7/stratumspecific-bias.html#conclusions-from-this-portion-of-study",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Conclusions from this portion of study",
    "text": "Conclusions from this portion of study\nNote bias in the estimated density for the entire study area is never greater than 10%, yet another demonstration of pooling robustness. Even with widely differing detection functions, the estimated density ignoring stratum-specific differences is essentially unbiased.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence interval coverage for stratum-specific estimates approaches nominal levels when \\(\\Delta \\approx 1\\). Coverage for the density estimate in the entire study area is nominal for all values of \\(\\Delta\\) with the exception of \\(\\Delta&lt;0.7\\).",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "href": "Pr7/stratumspecific-bias.html#model-selection-sensitivity",
    "title": "Bias in stratum-specific abundance estimates",
    "section": "Model selection sensitivity",
    "text": "Model selection sensitivity\nThis small simulation demonstrates the peril of making stratum-specific estimates when using a detection function that does not recognise stratum-specific detection function differences. This situation can arise when numbers of stratum-specific detections are too small to support stratum-specific detection functions. This set of simulations was devised such that there was sufficient effort in each stratum to avoid small numbers of detections. Even so, use of the “wrong” (pooled) detection function leads to considerable bias in density estimates.\n\nplot(delta.multiplier, modelsel, \n     main=\"Stratum-specific model chosen\", type=\"b\", pch=20,\n     xlab=expression(Delta), ylab=\"Stratum covariate chosen\")\nabline(h=0.50)\n\n\n\n\n\n\n\n\nThere are two messages from this model selection assessment. Only when \\(\\Delta &lt; 0.8\\) or \\(\\Delta &gt; 1.2\\) is there a better than even chance AIC will detect the difference in detectability between strata. Values of \\(\\Delta\\) in this region do not lead to extreme bias in stratum-specific density estimates when the pooled detection function model is used. There is roughly a 10% negative bias in density estimates of the north stratum and a 5% positive bias in density estimates of the southern stratum.",
    "crumbs": [
      "Strata",
      "Bias in stratum-specific abundance estimates"
    ]
  },
  {
    "objectID": "Pr8/covariates.html",
    "href": "Pr8/covariates.html",
    "title": "Covariates practical 8",
    "section": "",
    "text": "Lecture slides for 16 October 2024\n\n\n\n\n\n\n\n\nIt is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\n\n\n\n\n\nPhoto by Roman Mager on Unsplash\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata–we examine whether the geographic stratum effect can be modelled as a covariate.",
    "crumbs": [
      "Covariates",
      "Covariates practical 8"
    ]
  },
  {
    "objectID": "Pr8/covariates.html#covariates-in-detection-function",
    "href": "Pr8/covariates.html#covariates-in-detection-function",
    "title": "Covariates practical 8",
    "section": "",
    "text": "It is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.",
    "crumbs": [
      "Covariates",
      "Covariates practical 8"
    ]
  },
  {
    "objectID": "Pr8/Pr8-instructions.html",
    "href": "Pr8/Pr8-instructions.html",
    "title": "Covariates in detection function model 💻",
    "section": "",
    "text": "This exercise consists of three data sets of increasing difficulty. The first problem, MCDS with point transects, is complicated and (using the functionality available in R) also includes some basic exploratory analysis of the covariates. Section 2 and 3 are optional but will take you deeper into the heart of understanding multiple covariates.",
    "crumbs": [
      "Covariates",
      "Covariates in detection function model 💻"
    ]
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "href": "Pr8/Pr8-instructions.html#exploratory-data-analysis",
    "title": "Covariates in detection function model 💻",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIt is important to gain an understanding of the data prior to fitting detection functions (Buckland et al., 2015). With this in mind, preliminary analysis of distance sampling data involves:\n\nassessing the shape of the collected data,\nconsidering the level of truncation of distances, and\nexploring patterns in potential covariates.\n\nWe begin by assessing the distribution of distances to decide on a truncation distance.\n\nhist(amakihi$distance)\n\nTo see if there are differences in the distribution of distances recorded by the different observers and in each hour after sunrise, boxplots can be used. Note how the ~ symbol is used to define the discrete groupings (i.e. observer and hour).\n\n# Boxplots by obs\nboxplot(amakihi$distance~amakihi$OBs, xlab=\"Observer\", ylab=\"Distance (m)\")\n# Boxplots by hour after sunrise\nboxplot(amakihi$distance~amakihi$HAS, xlab=\"Hour\", ylab=\"Distance (m)\")\n\nThe components of the boxplot are:\n\nthe thick black line indicates the median\nthe lower limit of the box is the first quartile (25th percentile) and the upper limit is the third quartile (75th percentile)\nthe height of the box is the interquartile range (75th - 25th quartiles)\nthe whiskers extend to the most extreme points which are no more than 1.5 times the interquartile range.\ndots indicate ‘outliers’ if there are any, i.e. points beyond the range of the whiskers.\n\nFor minutes after sunrise (a continuous variable), we create a scatterplot of MAS (on the \\(x\\)-axis) against distances (on the \\(y\\)-axis). The plotting symbol (or character) is selected with the argument pch:\n\n# Plot of MAS vs distance (using dots)\nplot(x=amakihi$MAS, y=amakihi$distance, xlab=\"Minutes after sunrise\",\n     ylab=\"Distance (m)\", pch=20)\n\nYou may also want to think about potential collinerity (linear relationship) between the covariates - if collinear variables are included in the detection function, they will be explaining some of the same variation in the distances and this will reduce their importance as a potential covariate. How might you investigate the relationship between HAS and MAS?\nFrom these plots can you tell if any of the covariates will be useful in explaining the distribution of distances?",
    "crumbs": [
      "Covariates",
      "Covariates in detection function model 💻"
    ]
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "href": "Pr8/Pr8-instructions.html#adjusting-the-raw-covariates",
    "title": "Covariates in detection function model 💻",
    "section": "Adjusting the raw covariates",
    "text": "Adjusting the raw covariates\nWe would like to treat OBs and HAS as factor variables as in the original analysis; OBs is, by default, treated as a factor variable because it consists of characters rather than numbers. HAS, on the other hand, consists of numbers and so by default would be treated as a continuous variable (i.e. non-factor). That is fine if we want the effect of HAS to be monotonic (i.e. detectability either increases or decreases as a function of HAS). If we want HAS to have a non-linear effect on detectability, then we need to indicate to R to treat it as a factor as shown below.\n\n# Convert HAS to a factor\namakihi$HAS &lt;- factor(amakihi$HAS)\n\nThe next adjustment is to change the reference level of the observer and hour factor covariates - the only reason to do this is to get the estimated parameters in the detection function to match the parameters estimated in T. A. Marques et al. (2007). You would not carry out this step on your own data. By default R uses the first factor level but by using the relevel function, this can be changed:\n\n# Set the reference level \namakihi$OBs &lt;- relevel(amakihi$OBs, ref=\"TKP\")\namakihi$HAS &lt;- relevel(amakihi$HAS, ref=\"5\")",
    "crumbs": [
      "Covariates",
      "Covariates in detection function model 💻"
    ]
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#candidate-models",
    "href": "Pr8/Pr8-instructions.html#candidate-models",
    "title": "Covariates in detection function model 💻",
    "section": "Candidate models",
    "text": "Candidate models\nWith three potential covariates, there are 8 possible models for the detection function:\n\nNo covariates\nOBs\nHAS\nMAS\nOBs + HAS\nOBs + MAS\nHAS + MAS\nOBs + HAS + MAS\n\nEven without considering covariates there are also several possible key function/adjustment term combinations available: if all key function/covariate combinations are considered the number of potential models is large. Note that covariates are not allowed if a uniform key function is chosen and if covariate terms are included, adjustment terms are not allowed. Even with these restrictions, it is not best practice to take a scatter gun approach to detection function model fitting. Buckland et al. (2015) considered 13 combinations of key function/covariates. Here, we look at a subset of these.\nFit a hazard rate model with no covariates or adjustment terms and make a note of the AIC. Note, that 10% of the largest distances are truncated - you may have decided on a different truncation distance.\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\namak.hr &lt;- ds(amakihi, transect=\"point\", key=\"hr\", truncation=\"10%\",\n              adjustment=NULL, convert_units = conversion.factor)\n\nMake a note of the AIC for this model.\nNow fit a hazard rate model with OBs as a covariate in the detection function and make a note of the AIC. Has the AIC reduced by including a covariate?\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\namak.hr.obs &lt;- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs,\n                  truncation=\"10%\", convert_units = conversion.factor)\n\nFit a hazard rate model with OBs and HAS in the detection function:\n\namak.hr.obs.has &lt;- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs+HAS,\n                      truncation=\"10%\", convert_units = conversion.factor)\n\nTry fitting other possible formula and decide which model is best in terms of AIC. To quickly compare AIC values from different models, use the AIC command as follows (note only models with the same truncation distance can be compared):\n\n# AIC values\nAIC(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nAnother useful function is summarize_ds_models - this has the advantage of ordering the models by AIC (smallest to largest).\n\n# Compare models\nsummarize_ds_models(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nOnce you have decided on a model, plot your selected detection function.",
    "crumbs": [
      "Covariates",
      "Covariates in detection function model 💻"
    ]
  },
  {
    "objectID": "Pr8/Pr8-instructions.html#analysis",
    "href": "Pr8/Pr8-instructions.html#analysis",
    "title": "Covariates in detection function model 💻",
    "section": "Analysis",
    "text": "Analysis\nThe data are available in the Distance package:\n\ndata(ETP_Dolphin)\nhead(ETP_Dolphin, n=3)\n\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint - try dividing the histogram of distances into a large number of intervals.)\nAs there are a number of potential covariates to be used in this example (i.e. search method, cue, Beaufort class and month), try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except group size and because they have numeric codes, use the factor function to let R know to treat them as factors.\nNote that both distances and transect lengths were recorded in nautical miles and area in nautical miles squared and so the argument convert_units does not need to be specified.\nKeep in mind that this is a large dataset (&gt; 1000 observations), and hence estimation may take a while. You will likely end up with quite a few models as there are several potential covariates and no ‘right’ answers. Discuss your choice of final model (or models) with your colleagues - did you make the same choices?",
    "crumbs": [
      "Covariates",
      "Covariates in detection function model 💻"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html",
    "href": "Pr9/countmodel-lines.html",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "",
    "text": "Investigators often wish to make inferences from their surveys beyond simply reporting “What is the animal density?” Ecological curiosity desires answers to questions such as\nAnswers to such questions are sought through use of generalised linear modelling (of which analysis of variance and regression are subsets). However, simple modelling of counts fails to take into account imperfect detectability. When detectability is imperfect, the response variable possesses uncertainty derived from uncertainty in the probability of detection.\nA number of authors have recognised the need to incorporate this uncertainty into the analysis of habitat effects upon estimated density. One approach (employed here) conducts the analyses in two stages: first detectability is estimated and \\(P_a\\) or \\(\\mu\\) or \\(\\rho\\) is estimated. A second stage fits a model to count data adjusting using the estimated detectability parameters. This approach is exemplified by Buckland et al. (2009). Alternatively, single state approaches have been described by Rodríguez-Caro et al. (2017).\nPropagation of uncertainty from the first step into the second step is accomplished via a bootstrap, with the effective area sampled serving as an offset in the generalised linear modelling (GLM) analysis. The method described herein only applies when the link function is the log link, for the following reason.\nWe want to model bird density in our GLM, derived from the counts we observed on each transect. Density is defined as\n\\[D = \\frac{n}{\\text{Eff Area}} \\] where \\(n\\) is the count on each transect.\nTherefore\n\\[\\frac{n}{\\text{Eff Area}} = e^{\\text{linear predictor}}\\] when using a log link function.\nOur count model works with n as the response variable \\[\n\\begin{align}\nn &= \\text{Eff Area} \\cdot e^{\\text{linear predictor}} \\\\\n&= e^{\\text{linear predictor} + log(\\text{Eff Area})}\n\\end{align}\n\\]\nmaking \\(log(\\text{Eff Area})\\) the offset. If other link functions are used, the offset will be different.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#data-organisation",
    "href": "Pr9/countmodel-lines.html#data-organisation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Data organisation",
    "text": "Data organisation\nIn Carlisle’s data set, sightings information is kept separate from information about each site. For our purposes, we will merge those together. In addition, some field names are changed for consistency with functions in the Distance package.\n\nnewsparrow &lt;- merge(sparrowDetectionData, sparrowSiteData, by=\"siteID\", all=TRUE)\nnames(newsparrow) &lt;- sub(\"observer\", \"obs\", names(newsparrow))\nnames(newsparrow) &lt;- sub(\"dist\", \"distance\", names(newsparrow))\nnames(newsparrow) &lt;- sub(\"length\", \"Effort\", names(newsparrow))\nnewsparrow$distance &lt;- as.numeric(newsparrow$distance)\nnewsparrow$Effort &lt;- as.numeric(newsparrow$Effort)",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "href": "Pr9/countmodel-lines.html#analysis-parameters-specification",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Analysis parameters specification",
    "text": "Analysis parameters specification\nAlthough not formally written as a set of functions, we bring to the front of the code arguments the user will need to change to alter to suite their needs. Candidate transect-level predictors for the detection function are observer, bare, herb, shrub, height, shrubclass. The same predictors, with the exception of observer could be used to model the sparrow counts.\n\n\n\n\n\n\nImportant\n\n\n\n\nalpha &lt;- 0.05          # type I error rate for confidence intervals\npointtransect &lt;- FALSE        # survey conducted using lines or points\ndettrunc &lt;- 100  # truncation for detection function\ntransect.length &lt;- newsparrow$Effort[1] # each transect the same length\nmeterstohectares &lt;- 10000\nmyglmmodel &lt;- as.formula(myCount ~ shrub + offset(log(effArea)))  # habitat model to fit to animal density\nnboot &lt;- 50       # number of bootstrap replicates\nset.seed(255992)   # random number seed",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#fit-detection-function",
    "href": "Pr9/countmodel-lines.html#fit-detection-function",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Fit detection function",
    "text": "Fit detection function\nWe fit a series of detection function models using the transect-level covariates available in the Brewer’s sparrow data set.\n\nsurveytype &lt;- ifelse(pointtransect, \"point\", \"line\")\nwoo.o &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~obs, \n            transect=surveytype, quiet=TRUE)\nwoo.bare &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~bare,\n                transect=surveytype, quiet=TRUE)\nwoo.ht &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~height, \n             transect=surveytype, quiet=TRUE)\nwoo.shrc &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~shrubclass, \n            transect=surveytype, quiet=TRUE)\nwoo.shrub &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~shrub, \n            transect=surveytype, quiet=TRUE)\nwoo &lt;- ds(data=newsparrow, truncation=dettrunc, formula=~1, \n          transect=surveytype, quiet=TRUE)\nknitr::kable(summarize_ds_models(woo.o,  woo.bare,\n                                 woo.ht, woo.shrc, woo.shrub, woo), digits=3, \n             caption=\"Halfnormal detection function model selection of covariates.\", row.names = FALSE)\n\n\nHalfnormal detection function model selection of covariates.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM p-value\n\\(\\hat{P_a}\\)\nse(\\(\\hat{P_a}\\))\n\\(\\Delta\\)AIC\n\n\n\n\n\nHalf-normal\n~bare\n0.526\n0.556\n0.025\n0.000\n\n\n\nHalf-normal\n~shrubclass\n0.511\n0.559\n0.025\n1.689\n\n\n\nHalf-normal\n~height\n0.492\n0.558\n0.025\n2.014\n\n\n\nHalf-normal\n~shrub\n0.476\n0.559\n0.025\n2.691\n\n\n\nHalf-normal\n~1\n0.469\n0.563\n0.025\n4.843\n\n\n\nHalf-normal\n~obs\n0.587\n0.559\n0.025\n8.837\n\n\n\n\nwinner &lt;- woo.bare\n\nAll of the candidate models fit the Brewer’s sparrow line transect data. Also note that the estimate detection probability of all six models is the same to the third decimal. The detection function model using bare ground as a predictor is slightly favoured by AIC; we will base our inference on the detection function that includes bare as a covariate. There is likely little effect of this model selection choice upon the ecological question of interest.\n\nPlot of detection function\n\nif(winner$ddf$ds$aux$ddfobj$scale$formula == \"~obs\") {\n  plot(woo.o, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with observer covariate\", pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs1\"), col=\"red\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs2\"), col=\"green\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs3\"), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs4\"), col=\"blue\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.o, data.frame(obs=\"obs5\"), col=\"purple\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Observer\", legend=1:5,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"darkgreen\", \"blue\", \"purple\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~bare\") {\n  plot(woo.bare, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with bare ground covariate\", pdf=pointtransect) \n  quantvals &lt;- quantile(sparrowSiteData$bare, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.bare, data.frame(bare=quantvals[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Bare ground\", legend=quantvals,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"blue\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~height\") {\n  plot(woo.ht, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with shrub height covariate\", pdf=pointtransect) \n  threeheights &lt;- quantile(sparrowSiteData$height, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.ht, data.frame(height=threeheights[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect)\n  legend(\"topright\", title=\"Shrub height\", legend=threeheights,\n         lwd=2, lty=1, col=c(\"red\", \"green\", \"blue\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~shrubclass\") {\n  plot(woo.shrc, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with shrub height class covariate\", pdf=pointtransect) \n  add_df_covar_line(woo.shrc, data.frame(shrubclass=\"Low\"), col=\"red\", lwd=2, lty=1, pdf=pointtransect) \n  add_df_covar_line(woo.shrc, data.frame(shrubclass=\"High\"), col=\"green\", lwd=2, lty=1, pdf=pointtransect) \n  legend(\"topright\", title=\"Shrub height class\", legend=c(\"Low\", \"High\"),\n         lwd=2, lty=1, col=c(\"red\", \"green\"))\n} else if(winner$ddf$ds$aux$ddfobj$scale$formula == \"~shrub\") {\n  plot(woo.shrc, showpoints=FALSE, main=\"Brewer's sparrow\\nDetection with percent shrub covariate\", pdf=pointtransect) \n  quantvals &lt;- quantile(sparrowSiteData$shrub, probs = c(0.25, 0.5, 0.75))\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[1]), col=\"red\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[2]), col=\"darkgreen\", lwd=2, lty=1, pdf=pointtransect)\n  add_df_covar_line(woo.shrub, data.frame(shrub=quantvals[3]), col=\"blue\", lwd=2, lty=1, pdf=pointtransect) \n  legend(\"topright\", title=\"Shrub cover\", legend=quantvals,\n         lwd=2, lty=1, col=c(\"red\", \"green\"))\n}",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "href": "Pr9/countmodel-lines.html#prepare-for-glm-computation",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Prepare for GLM computation",
    "text": "Prepare for GLM computation\nConsiderable processing is necessary to correctly calculate the offset value for the GLM. The offset is the effective area sampled by each transect. Effective area \\((EA)\\) is computed differently, depending upon whether sampling was done with line or point transects.\n\nFor line transects the computation uses the effective strip half-width \\((ESW)\\) multiplied by 2 (for each side of the transect), multiplied by the transect length. \\(ESW\\) is derived for each detection by acquiring the detection probability for the detection from the fitted detection function object detfnobj$ddf$fitted. However a detection probability is only computed for detections at distances less than the truncation distance; so some coding is required to deal with detections beyond the truncation distance. \\(ESW\\) is derived from \\(P_a\\) as \\(P_a \\cdot w\\) where \\(w\\) is the truncation distance.\nFor point transects, effective area is computed as \\(\\pi \\cdot P_a \\cdot w^2\\). This value is returned directly when the predict() function is used, but is derived by calculation from the fitted detection function object detfnobj$ddf$fitted for each detection.\n\nThere is also a distinction between line and point transect computations regarding units in which data are recorded. For line transect example of Brewers Sparrows, Effort was measured in meters, but we wish to produce our estimates of density in numbers per hectare. With Thresher Sparrows, radial detection distance was measured in meters, but we wish to measure bird density in numbers per hectare. In both cases, division by 10000 to convert square meters to hectares.\nBecause only need transect level information, the observation-level information can be removed, to make computation easier. However, transects on which animals were not detected (counts=0) must also be carried forward to the GLM analysis; because 0’s constitute legitimate observations. If the offset values remained missing values (NA), the relationship between animal density and the predictor covariate(s) would not be properly estimated.\n\nsparrowSiteData$effArea &lt;- 2*predict(winner,newdata=data.frame(obs=sparrowSiteData$obs, \n                                                               bare=sparrowSiteData$bare,\n                                                               herb=sparrowSiteData$herb, \n                                                               shrub=sparrowSiteData$shrub,\n                                                               height=sparrowSiteData$height, \n                                                               shrubclass=sparrowSiteData$shrubclass),\n                            esw=TRUE)$fitted*transect.length/meterstohectares\nsitesadj &lt;- sparrowSiteData",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#digression-for-computing-effective-area",
    "href": "Pr9/countmodel-lines.html#digression-for-computing-effective-area",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Digression for computing Effective Area",
    "text": "Digression for computing Effective Area\nI could not convince predict.ds() to behave, so I reverted to first principles. In the function below, I “manually” compute the detection function in the presence of a covariate, demonstrated in the code as shrub as if shrub was the covariate in the detection function. Effective strip width is computed for each transect (below) by integrating this function at the specified level of the covariate for each transect.\n\ngz&lt;-function(z,\n             beta, sigintercept, sigcoef, DistWin,\n             key=\"HR\", w=max(z), predictor){\n#this is a generic detection function that returns the probability of detecting an animal\n#    z               generic distance (perpendicular) - can be scalar or vector\n#    beta            shape coefficient\n#    sigintercept  intercept coefficient for sigma\n#    sigcoef         coefficient for specific factor level\n#    key             the detection function key, works for hazard rate and half normal\n#    w               truncation distance, by default the max of the distances\n#    predictor     the covariate within the detection function influencing sigma\n#\n#RETURNS: a probability\n  \n  if(key != \"HN\" & key != \"HR\") {\n    stop(\"Argument 'key' must be either HN or HR\")\n  }\n  sigma &lt;- exp(sigintercept + sigcoef*predictor)\n  exponent &lt;- exp(beta)\n  if(key==\"HR\") {\n    scale.dist &lt;- z/sigma\n    inside &lt;- -(scale.dist)^(-exponent)\n    gx &lt;- 1 - exp(inside)\n  } else {\n    scale.dist &lt;- z  # debatably don't scale for half normal\n    inside &lt;- -(scale.dist^2/(2*sigma^2))\n    gx &lt;- exp(inside)\n  }\n  return(gx)\n}",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "href": "Pr9/countmodel-lines.html#estimate-relationship-of-density-and-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Estimate relationship of density and covariate",
    "text": "Estimate relationship of density and covariate\nFit a GLM to the observed counts, using as an offset the estimated effective area. By default, specifying family as poisson assumes a log link function. Consequently, the offset must also use the log transform.\nTo generalise the code, and recognising the same call to glm() will need to be made elsewhere in this analysis, we specify the GLM model we wish to fit as an object of type formula. This was specified in the Analysis parameters specification section above.\n\nunivarpredictor &lt;- all.vars(myglmmodel)[2]\nglmmodel &lt;- glm(formula=myglmmodel, family=\"poisson\", data=sitesadj)\nmodelsum &lt;- summary(glmmodel)\ntablecaption &lt;- paste(\"GLM coefficients from counts as function of\", \n                      univarpredictor, \"with log(effective area) offset.\")\nkable(modelsum$coef, digits=4, caption=tablecaption)\n\n\n\nTable 1: GLM coefficients from counts as function of shrub with log(effective area) offset.\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-1.2375\n0.1431\n-8.6477\n0\n\n\nshrub\n0.0886\n0.0101\n8.8100\n0",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#visualise",
    "href": "Pr9/countmodel-lines.html#visualise",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Visualise",
    "text": "Visualise\nEven though we have computed the effective area for each transect based upon the fitted detection function, we have not used that effective area to adjust the observed counts. The simple formula\n\\[\\hat{D}_i = \\frac{n_i}{EA_i}, i = 1, \\ldots , n_{transects}\\] where \\(EA_i\\) is the effective area for the \\(i^{th}\\) transect, describes this adjustment.\nWe plot the estimated density against the continuous univariate predictor.\n\nsitesadj$density &lt;- sitesadj$myCount / sitesadj$effArea",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "href": "Pr9/countmodel-lines.html#incorporate-uncertainty",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Incorporate uncertainty",
    "text": "Incorporate uncertainty\nWe resample our transects with replacement to assess the uncertainty in our point estimates of the relationship between the habitat covariate and the response variable. Specify the number of bootstrap resamples required and allocate storage for the replicate estimates of the GLM parameters: intercept and slope.\n\nintercept.est &lt;- vector(\"numeric\", length=nboot)\nslope.est &lt;- vector(\"numeric\", length=nboot)\n\nSpecify the original site data frame sparrowSiteData in the following code. The code will harvest the estimated slope and intercept coefficients from each replicate. It is uncertainty in the slope coefficient (\\(\\widehat{\\beta_1}\\)) that determines our inference about the effect of shrub cover upon Brewer’s sparrow density. The uncertainty in the slope coefficient as calculated by the code below differs from the uncertainty reported in Table 1 because below we are incorporating uncertainty in the relationship induced by uncertainty in the detection function, which is not recognised in Table 1.\n\nnum.transects &lt;- dim(sparrowSiteData)[1]\nfor (theboot in 1:nboot) {\n  newdetects &lt;- data.frame() \n  bob &lt;- sample(sparrowSiteData$siteID, replace=TRUE, size=length(unique(sparrowSiteData$siteID)))\n#  print(unique(bob))\n  for (bootsite in 1:length(bob)) { \n    thissite &lt;- bob[bootsite] \n    glob &lt;- newsparrow[newsparrow$siteID==thissite, ]\n    glob$siteID &lt;- sprintf(\"rep%02d\", bootsite)\n    newdetects &lt;- rbind(newdetects, glob)  \n  }\n  newdetects &lt;- newdetects[order(newdetects$siteID), ]\n# Refit the detection function model, using observer as a covariate, to the bootstrap replicate\n  detectcovar &lt;- as.formula(winner$ddf$ds$aux$ddfobj$scale$formula)\n  detfnmod &lt;- ds(data=newdetects, truncation=dettrunc, formula=detectcovar, quiet=TRUE, optimizer = \"R\")\n#  Compute effective area offset for each transect\n#  09Mar24 simplify to just transect-level data\n  newtransects &lt;- newdetects[!duplicated(newdetects$siteID), ]\n  esw &lt;- vector(mode=\"numeric\", length=num.transects)\n  hold &lt;- detfnmod$ddf$par\n  for (transect in 1:num.transects) { \n    esw[transect] &lt;- integrate(gz, lower=0, upper=dettrunc, key=\"HN\", \n                               beta=0, sigintercept = hold[1], \n                               sigcoef = hold[2], DistWin=FALSE, \n                               predictor=newtransects[transect, substr(winner$ddf$ds$aux$ddfobj$scale$formula, 2,10)])$value\n  }\n#   If point transects, use effArea =  2 * esw * pi * truncation^2  \n  newtransects$effArea &lt;- 2 * esw * transect.length/meterstohectares\n#   refit the GLM for this bootstrap replicate\n  glmresult &lt;- glm(formula= myglmmodel, family=\"poisson\", data=newtransects)\n  intercept.est[theboot] &lt;- coef(glmresult)[1]\n  slope.est[theboot] &lt;- coef(glmresult)[2] \n}\n\nCode below creates a comprehensive figure of the findings. Steps include\n\ngenerates the predicted relationship of the habitat and Brewer’s sparrow density for each bootstrap replicate,\ncomputes point-wise confidence intervals,\nplot the estimated relationship, all bootstrap replicate relationships and the confidence intervals,\ninsets sampling distribution of (\\(\\widehat{\\beta_1}\\)) and places a confidence interval on the point estimate of (\\(\\widehat{\\beta_1}\\)).\n\n\npredData &lt;- data.frame(predictor=seq(min(sparrowSiteData[ , univarpredictor]),\n                                 max(sparrowSiteData[, univarpredictor]), length.out=100)) \norig.fit &lt;- data.frame(x=predData$predictor, \n                       y=exp(coef(glmmodel)[1] + (coef(glmmodel)[2]*predData$predictor)))\n# long form data frame of all replicate predicted values\nlongform &lt;- NULL\nfor (i in 1:nboot) {\n  mypredict &lt;- exp(intercept.est[i] + (slope.est[i]*predData$predictor))\n  longform &lt;- c(longform, mypredict)\n}\nbig.df &lt;- data.frame(predict=longform)\nbig.df$shrub &lt;- predData$predictor\nbig.df$group &lt;- rep(1:nboot, each=length(predData$predictor))\n# point-wise confidence intervals\nquants &lt;- big.df %&gt;% \n  group_by(shrub) %&gt;% \n  summarise(lower = quantile(predict, alpha/2),\n            upper = quantile(predict, 1-alpha/2)) %&gt;% \n  tidyr::gather(stat, value, -shrub)\n# main plot\nb0label &lt;- bquote(~widehat(beta[0]) == .(round(coef(glmmodel)[1],4)))\nb1label &lt;- bquote(~widehat(beta[1]) == .(round(coef(glmmodel)[2],4)))\nfig1 &lt;- ggplot(dat=sitesadj, aes(shrub, density)) +\n  geom_point() +\n  geom_line(aes(y=predict, group=group), big.df, alpha=0.1) + \n  geom_line(aes(y=y, x=x), orig.fit, colour=\"red\", size=2) +\n  geom_line(aes(y=value, group=stat), quants, col='blue', size=1) +\n  annotate(geom=\"text\", x=20, y=0.6, label= b0label, size=5) +\n  annotate(geom=\"text\", x=20, y=0.3, label= b1label, size=5) +\n  labs(x=\"Percent shrub cover\", y=\"Estimate bird density (per ha)\",\n       title=\"Brewer's sparrow density as function of shrub cover\",\n       subtitle=paste0(\"Detection function model includes \", as.character(detectcovar)[2])) +\n  theme_bw()\n# inset histogram for sampling distribution\nbounds &lt;- quantile(slope.est, probs = c(alpha/2, 1-alpha/2))\nfig2 &lt;- ggplot() + aes(x=slope.est) + geom_histogram() +\n  geom_vline(xintercept=bounds, size=1) +\n  labs(x=\"Estimate of slope\", y=\"Frequency\") +\n  annotate(geom=\"text\", x=bounds[1]-.01, y=11, label=round(bounds[1],3)) +\n  annotate(geom=\"text\", x=bounds[2]+.01, y=11, label=round(bounds[2],3)) \ncomplete &lt;- fig1 + inset_element(fig2, right=.5, bottom=.4, left=.01, top=.99)\ncomplete\n\n\n\n\nRelationship between univariate predictor and Brewer’s sparrow density as modelled by GLM. Offset is estimated covered area. Confidence intervals incorporate uncertainty from imperfect detectability. Inset is sampling distribution of the parameter of interest for Brewer’s sparrow.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "href": "Pr9/countmodel-lines.html#inference-regarding-the-habitat-covariate",
    "title": "Effect of habitat covariate upon estimated density—lines",
    "section": "Inference regarding the habitat covariate",
    "text": "Inference regarding the habitat covariate\nTo assess the effect of the transect-level environmental covariate upon bird density, we focus our attention on \\(\\hat{\\beta_1}\\). The greater in magnitude the slope of the covariate, the greater the effect of the covariate upon animal density. If the estimated slope (\\(\\hat{\\beta_1}\\)) is indistinguishable from zero, we infer the habitat covariate has no influence upon animal density. For the habitat characteristic shrub, there is a positive response of Brewer’s sparrow to increasing shrub cover.",
    "crumbs": [
      "Multipliers",
      "Effect of habitat covariate upon estimated density---lines"
    ]
  },
  {
    "objectID": "Pr9/multi-analysis.html",
    "href": "Pr9/multi-analysis.html",
    "title": "Multispecies and multisession distance sampling analysis",
    "section": "",
    "text": "A multispecies data set with multiple visits\nIt is increasingly common for investigators to conduct surveys in which multiple species are detected and density estimates for several species are of interest. There are many ways of analysing such data sets, but care must be taken. Not all approaches will produce correct density estimates. To demonstrate one of the ways to produce incorrect estimates, we will use the line transect survey data reported in Buckland (2006). This survey (and data file) recorded detections of four species of songbirds. We conduct an analysis of chaffinch (Fringilla coelebs) (coded c in the data file), but similar results would arise with the other species.\nBegin by reading the flat file in a comma delimited format. Note the URL for the data file is very long, double check that you can read the URL including the Github token.\n\nURLpart1 &lt;- \"https://raw.githubusercontent.com/distanceexamples/Distance-multispecies/main/montrave-line.csv\"\nURLpart2 &lt;- \"?token=GHSAT0AAAAAABP6QDHAQ677QTIJEKSK2WYEYWG4EYA\"\nbirds &lt;- read.csv(file=paste0(URLpart1, URLpart2))\n\n\n\nSurvey design\nBuckland’s design consisted of visiting each of the 19 transects in his study twice. To examine some of the errors that can arise from improper analysis, I choose to treat the two visits as strata for the express purpose of generating stratum (visit) -specific density estimates. Density estimates reported in Buckland (2006) are in units of birds \\(\\cdot hectare^{-1}\\).\n\nbirds$Region.Label &lt;- birds$visit\ncu &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n\n\n\nAnalysis of only one species (incorrectly)\nThe direct approach to producing a density estimate for the chaffinch would be to subset the original data frame and use the species-specific data frame for analysis. Begin by performing the subset operation.\n\nchaf &lt;- birds[birds$species==\"c\", ]\n\nWhen the data are subset, the integrity of the survey design is not preserved. A simple frequency table of the species-specific data frame flags up a number of transect/visit combinations where no chaffinches were detected. The result is that the subset data frame suggests 3 of the 19 transects lacked chaffinch detections on the first visit and one of the 19 transects lacked chaffinch detections on the second visit. This revelation, in itself, causes no problems for our estimate of density of chaffinches.\n\ndetects &lt;- table(chaf$Sample.Label, chaf$visit)\ndetects &lt;- as.data.frame(detects)\nnames(detects) &lt;- c(\"Transect\", \"Visit\", \"Detections\")\ndetects$Detections &lt;- cell_spec(detects$Detections, \n                          background = ifelse(detects$Detections==0, \"red\", \"white\"))\nknitr::kable(detects)\n\n\n\n\n\n\n\n\n\nTransect\nVisit\nDetections\n\n\n\n\n1\n1\n3\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n1\n3\n\n\n5\n1\n5\n\n\n6\n1\n4\n\n\n7\n1\n2\n\n\n8\n1\n0\n\n\n9\n1\n1\n\n\n10\n1\n1\n\n\n11\n1\n0\n\n\n13\n1\n1\n\n\n14\n1\n1\n\n\n15\n1\n3\n\n\n16\n1\n2\n\n\n17\n1\n3\n\n\n18\n1\n3\n\n\n19\n1\n0\n\n\n1\n2\n1\n\n\n2\n2\n4\n\n\n3\n2\n3\n\n\n4\n2\n2\n\n\n5\n2\n4\n\n\n6\n2\n3\n\n\n7\n2\n3\n\n\n8\n2\n1\n\n\n9\n2\n0\n\n\n10\n2\n2\n\n\n11\n2\n1\n\n\n13\n2\n1\n\n\n14\n2\n1\n\n\n15\n2\n1\n\n\n16\n2\n1\n\n\n17\n2\n1\n\n\n18\n2\n4\n\n\n19\n2\n1\n\n\n\n\n\nHowever, there is a problem hidden within the table above. Transect 12 does not appear in the table because there were no detections of chaffinches on either visit. Consequently, there were 4 transects without chaffinches on the first visit and 2 transects without chaffinches on the second visit, rather than the 3 transects and 1 transect you might mistakenly conclude do not have chaffinch detections if you relied completely upon the table.\nLet’s see what the ds() function thinks about the survey effort using information from the species-specific data frame.\n\nchaf.wrong &lt;- ds(chaf, key=\"hn\", convert_units = cu, truncation=95, formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(chaf.wrong$dht$individuals$summary) %&gt;%\n  kable_paper(full_width=FALSE) %&gt;%\n  column_spec(6, background=\"salmon\") %&gt;%\n  column_spec(7, background=\"steelblue\")\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\n\n\n\n\n1\n33.2\n82.061\n4.319\n39\n15\n9.029868\n1.1159303\n0.1235821\n\n\n2\n33.2\n83.562\n4.398\n34\n17\n7.730787\n0.9798153\n0.1267420\n\n\nTotal\n66.4\n165.623\n8.717\n73\n32\n8.380327\n0.7425191\n0.0886026\n\n\n\n\n\n\n\n\nExamine the column labelled k (the number of transects) for each of the visits. Rather than the 19 transects that were surveyed on each visit, the ds() function erroneously believes there were only 15 transects surveyed on the first visit and 17 transects surveyed on the second visit.\nNote also the number of detections per kilometer; roughly 9 on the first visit and 7.7 on the second visit. These encounter rates exclude kilometers of effort on transects where there were no detections. We will return to this comparison later.\n\n\nUse explicit data hierarchy\n\n\n\n\n\n\nDescribing the survey design to ds\n\n\n\nAdditional arguments can be passed to ds() to resolve this problem. Consulting the ds() documentation\n\nregion_table data.frame with two columns:\n\nRegion.Label label for the region\nArea area of the region\nregion_table has one row for each stratum. If there is no stratification then region_table has one entry with Area corresponding to the total survey area. If Area is omitted density estimates only are produced.\n\nsample_table data.frame mapping the regions to the samples (i.e. transects). There are three columns:\n\nSample.Label label for the sample\nRegion.Label label for the region that the sample belongs to.\nEffort the effort expended in that sample (e.g. transect length).\n\n\n\n\nThis analysis that produces erroneous results can be remedied by explicitly letting the ds() function know about the study design; specifically, how many strata and the number of transects within each stratum (and associated transect lengths).\nConstruct the region table and sample table showing the two strata with equal areas and each labelled transect (of given length) is repeated two times.\n\nbirds.regiontable &lt;- data.frame(Region.Label=as.factor(c(1,2)), Area=c(33.2,33.2))\nbirds.sampletable &lt;- data.frame(Region.Label=as.factor(rep(c(1,2), each=19)),\n                                Sample.Label=rep(1:19, times=2),\n                                Effort=c(0.208, 0.401, 0.401, 0.299, 0.350,\n                                         0.401, 0.393, 0.405, 0.385, 0.204,\n                                         0.039, 0.047, 0.204, 0.271, 0.236,\n                                         0.189, 0.177, 0.200, 0.020))\n\n\n\nSimple detection function model\nThe chaffinch analysis is performed again, this time supplying the region_table and sample_table information to ds(). The correct number of transects (19) sampled on both visits (even though chaffinch was not detected on 4 transects on visit 1 and 2 transects on visit 2) is now recognised. Hence, the use of region table and sample table solves the problem of effort miscalculation if a species is not detected on all transects.\n\ntr &lt;- 95   # as per Buckland (2006)\nonlycf &lt;- ds(data=birds[birds$species==\"c\", ], \n             region_table = birds.regiontable,\n             sample_table = birds.sampletable,\n             trunc=tr, convert_units=cu, key=\"hn\", formula = ~Region.Label)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 650.999\n\nknitr::kable(onlycf$dht$individuals$summary) %&gt;%\n  kable_paper(full_width=FALSE) %&gt;%\n  column_spec(6, background=\"salmon\") %&gt;%\n  column_spec(7, background=\"steelblue\")\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\n\n\n\n\n1\n33.2\n91.77\n4.83\n39\n19\n8.074534\n1.2196305\n0.1510465\n\n\n2\n33.2\n91.77\n4.83\n34\n19\n7.039338\n1.0612781\n0.1507639\n\n\nTotal\n66.4\n183.54\n9.66\n73\n38\n7.556936\n0.8083641\n0.1069698\n\n\n\n\n\n\n\n\n\n\nConsequence of incorrect analysis\nTo drive home the consequence of failing to properly specify the survey effort, contrast the encounter rate for the two visits from the incorrect calculations above (9.0 and 7.7 respectively), with the correct calculation (8.1 and 7.0 respectively). The number of transects is incorrect with the knock-on effect of effort being incorrect. If effort is incorrect then so too is covered area.\nThe ripple effect from incomplete information about the survey design results in positively biased estimates of density.\n\n\n\n\n\nReferences\n\nBuckland, S. T. (2006). Point-transect surveys for songbirds: Robust methodologies. The Auk, 123(2), 345–357. https://doi.org/10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2",
    "crumbs": [
      "Multipliers",
      "Multispecies and multisession distance sampling analysis"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html",
    "href": "Pr9/Pr9-instructions.html",
    "title": "Analyses using multipliers 💻",
    "section": "",
    "text": "We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets 2 and 3 deal with instantaneous cues and so only cue rate needs to be taken into account.",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#getting-started",
    "href": "Pr9/Pr9-instructions.html#getting-started",
    "title": "Analyses using multipliers 💻",
    "section": "Getting started",
    "text": "Getting started\nThese data (called sikadeer) are available in the Distance package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?\n\nlibrary(Distance)\ndata(sikadeer)\nconversion.factor &lt;- convert_units(\"centimeter\", \"kilometer\", \"square kilometer\")",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "href": "Pr9/Pr9-instructions.html#fit-detection-function-to-dung-pellets",
    "title": "Analyses using multipliers 💻",
    "section": "Fit detection function to dung pellets",
    "text": "Fit detection function to dung pellets\nFit the usual series of models (i.e. half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don’t spend too long on this). Call your model deer.df. This detection function will be used to obtain \\(\\hat D_{\\textrm{pellet groups}}\\).\nHave a look at the Summary statistics for this model - what do you notice about the allocation of search effort in each woodland?",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr9/Pr9-instructions.html#multipliers",
    "href": "Pr9/Pr9-instructions.html#multipliers",
    "title": "Analyses using multipliers 💻",
    "section": "Multipliers",
    "text": "Multipliers\nThe next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.\nData to calculate this has been collected in the file IntroDS_9.1.csv that can be read from the Github internet repository. Following code comes from Meredith (2017).\n\nMIKE.persistence &lt;- function(DATA) {\n  \n#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data \n#  Input: data frame with at least two columns:\n#         DAYS - calendar day on which dung status was observed\n#         STATE - dung status: 1-intact, 0-decayed\n#  Output: point estimate, standard error and CV of mean persistence time\n#\n#  Attribution: code from Mike Meredith website: \n#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm\n#   Citing: CITES elephant protocol\n#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf\n  \n  ##   Fit logistic regression model to STATE on DAYS, extract coefficients\n  dung.glm &lt;- glm(STATE ~ DAYS, data=DATA, family=binomial(link = \"logit\"))\n  betas &lt;- coefficients(dung.glm)\n  ##   Calculate mean persistence time\n  mean.decay &lt;- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]\n  ## Calculate the variance of the estimate\n  vcovar &lt;- vcov(dung.glm)\n  var0 &lt;- vcovar[1,1]  # variance of beta0\n  var1 &lt;- vcovar[2,2]  # variance of beta1\n  covar &lt;- vcovar[2,1] # covariance\n  deriv0 &lt;- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]\n  deriv1 &lt;- -mean.decay/betas[2]\n  var.mean &lt;- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2\n  ## Calculate the SE and CV and return\n  se.mean &lt;- sqrt(var.mean)\n  cv.mean &lt;- se.mean/mean.decay\n  out &lt;- c(mean.decay, se.mean, 100*cv.mean)\n  names(out) &lt;- c(\"Mean persistence time\", \"SE\", \"%CV\")\n  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab=\"Days since initiation\",\n       ylab=\"Dung persists (yes=1)\",\n       main=\"Eight dung piles revisited over time\")\n  curve(predict(dung.glm, data.frame(DAYS=x), type=\"resp\"), add=TRUE)\n  abline(v=mean.decay, lwd=2, lty=3)\n  return(out)\n}\ndecay &lt;- read.csv(\"https://raw.githubusercontent.com/erex/Oct-Quarto/main/Pr9/IntroDS_9.1.csv\")\npersistence.time &lt;- MIKE.persistence(decay)\nprint(persistence.time)\n\nRunning the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been ‘jittered’ to avoid over-plotting.\nAn estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.\nAs stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:\n\ncreation contains estimates of the dung production rate and associated standard error\ndecay contains the dung decay rate and associated standard error where XX and YY are the estimates you obtained from the dung decay rate analysis.\n\n\n# Create list of multipliers\nmult &lt;- list(creation = data.frame(rate=25, SE=0),\n#             decay    = data.frame(rate=XX, SE=YY))\nprint(mult)\n\nThe final step is to use these multipliers to convert \\(\\hat D_{\\textrm{pellet groups}}\\) to \\(\\hat D_{\\textrm{deer}}\\) (as in the equations above) - for this we need to employ the dht2 function. In the command below the multipliers= argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:\n\nstrat_formula=~Region.Label is specified to take into account the design (i.e. different woodlands or blocks).\nstratification=\"effort_sum\" is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.\ndeer.df is the detection function you have fitted.\n\n\n# Weight by effort because we have repeats\ndeer.ests &lt;- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,\n                 convert_units=conversion.factor, multipliers=mult, \n                 stratification=\"effort_sum\", total_area=13.9)\nprint(deer.ests)\n\nThe function dht2 also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata.",
    "crumbs": [
      "Multipliers",
      "Analyses using multipliers 💻"
    ]
  },
  {
    "objectID": "Pr3/prac3.html",
    "href": "Pr3/prac3.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "Pr3/prac3.html#accessing-the-data",
    "href": "Pr3/prac3.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe summary indicates that the minimum distance is 0 and the maximum is 35.8 metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "Pr3/prac3.html#truncation",
    "href": "Pr3/prac3.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLooking at a summary of the model object.\n\n\n\n\n\n\nQuestions:\n\n\n\n\n\n\nHow many objects were detected? \nWhat is the maximum observed perpendicular distance? \n\n\n\n\nPlot the detection function and specify many histogram bins:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nGenerate a summary and plot to see what effect truncation has had.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Pr3/prac3.html#exploring-different-models",
    "href": "Pr3/prac3.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "Pr3/prac3.html#converting-exact-distances-to-binned-distances",
    "href": "Pr3/prac3.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe could be more intelligent in our choice of cutpoints for our binned analysis of we recognise the rounding in the original data. Given the rounding is to distances ending in 0, we do not want our cutpoints to also end in 0. Rather, we want the 0 values in the data to land in the approximate centre of our manufactured bins. We accomplish this by creating this series of cutpoints:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompare density estimate produced by binning the distances with density estimates produced by different key function models fitted to exact distances.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\nFor this data set (and most good data sets), binning of exact distances has a dramaticconsiderablesmallno effect on estimated abundance.\n\n\n\nClick here for a hint\n\nThis text will only appear after you ask for help"
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 49 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n 95% of the data fall within this range there is a 95% probability that the true mean lies within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "Pr3/prac3.html#your-answer-tally",
    "href": "Pr3/prac3.html#your-answer-tally",
    "title": "Assessing line transect detection functions",
    "section": "Your answer tally",
    "text": "Your answer tally\nOf the questions available on this page, you have answered correctly."
  },
  {
    "objectID": "Pr3/prac3.html#questions-1",
    "href": "Pr3/prac3.html#questions-1",
    "title": "Assessing line transect detection functions",
    "section": "Questions:",
    "text": "Questions:\nContrast the truncation distances and number of detections in the analysis from the different forms of truncation. Previously, you noted there were 105 detections at a maximum of 35.8 meters in the untruncated data.\n\nUntruncated data\n\nWhat is the estimated proportion of objects detected (to 3 decimals) to 35.8m in the untruncated data? \n\n\n\nTruncation at 20m\n\nHow many objects were included in the analysis? \n\nIn other words, how many objects were removed from the analysis? \n\nWhat is the estimated proportion of objects detected (to 3 decimals) to 20m? \n\n\n\nTruncation of 10% of the detections\n\nHow many objects were included in the analysis? \n\nIn other words, how many objects were removed from the analysis? \n\nWhat is the maximum observed perpendicular distance (to 2 decimals)? \nWhat is the estimated proportion of objects detected (to 3 decimals) with 10% truncation? \n\n\n\nDensity estimate differences because of truncation\n\nWhat is the estimated density (to 1 decimal) with no truncation? \nWhat is the estimated density (to 1 decimal) with truncation at 20m? \nWhat is the estimated density (to 1 decimal) with 10% truncation? \nFor this data set (and most good data sets), truncation has a dramaticconsiderablesmallno effect on estimated abundance."
  },
  {
    "objectID": "Pr2/prac2-tutorial.html#practical-2-duck-nest-analysis",
    "href": "Pr2/prac2-tutorial.html#practical-2-duck-nest-analysis",
    "title": "Tutorial–ducknest analysis in R ✏️",
    "section": "Practical 2 – Duck nest analysis",
    "text": "Practical 2 – Duck nest analysis\nThis has been your first experience using the Distance package. There are only a handful of functions you need to successfully complete a distance sampling analysis. This practical gave you experience with these functions. The data are familiar to you because they the same data you used when trying to fit a detection function by hand in Exercise 1. This exercise lets the computer do the work. Compare the estimate of duck nest density produced by Distance with the estimate you manually produced.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the estimate value of \\(\\hat{P}_a\\)?\n\n 0.869 0.039 614.25 0.933\n\nGiven your estimate of \\(\\hat{P}_a\\), what is its meaning?\n\n distance to which nests were detected area under the curve proportion of nests detected within 2.4m of transects number of iterations to estimate detection function parameters probability of detecting a nest within 2.4m of transect\n\n\n\nDensity converted to abundance\nIn the ducknest data frame, I failed to specify the size of the study area in the Area field. As a consequence, the ds function is only able to provide a density estimate. Given the size of the Monte Vista refuge is 47.7 \\(km^2\\), and the estimate of nest density from the output provided, complete the code fragment to provide an estimate of the abundance of nests.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the estimated number of nests on the refuge? \nWhat assumption does our estimated abundance of nests on the refuge rest?\n\n Nests are uniformly distributed over refuge Ducks like water Transects representatively sample the refuge Detection function model fits the data\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHow should the plot be interpreted?\n\n model that fits has all points near the 45° line this plot has nothing to do with model fit a rapid rise in the graphed points implies good model fit\n\nThe numerical output from gof_ds is from the Cramer-von Mises test. Interpret the output.\n\n P-value is &lt;1, therefore the model fit is poor The test statistic is smaller than the P-value, therefore the fit is adequate The test statistic is close to 0, therefore the fit is poor P-value is close to one, indicating there is support for proposition the model fits the data\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo reinforce the idea that similar results are produced (for good data) from the different key functions, write a few lines of code to explore this, using R as a calculator. Value entered to one decimal place.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the percentage difference between smallest and greatest density estimates using these three models?",
    "crumbs": [
      "Detection functions",
      "Tutorial--ducknest analysis in R ✏️"
    ]
  },
  {
    "objectID": "Pr2/prac2-tutorial.html#questions",
    "href": "Pr2/prac2-tutorial.html#questions",
    "title": "Tutorial–ducknest analysis in R",
    "section": "Questions:",
    "text": "Questions:\nContrast the truncation distances and number of detections in the analysis from the different forms of truncation. Previously, you noted there were 105 detections at a maximum of 35.8 meters in the untruncated data.\n\nUntruncated data\n\nWhat is the estimated proportion of objects detected (to 3 decimals) to 35.8m in the untruncated data? \n\n\n\nTruncation at 20m\n\nHow many objects were included in the analysis? \n\nIn other words, how many objects were removed from the analysis? \n\nWhat is the estimated proportion of objects detected (to 3 decimals) to 20m? \n\n\n\nTruncation of 10% of the detections\n\nHow many objects were included in the analysis? \n\nIn other words, how many objects were removed from the analysis? \n\nWhat is the maximum observed perpendicular distance (to 2 decimals)? \nWhat is the estimated proportion of objects detected (to 3 decimals) with 10% truncation? \n\n\n\nDensity estimate differences because of truncation\n\nWhat is the estimated density (to 1 decimal) with no truncation? \nWhat is the estimated density (to 1 decimal) with truncation at 20m? \nWhat is the estimated density (to 1 decimal) with 10% truncation? \nFor this data set (and most good data sets), truncation has a dramaticconsiderablesmallno effect on estimated abundance."
  },
  {
    "objectID": "Pr2/prac2-tutorial.html#exploring-different-models",
    "href": "Pr2/prac2-tutorial.html#exploring-different-models",
    "title": "Tutorial–ducknest analysis in R",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "Pr2/prac2-tutorial.html#converting-exact-distances-to-binned-distances",
    "href": "Pr2/prac2-tutorial.html#converting-exact-distances-to-binned-distances",
    "title": "Tutorial–ducknest analysis in R",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe could be more intelligent in our choice of cutpoints for our binned analysis of we recognise the rounding in the original data. Given the rounding is to distances ending in 0, we do not want our cutpoints to also end in 0. Rather, we want the 0 values in the data to land in the approximate centre of our manufactured bins. We accomplish this by creating this series of cutpoints:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompare density estimate produced by binning the distances with density estimates produced by different key function models fitted to exact distances.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\nFor this data set (and most good data sets), binning of exact distances has a dramaticconsiderablesmallno effect on estimated abundance.\n\n\n\nClick here for a hint\n\nThis text will only appear after you ask for help"
  },
  {
    "objectID": "Pr4/prac4-tutorial.html",
    "href": "Pr4/prac4-tutorial.html",
    "title": "Tutorial–assessing precision ✏️",
    "section": "",
    "text": "Photo by Michael Shannon from Unsplash",
    "crumbs": [
      "Precision",
      "Tutorial--assessing precision ✏️"
    ]
  },
  {
    "objectID": "Pr4/prac4-tutorial.html#practical-4-poor-precision-in-an-abnormal-situation",
    "href": "Pr4/prac4-tutorial.html#practical-4-poor-precision-in-an-abnormal-situation",
    "title": "Tutorial–assessing precision ✏️",
    "section": "Practical 4 – Poor precision in an abnormal situation",
    "text": "Practical 4 – Poor precision in an abnormal situation\nRemember that the “usual” estimator for encounter rate variance is based upon the idea that transects are distributed randomly rather than systematically with a random start. Under rare circumstances, this can over-estimate encounter rate variance and consequently, variance of density and abundance estimates. Revisit the output from Practical 4 and answer these questions.\n\nQuestions\nBefore getting absorbed in the analysis, what difficulties were caused because of the survey design.\n\nIdentify the design flaws of the survey that produced such poor precision:\n\n too few replicate transects ignorance of animal gradient truncation too narrow leading to too few detections inadequate survey effort\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhy is the estimate of abundance exactly half the estimate of density in this analysis?\n\n \\(\\hat{N} = \\hat{D} \\times A\\), where A=1/2 that is characteristic of the half normal detection function inadequate survey effort because of convert_units, square kilometers are twice as large as kilometers\n\n\nHere is the summary output from the standard analysis of this data set. Alarm bells should ring in your head as you examine this output; specifically the variance components constituting the uncertainty in your abundance or density estimates.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn normal circumstances, approximately 75% of abundance estimate uncertainty in line transects comes from encounter rate variance. What percentage of uncertainty here comes from encounter rate uncertainty? \n\n\nDoes bootstrapping resolve the issue?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhy does the use of the bootstrap not resolve the poor precision problem?\n\n bootstrap only improves precision of estimated detection function bootstrap does not perform well with half normal detection function resampling transects does not alter variability in encounter rate between transects bootstrapping is over-rated, never works well",
    "crumbs": [
      "Precision",
      "Tutorial--assessing precision ✏️"
    ]
  },
  {
    "objectID": "Pr5/prac5-tutorial.html#practical-5-point-transects",
    "href": "Pr5/prac5-tutorial.html#practical-5-point-transects",
    "title": "Tutorial–point transect surveys ✏️",
    "section": "Practical 5 – Point transects",
    "text": "Practical 5 – Point transects\nThis data set was simulated so we know both the true population density and the true underlying detection function. We remain interested in the robustness of density estimates across the range of key function models. Examine the largest and smallest density estimates to discover the ranges of density estimates.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRepeat the calculations that you made at the conclusion of Exercise 3; this time looking at the relative difference between the most extreme density estimates for this data set (remember true density is 79.8 per \\(km^2\\), just as it was for the simulated line transect data). Use the three key functions (uniform with cosine adjustment, half normal and hazard rate) with a 20m truncation distance suggested in the exercise.\nDetermine the magnitude (in percent) of the range in density estimates produced by these three models fitted to the simulated data. Reflect on the magnitude of this range vis-a-vis the range in estimates among models for the line transect data set.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nTo the nearest 5%, what is the relative percentage difference between smallest and largest estimates?",
    "crumbs": [
      "Points",
      "Tutorial--point transect surveys ✏️"
    ]
  },
  {
    "objectID": "Pr5/prac5-tutorial.html#wren-data-from-buckland-montrave-study",
    "href": "Pr5/prac5-tutorial.html#wren-data-from-buckland-montrave-study",
    "title": "Tutorial–point transect surveys ✏️",
    "section": "Wren data from Buckland Montrave study",
    "text": "Wren data from Buckland Montrave study\nThe analysis you were asked to do for these data sets was not as exhaustive as the previous analysis. There is not a suite of models fitted to the two data sets; only a single model for each type of point transect. The model selection was carried out in Buckland (2006). Emphasis here is upon recognising differences in estimated density arising from the two methods of data collection.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nContrast the point estimates and precision derived from the two survey methods for winter wrens.\n\n\nHow much larger (nearest 5%) is 5-minute density estimate than snapshot density estimate? \nWhich data collection method produces the more precise estimate (by a considerable amount)? 5 minutesnapshot\nThere is evidence of evasive movement if you look at the distribution of radial distances. Evasive movement was sufficient to make inference suspect for either the 5-minute or snapshot data. TRUEFALSE",
    "crumbs": [
      "Points",
      "Tutorial--point transect surveys ✏️"
    ]
  },
  {
    "objectID": "Pr6/prac6-tutorial.html#exercise-6-survey-design",
    "href": "Pr6/prac6-tutorial.html#exercise-6-survey-design",
    "title": "Tutorial–design of surveys ✏️",
    "section": "Exercise 6 – Survey design",
    "text": "Exercise 6 – Survey design\n\n\n\n\n\n\nCaution\n\n\n\n\n\n\nGenerating this self-assessment check takes 3-5 minutes, be patient.\nOpen another tab in your browser and check your email while waiting, then come back to this tab.\n\n\n\n\nOutput provided by the dssd package is extensive; the following is intended to help you find the most important aspects of that output so you can effectively assess the merits of the survey designs you produce. The following questions are intended to have you pick through the output and think about what some of those values are telling you. The questions also ask you to do some “sense checking” to ensure the software is performing the way you expect (never trust software to do the right thing).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAnswer these questions by examining at the output above\nUse some common sense about the parallel aerial survey you have designed. If the design specification is to place transects 5000m (5km) apart and further if truncation distance is 2000m (on either side of the aircraft), employ some simple geometry to assess whether dssd is doing the right thing.\n\nWhat should be the percentage of the study area covered by survey effort? (to the nearest 5 percent)? \n\nDo you understand the answer based upon basic geometry?\n\n\nThe design problem you were to solve treated the fuel capacity of the aircraft as the constraint around which you needed to operate. Remember, the aircraft chosen could only travel 250km without refueling. That constraint lead you to produce a survey in which the parallel transects were spaced at 5km.\nHowever, now take a broader view of the problem from a statistical rather than a logistical perspective. Look in the output provided to determine the number of replicate transects resulting from spacing the transects 5km apart and orienting the transects east-to-west.\n\n\nHow many replicate transects result from this realisation of the design? \nBased upon what you learned from the precision discussion of last Thursday, this is a sufficient number of transects to estimate encounter rate variance well: TRUEFALSE\nWhat possible solutions (more than one answer) might there be to this lack of replication?\n\n find another study area hire an aircraft with greater fuel capacity shorten the truncation distance",
    "crumbs": [
      "Design",
      "Tutorial--design of surveys ✏️"
    ]
  },
  {
    "objectID": "Pr6/prac6-tutorial.html#comparing-parallel-and-zigzag-design",
    "href": "Pr6/prac6-tutorial.html#comparing-parallel-and-zigzag-design",
    "title": "Tutorial–design of surveys ✏️",
    "section": "Comparing parallel and zigzag design",
    "text": "Comparing parallel and zigzag design\nDesign-based inference rests upon the premise every location in the study area has the same chance of being sampled as every other location. In other words, coverage scores is uniform. We use the heat maps created by plotting design objects to visually assess this uniformity.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat numerical output might provide a more objective relative assessment of uniformity between designs?\n\n mean line length sd (standard deviation) of trackline length median coverage score sd (standard deviation) of coverage score\n\nUsing the metric you chose in the previous question, which (parallel or zigzag) appears to have the more uniform coverage score?\n\n parallel zigzag",
    "crumbs": [
      "Design",
      "Tutorial--design of surveys ✏️"
    ]
  },
  {
    "objectID": "Pr6/prac6-tutorial.html#tentsmuir-point-transect-survey-design",
    "href": "Pr6/prac6-tutorial.html#tentsmuir-point-transect-survey-design",
    "title": "Tutorial–design of surveys ✏️",
    "section": "Tentsmuir point transect survey design",
    "text": "Tentsmuir point transect survey design\nWhen designing the Tentsmuir survey, note that the design feature being specified is the number of transects. This is in contrast to the St Andrews Bay survey, in which you specified the spacing of transects (to ensure returning to the airport). With the Tentsmuir survey, specifying the number of point transects causes dssd to determine the number of point transects that can be systematically placed within the two strata of this study.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat was the resulting spacing (m) for the main stratum for one realisation of the design? (nearest 10 meters) \nWhat proportion of the main stratum receives survey effort from the points with sampling radius of 100m? (nearest 5 percent) \nWhat was the resulting spacing (m) for the Morton Lochs for one realisation of the design stratum? (nearest 10 meters) \nWhat proportion of the Morton Lochs stratum receives survey effort from the points with sampling radius of 100m? (nearest 5 percent) \n\n\nNote that even though the main stratum has 10 more point transects placed within it, the proportion of that stratum covered by sampling effort is much, much smaller than the coverage proportion for the smaller Morton Loch stratum.\n\nUsing the metric you chose in the previous question, which stratum appears to have the more uniform coverage score? mainMorton Lochs\n\nAlso note that the range of coverage scores is quite different between the strata: 0.0-0.15 for main, 0.19-0.73 for Morton Lochs. The mean coverage score for Morton Lochs (0.58) is much closer to the maximum than to the minimum because a smaller number of coverage grid points (those near the edge of the stratum) suffer from low coverage scores. This small stratum (71ha, 1/20th the size of the main stratum) has a high perimeter-to-area ratio, a situation in which edge effects are likely to arise.",
    "crumbs": [
      "Design",
      "Tutorial--design of surveys ✏️"
    ]
  },
  {
    "objectID": "Pr7/prac7-tutorial.html",
    "href": "Pr7/prac7-tutorial.html",
    "title": "Tutorial–analysis of stratified surveys ✏️",
    "section": "",
    "text": "Photo by Rowan Simpson on Unsplash",
    "crumbs": [
      "Strata",
      "Tutorial--analysis of stratified surveys ✏️"
    ]
  },
  {
    "objectID": "Pr7/prac7-tutorial.html#exercise-7-analysis-of-stratified-surveys",
    "href": "Pr7/prac7-tutorial.html#exercise-7-analysis-of-stratified-surveys",
    "title": "Tutorial–analysis of stratified surveys ✏️",
    "section": "Exercise 7 – Analysis of stratified surveys",
    "text": "Exercise 7 – Analysis of stratified surveys\nThis set of questions is completely about the practicalities of interpreting output from ds. It is far too easy to be beguiled by the rafts of output generated by a distance sampling analysis; we often just look at the estimated density or abundance and scurry along–particularly if the data are not your own.\nClosely examine the results from this particular analysis, where the strata are analysed separately (distinct detection functions). Recall the reason strata were introduced with this data set is because the southern portion of the study area (Southern Ocean) likely possessed a richer food source, hence there was a belief that there would be higher minke whale densities in the south than in the north. Let’s examine the output to see if this belief was supported by the data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAnswer these questions by examining at the output above\nBefore answering the questions, you’ll need to orient yourself to the order in which the stratum results are presented. Hint: the stratum label appears in each piece of output.\n\n\nHow many times larger (geographically) is the northern stratum compared to the southern stratum? (to the nearest 0.5) \nHow many times larger is the estimated abundance in the northern stratum, compared to the south? (to the nearest 0.5) \nExplain the paradox how can the southern stratum have a higher density but lower abundance of minke whales?\n\n detectability differs between strata whales from the southern stratum vacation in the northern stratum the greater area of the northern stratum compensates for the low whale density density is actually the reciprocal of abundance, a mathematical paradox\n\n\n\n\n\n\n\n\nComments about minke stratified analysis\n\n\n\n\n\n\nIt is common for high quality habitat to be more scarce than low quality habitat.\nThis serves as a lesson for future survey design:\n\ndo not design a survey such that all survey effort is allocated toward sampling the high quality habitat where you think most animals reside. It might be the case that the lower quality habitat actually contains most of your study animals.\n\n\n\n\n\n\n\n\nAbout the possible difference in detectability\nYou can see that the estimate of \\(\\hat{P}_a\\) is different for the two strata. But examine the output to find the estimates of the scale parameter \\(\\hat{\\sigma}\\) for the two strata. Notice they both appear to be negative. Use the code below to convert those estimates of \\(\\hat{\\sigma}\\) into biologically interpretable quantities. Enter values with 2 decimal places.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWith your knowledge of \\(\\hat{\\sigma}\\) for each stratum, describe the visibility in the northern vs southern strata.\n\n larger estimate of sigma in the north implies better visibility in the north larger estimate of sigma in the south suggests better visibility in the south smaller estimate of sigma in the south implies better visibility in the south\n\n\n\n\nClick here for a hint\n\nWeather conditions are poorer in the south which is closer to the ice edge.",
    "crumbs": [
      "Strata",
      "Tutorial--analysis of stratified surveys ✏️"
    ]
  },
  {
    "objectID": "Pr7/prac7-tutorial.html#statistical-difference-in-density-between-strata-supplement",
    "href": "Pr7/prac7-tutorial.html#statistical-difference-in-density-between-strata-supplement",
    "title": "Tutorial–analysis of stratified surveys ✏️",
    "section": "Statistical difference in density between strata supplement",
    "text": "Statistical difference in density between strata supplement\nIt is trivially easy to calculate the difference between two density estimates. The messy part is comparing the magnitude of that difference against the uncertainty in that difference (ratio of signal (difference) to noise (uncertainty)). This signal-to-noise ratio is measured by the traditional t-test, with the numerator being the signal (difference) while the denominator is a measure of noise (standard error of the estimated difference).\nThe challenge is to estimate the variance of the difference in estimated density estimates. The variance of a difference is the sum of variances in the two estimates when those estimates are independent; which they are when the density estimates are independently computed. The calculation is more tricky when the two density estimates share a common detection function; the estimates are no longer independent. That is why there are two different formulas for computing the test statistic for estimated density differences, and the function that computes the test statistic can cope with the two situation.\n&gt; density.difference.ds(hazard.pooled)\n  n.detect  cv.ER line.length n.transects group.size group.size.cv\n1       39 0.2323         484          13     2.1538        0.1397\n2       49 0.3724        1370          12     2.3265        0.2303\n  nparm.detect  D.hat D.hat.cv     f0   f0cv\n1            2 0.0929   0.2916 1.0711 0.1073\n2            2 0.0446   0.4508 1.0711 0.1073\n  D1.minus.D2 SE.difference t.statistic df.t.stat P.value     LCB    UCB\n1      0.0484        0.0402      1.2048   49.2919   0.234 -0.0323 0.1291\n\n&gt; density.difference.ds(ideal.hr, marginal.hr)\n  n.detect  cv.ER line.length n.transects group.size group.size.cv\n1       39 0.2323         484          13     2.1538        0.1397\n2       49 0.3724        1370          12     2.3265        0.2303\n  nparm.detect  D.hat D.hat.cv     f0   f0cv\n1            2 0.1167   0.3182 1.3450 0.1338\n2            2 0.0365   0.4451 0.8781 0.1315\n  D1.minus.D2 SE.difference t.statistic df.t.stat P.value     LCB    UCB\n1      0.0802        0.0405      1.9779   27.6636   0.058 -0.0029 0.1633\n\n\n\n\nWhy does the CV(detection function) increase when separate detection functions are fitted to each stratum?\n\n estimated difference in density is larger when separate detection functions are fitted  number of detections is larger (perhaps sufficient) only when the two strata are combined\n\nWhy does the CV(encounter rate) not increase when separate detection functions are fitted to each stratum?\n\n pooled detection functions reduce encounter rate variability encounter rate variability depends upon weather conditions encounter rate variability is always computed stratum-by-stratum\n\nWhat is the relative magnitude of CV(detection function) to CV(encounter rate) for either pooled or separate analyses?\n\n cv(encounter rate) is half cv(detection function) cv(encounter rate) roughly equal cv(detection function) cv(encounter rate) is double cv(detection function)\n\nWhat are contributing factors causing the significance value of the independent differences to be smaller (0.058) than the significance of the pooled analysis (0.234)?\n\n greater variability in encounter rate greater uncertainty in detection function improved precision in density estimate from pooled analysis expanded difference in the stratum-specific density estimates loss of degrees of freedom in the test statistic",
    "crumbs": [
      "Strata",
      "Tutorial--analysis of stratified surveys ✏️"
    ]
  },
  {
    "objectID": "Pr8/prac8-tutorial.html",
    "href": "Pr8/prac8-tutorial.html",
    "title": "Tutorial–covariates in detection function ✏️",
    "section": "",
    "text": "Photo by Barth Bailey from Unsplash",
    "crumbs": [
      "Covariates",
      "Tutorial--covariates in detection function ✏️"
    ]
  },
  {
    "objectID": "Pr8/prac8-tutorial.html#exercise-8-covariates-in-detection-function",
    "href": "Pr8/prac8-tutorial.html#exercise-8-covariates-in-detection-function",
    "title": "Tutorial–covariates in detection function ✏️",
    "section": "Exercise 8 – Covariates in detection function",
    "text": "Exercise 8 – Covariates in detection function\nUse of covariates in detection function models require more vigilance in assessing possible covariates prior to analysis and in the interpretation of the analysis. This set of questions asks that you take a more critical look at the results of the analyses presented in the covariate practical.\n\nAmakihi songbird data\nBefore using the ds() function, exploratory data analysis screens candidate covariates for potential utility in the detection function as well as possible difficulties that might arise if multiple covariates are included. I alerted you to the problems colinearity in predictors cause.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nAnswer these questions by examining at the output above\n\nWhich of these diagnostic plots indicate a colinearity problem?\n\n top left top right bottom left bottom right\n\nWhat should you do about this?\n\n Do not include observer as covariate in your model Do not include hour and minutes together in your model Do not include observer and hour together in your model Certainly include minutes in your model\n\n\n\n\nInfluence of small values of \\(\\widehat{P_a(z_i)}\\)\nIf a “large” proportion of detections are produced by very small detection probabilities \\(\\widehat{P_a(z_i)}\\) the result can be improbably large abundance estimates. The function p_dist_table() helps you detect this potential problem. Two tables below show results of two calls to p_dist_table() for the amakihi data set.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhy does more severe truncation cause the distribution of \\(\\widehat{P_a(z_i)}\\) estimates to shift to larger values?\n\n detections at large distances have low probability of occurence because of the distance effect detections made by poor observer are removed detections made late in the day are removed",
    "crumbs": [
      "Covariates",
      "Tutorial--covariates in detection function ✏️"
    ]
  },
  {
    "objectID": "Pr8/prac8-tutorial.html#eastern-tropical-pacific-dolphin-analysis",
    "href": "Pr8/prac8-tutorial.html#eastern-tropical-pacific-dolphin-analysis",
    "title": "Tutorial–covariates in detection function ✏️",
    "section": "Eastern Tropical Pacific dolphin analysis",
    "text": "Eastern Tropical Pacific dolphin analysis\nAIC indicates that search.method is the preferred single covariate in a candidate detection function. That’s fine, however, examine the details of the fitted model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWithin this output is a diagnostic that should cause you to question the veracity of this model. What is that diagnostic?\n\n number of observations is incorrect point estimate of \\(\\beta(Search_3)\\) is much larger than other \\(\\beta\\) values standard error of \\(\\beta(Search_3)\\) is 26X larger than its point estimate standard error of shape coefficient exceeds point estimate of shape coefficient",
    "crumbs": [
      "Covariates",
      "Tutorial--covariates in detection function ✏️"
    ]
  },
  {
    "objectID": "Pr8/prac8-tutorial.html#savannah-sparrows-with-pasture-covariate",
    "href": "Pr8/prac8-tutorial.html#savannah-sparrows-with-pasture-covariate",
    "title": "Tutorial–covariates in detection function ✏️",
    "section": "Savannah sparrows with pasture covariate",
    "text": "Savannah sparrows with pasture covariate\nCheck that the image of the probability density function by pasture, created for the 1981 data set is correct. Checking this involves converting the table of \\(\\hat{\\beta}\\) coefficients into estimates of \\(\\hat{\\sigma}\\) which are more easily interpreted. Below is the output from the 1981 data set with a half-normal key function and pasture as a covariate.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFrom the output provide \\(\\hat{\\beta}\\) estimates to the code below to convert them into pasture-specific \\(\\hat{\\sigma}\\) estimates.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterpret the strength of the pasture effect for each pasture by examining the relative magnitude of the standard error to the point estimates of the \\(\\hat{\\beta}\\) coefficients.\n\nEstimated detection probabilities are most similar for which pair of pastures?\n\n pasture 0 and pasture 1 pasture 0 and pasture 2 pasture 0 and pasture 3",
    "crumbs": [
      "Covariates",
      "Tutorial--covariates in detection function ✏️"
    ]
  },
  {
    "objectID": "Pr3/prac3-tutorial.html#practical-3-model-criticism",
    "href": "Pr3/prac3-tutorial.html#practical-3-model-criticism",
    "title": "Tutorial–model criticism ✏️",
    "section": "Practical 3 – Model criticism",
    "text": "Practical 3 – Model criticism",
    "crumbs": [
      "Criticism",
      "Tutorial--model criticism ✏️"
    ]
  },
  {
    "objectID": "Pr3/prac3-tutorial.html#line-transect-analysis",
    "href": "Pr3/prac3-tutorial.html#line-transect-analysis",
    "title": "Tutorial–model criticism ✏️",
    "section": "Line transect analysis",
    "text": "Line transect analysis\nThis data set was simulated so we know both the true population density and the true underlying detection function. Our interest lies in the robustness of the density estimates in the face of model uncertainty. With actual data, we will not know the shape of the underlying process that gives rise to the detection process. It would be reassuring if density estimates were relatively insensitive to choice of detection function model. Let’s find out how sensitive our estimates are for this data set.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExamine the sensitivity of the density estimates from the three models fitted to data truncated at 20m:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nTo the nearest percent, what is the relative percentage difference between smallest and largest estimates presented above? \n\n\nWhat effect did adjustment terms have upon model fit for the half normal and hazard rate key functions with 20m truncation?\n\n caused the hazard rate model to outperform the half normal improved performance of the hazard rate model, but not the half normal adjustment terms did not appear in the final key function models\n\n\n\nModel fit\nOne oversight of the analysis of LTExercise simulated data is the failure to assess model fit. Using the gof_ds function, below is code to perform Cramer-von Mises goodness of fit tests upon all three key function models with 20m truncation. We use the argument plot=FALSE to skip production of the Q-Q plot in this instance.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAll of the three fitted models are admissable to use for inference? TRUEFALSE",
    "crumbs": [
      "Criticism",
      "Tutorial--model criticism ✏️"
    ]
  },
  {
    "objectID": "Pr3/prac3-tutorial.html#capercaillie-data",
    "href": "Pr3/prac3-tutorial.html#capercaillie-data",
    "title": "Tutorial–model criticism ✏️",
    "section": "Capercaillie data",
    "text": "Capercaillie data\nWatch out for danger signs in output of functions. Examine the output of this simple half normal fitted to the exact capercaillie distances. Consider the following output:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat strikes you as strange about the variability associated with encounter rate (se.ER and cv.ER)\n\n they are vanishingly small they are incredibly large\n\nThis is an actual data set, so we do not know the true density of capercaillie in this study area. However we can compare the point estimates of density derived from distances treated as exact and from binned distances.\n\n With binned distances, the estimate of \\(\\hat{D}\\) is an order of magnitude larger than with exact distances \\(\\hat{D}\\) from the half normal with binned distances is between the estimates of \\(\\hat{D}\\) with the half normal and hazard rate keys using exact distances We are not sure whether the detection function fitted to the exact distances fit the data",
    "crumbs": [
      "Criticism",
      "Tutorial--model criticism ✏️"
    ]
  },
  {
    "objectID": "announce/2022-12-29-tutorials/index.html",
    "href": "announce/2022-12-29-tutorials/index.html",
    "title": "How to use tutorials",
    "section": "",
    "text": "While I intend to discuss the solutions to every practical, I want you to have the opportunity to do some self-assessment either before we discuss the solution together, or if you choose to return to a practical sometime in the future.\nI have created a few short answer/multiple choice questions to accompany Practicals 2-8. These tutorials re-create output from the practicals and ask you to interpret that output.\nIn some tutorials, I ask you to complete a short bit of R code to perform some calculations (hints are provided to help you do this). In all tutorials I ask you to step back from pouring over output and ask to to make biological interpretations."
  },
  {
    "objectID": "announce/2022-12-29-tutorials/index.html#dont-worry-about-selecting-incorrect-answers",
    "href": "announce/2022-12-29-tutorials/index.html#dont-worry-about-selecting-incorrect-answers",
    "title": "How to use tutorials",
    "section": "Don’t worry about selecting incorrect answers",
    "text": "Don’t worry about selecting incorrect answers\nThere is no “grade” associated with these tutorials. The answers you provide cannot be seen by me or your fellow participants. The tutorials offer you another opportunity to explore distance sampling analyses. You can assess the results of your explorations through using the tutorials.\nUse of the tutorials is completely optional. If they help you learn about distance sampling analyses, by all means use them. If they interfere with your learning, concentrate on other facets of the workshop materials."
  },
  {
    "objectID": "announce/2022-12-31-humpbacks/index.html",
    "href": "announce/2022-12-31-humpbacks/index.html",
    "title": "Is this analysis sound?",
    "section": "",
    "text": "Received yet another email\nAnother past participant recently wrote this:\n\nI have done many models with covariates that fit the data and they give me very different results regarding abundance (ranging from 600 to 2000 animals). The model with best AIC gives me a very low Pa (0.085) and high abundance. Another model … improved Pa (0.349) and reduced abundance I don’t feel comfortable choosing a model because here my decision is very important.\n\nAsking for further details, this histogram of detection distances was revealed:\n\n\n\nWhat is your diagnosis?\n\n\nFurther investigation\nTo see what happened next, click the arrow at the right of the box below:\n\n\n\n\n\n\nAdditional model fitting\n\n\n\n\n\nModel with the lowest AIC was the hazard rate, trying to fit the spike at small distances. The rapid decline of the fitted hazard rate model resulted in the very low estimate of detection probability. The alternative model described would be the half normal (shown below).\n\nThe issue isn’t so much which model is better, but rather whether the histogram of perpendicular distances is to be believed. What causes the number of detections to fall from ~55 at distances &lt;200m, to ~35 at distances 100-200m.\nThe spike of detections at small distances is very difficult to fit; thinking hard about data collection protocol for the next survey is a reasonable course of action."
  },
  {
    "objectID": "announce/2023-07-12-adjtermsplot/index.html",
    "href": "announce/2023-07-12-adjtermsplot/index.html",
    "title": "Build your own detection function",
    "section": "",
    "text": "Build your own detection function\nTo show the effects of none, one or two cosine adjustment terms upon half normal and hazard rate key functions.\n\n\nThis is a very simple application intended to provide a visual impression of the effect of adjustment terms upon the shape of key functions. You will note the ease with which non-monotonic (functions that both decrease and increase with detection distance) detection functions can be created with only two adjustment terms."
  },
  {
    "objectID": "announce/2023-01-15-cameraeffort/index.html",
    "href": "announce/2023-01-15-cameraeffort/index.html",
    "title": "Effort computation for camera traps",
    "section": "",
    "text": "How to calculate effort\nComputation of effort for each camera involves a series of steps. Computation is not difficult, just remember the steps. We use as an example, Howe’s Maxwell duiker data. In his data (DuikerCameraTrap) supplied in the Distance package, the effort associated with each camera is\n\nsuppressPackageStartupMessages(library(Distance))\ndata(\"DuikerCameraTraps\")\nprint(DuikerCameraTraps$Effort[1])\n\n[1] 680232\n\n\nThis note describes how to that effort was calculated.\n\n\nHow many days were cameras deployed?\nFrom the Methods section of Howe, Buckland, Després-Einspenner, & Kühl (2017), we find the maximum (see Section 5 below) dates of deployment and collection\n\n\nstart &lt;- as.Date(\"2014-06-28\")\nend &lt;- as.Date(\"2014-09-21\")\ndaysbetween &lt;- as.numeric(difftime(end, start, units=\"days\"))\ndaysout &lt;- daysbetween - 1\nprint(daysout)\n\n[1] 84\n\n\nOnly days when there was no research presence are included in analysis, hence one day is subtracted from duration of cameras in the field.\n\n\nSeconds per day cameras in analysis\nData provided in the Distance package is for detections made during “peak” activity period for duikers. That peak period is specified further in the Methods section:\n\nIn the above snippet of text, Howe et al. (2017) states there are 8098 snapshot events per day within this peak activity period. For their cameras, the duration of a snapshot was 2 seconds. Let’s recreate that value:\n\nsnapshot.duration &lt;- 2\nmorning.start &lt;- as.POSIXct(\"2014-06-28 06:30:00\" , format=\"%Y-%m-%d %H:%M:%OS\")\nmorning.end &lt;- as.POSIXct(\"2014-06-28 08:59:58\" , format=\"%Y-%m-%d %H:%M:%OS\")\nevening.start &lt;- as.POSIXct(\"2014-06-28 16:00:00\" , format=\"%Y-%m-%d %H:%M:%OS\")\nevening.end &lt;- as.POSIXct(\"2014-06-28 17:59:58\" , format=\"%Y-%m-%d %H:%M:%OS\")\n\nmorning &lt;- difftime(morning.end, morning.start, units=\"secs\")\nevening &lt;- difftime(evening.end, evening.start, units=\"secs\")\ntotal.secs &lt;- as.numeric(morning) + as.numeric(evening)\ndaily.snapshots &lt;- total.secs / snapshot.duration\nprint(daily.snapshots)\n\n[1] 8098\n\n\n\n\nSnapshot events for entire duration\nNearly finished. Multiply daily snapshots by the maximum number of days of deployment:\n\ntotal.snapshots &lt;- daysout * daily.snapshots\nprint(total.snapshots)\n\n[1] 680232\n\nprint(total.snapshots == DuikerCameraTraps$Effort[1])\n\n[1] TRUE\n\n\nThis tells us the first sampling station had the camera operating for the full duration of the period (28 June through 21 September). However, that is not the case for all the cameras deployed in the survey.\n\n\nBut wait, there is more\nThis calculation is fine for the camera placed at the first sampling location. However, if you examine DuikerCameraTraps$Effort you will note that the number of snapshot moments differs between stations. If you have conducted field work with camera traps, this comes as no surprise to you. Each camera deployment is unique for several reasons:\n\nbattery failure\ncamera is dislodged by weather, animals or humans\ntime lag between camera deployments and collections\n\nparticularly in difficult environments like Côte d’Ivoire, travel time between sampling stations may be several days.\n\n\nIn the study described by Howe et al. (2017), sampling stations were separated by 1km. Consequently, deployment (and camera collection) dates differed among cameras. We can reconstruct the number of days cameras were active by dividing their number of snapshot moments by the number of snapshot moments per day:\n\nstations &lt;- unique(DuikerCameraTraps$Sample.Label)\nstation.effort &lt;- vector(\"numeric\", length=length(stations))\nfor( i in 1:length(stations)) {\n  station.effort[i] &lt;- DuikerCameraTraps$Effort[DuikerCameraTraps$Sample.Label==stations[i]]\n}\nstation.days &lt;- station.effort/daily.snapshots\n\nHence the number of days each station was active is shown in the following table:\n\nstation.table &lt;- data.frame(Station=stations, Snapshots=station.effort,\n                            Days=station.days)\nknitr::kable(station.table,\n             caption=\"Days each camera trap station was active for Duiker survey (peak activity)\",\n             align='r')\n\n\nDays each camera trap station was active for Duiker survey (peak activity)\n\n\nStation\nSnapshots\nDays\n\n\n\n\nA1\n680232\n84\n\n\nA2\n129568\n16\n\n\nA3\n672134\n83\n\n\nA4\n680232\n84\n\n\nB1\n680232\n84\n\n\nB2\n591154\n73\n\n\nB3\n591154\n73\n\n\nB4\n502076\n62\n\n\nC1\n680232\n84\n\n\nC2\n672134\n83\n\n\nC3\n655938\n81\n\n\nC4\n639742\n79\n\n\nC5\n623546\n77\n\n\nC6\n647840\n80\n\n\nD3\n631644\n78\n\n\nD4\n623546\n77\n\n\nD5\n542566\n67\n\n\nE3\n291528\n36\n\n\nE4\n631644\n78\n\n\nE5\n583056\n72\n\n\nE6\n566860\n70\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHowe, E. J., Buckland, S. T., Després-Einspenner, M.-L., & Kühl, H. S. (2017). Distance sampling with camera traps. Methods in Ecology and Evolution, 8(11), 1558–1565. https://doi.org/10.1111/2041-210X.12790"
  },
  {
    "objectID": "announce/2023-07-12-labelledoutput/index.html",
    "href": "announce/2023-07-12-labelledoutput/index.html",
    "title": "Annotated ds output",
    "section": "",
    "text": "Below is output produced from fitting a hazard rate key function (without adjustment terms) to the ducknest data set. It is important to study model output, not just focus upon the estimated density. There may be clues in the output to indicate that data have not been analysed as you intended (misspecification of truncation distances, mistyping of the key function, etc.)\nFirst four lines are R code to fit the model, remainder of lines is output produced by the summary() function.\n\nI have not labelled everything to prevent the image from becoming too cluttered. I have not labelled the different measures of precision associated with estimates and summary statistics. I have not fitted a model that included adjustment terms; had I done so, the output would also have contained estimates of the adjustment term coefficients (\\(\\alpha_i\\)) and their measures of precision.\nThe ducknest data are objects that appear singly (not in groups). Has this been a model fitted to objects that were detected as clusters, there would have been additional output. Likewise, if the survey had been a stratified survey, there would have been density (and abundance) estimates for each stratum.\nThe labelled output should assist you in navigating the model objects created by the ds() function."
  },
  {
    "objectID": "announce/2023-07-12-labelledoutput/index.html#making-sense-of-fitted-model-objects",
    "href": "announce/2023-07-12-labelledoutput/index.html#making-sense-of-fitted-model-objects",
    "title": "Annotated ds output",
    "section": "",
    "text": "Below is output produced from fitting a hazard rate key function (without adjustment terms) to the ducknest data set. It is important to study model output, not just focus upon the estimated density. There may be clues in the output to indicate that data have not been analysed as you intended (misspecification of truncation distances, mistyping of the key function, etc.)\nFirst four lines are R code to fit the model, remainder of lines is output produced by the summary() function.\n\nI have not labelled everything to prevent the image from becoming too cluttered. I have not labelled the different measures of precision associated with estimates and summary statistics. I have not fitted a model that included adjustment terms; had I done so, the output would also have contained estimates of the adjustment term coefficients (\\(\\alpha_i\\)) and their measures of precision.\nThe ducknest data are objects that appear singly (not in groups). Has this been a model fitted to objects that were detected as clusters, there would have been additional output. Likewise, if the survey had been a stratified survey, there would have been density (and abundance) estimates for each stratum.\nThe labelled output should assist you in navigating the model objects created by the ds() function."
  },
  {
    "objectID": "announce/2022-12-31-modelsel/index.html",
    "href": "announce/2022-12-31-modelsel/index.html",
    "title": "Is this analysis sound?",
    "section": "",
    "text": "Received this email\nA former workshop participant recently wrote more than a year after attending this workshop. She had been working on her own data and was concluding her analysis. She sent this table (produced by the summarize_ds_models() function) asking\n\nIs it fine for me to use the model with the smallest AIC for inference for this survey?\n\n\nHow would you respond?\n\n\n\nPostscript\nThe email also included a table of stratum-specific estimates. I hide this table so as not to spoil the answer to the question above. If you are curious, simply click on the arrow on the top right.\n\n\n\n\n\n\nEstimates from the preferred model\n\n\n\n\n\n The warning signs are more obvious in this output than from the model selection table. However, you are discouraged from looking at estimates produced by models until your model selection process in complete. This prevents subconscious bias to use models that produce results the analyst “likes.”"
  },
  {
    "objectID": "announce/2023-01-02-mysterysoln/index.html",
    "href": "announce/2023-01-02-mysterysoln/index.html",
    "title": "Solution to mystery analysis",
    "section": "",
    "text": "Demonstration\n\n\n\nStart-to-finish analysis of mystery data set\nI’m reasonably content to stick with this rule of thumb, could slice out one more data point, if I truncated at 50m.\nmyunits &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\nfirst &lt;- ds(mydata, convert_units = myunits)\nplot(first, breaks=seq(0,80,length=17))\n\nSpecified endpoints &gt; 79; values reset.\n\nabline(h=0.15, col=\"red\", lty=3)\ntext(x=50, y=0.2, \"Pr(detect)=0.15\", cex=.8)\n\n\n\n\n\n\n\ntruncate &lt;- 50"
  },
  {
    "objectID": "announce/2023-01-02-mysterysoln/index.html#aic-among-remaining-five-competitors",
    "href": "announce/2023-01-02-mysterysoln/index.html#aic-among-remaining-five-competitors",
    "title": "Solution to mystery analysis",
    "section": "AIC among remaining five competitors",
    "text": "AIC among remaining five competitors\n\nAIC(unicos, hn, hr, hn.sex, hr.sex)\n\n       df      AIC\nunicos  1 307.7994\nhn      1 307.3470\nhr      3 309.1185\nhn.sex  2 304.1897\nhr.sex  3 305.6042\n\n\nWhat emerges? Half normal and uniform with cosine adjustment have very similar shapes and are quite similar models. The hazard rate key without a covariate seems out of contention. What about the similarity in \\(\\hat{P_a}\\) between competing models?\n\nkable(summarize_ds_models(unicos, hn,hr, hn.sex, hr.sex, output=\"plain\"), digits=3, row.names = FALSE,\n                    caption=\"Five competing models with truncation at 50m\")  %&gt;%\n      kable_styling(full_width = F) %&gt;%\n      column_spec(5, background=\"yellow\", width=\"8em\")\n\n\nFive competing models with truncation at 50m\n\n\nModel\nKey function\nFormula\nC-vM $p$-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nhn.sex\nHalf-normal\n~sex\n0.727\n0.567\n0.092\n0.000\n\n\nhr.sex\nHazard-rate\n~sex\n0.863\n0.449\n0.198\n1.415\n\n\nhn\nHalf-normal\n~1\n0.683\n0.638\n0.091\n3.157\n\n\nunicos\nUniform with cosine adjustment term of order 1\nNA\n0.706\n0.624\n0.079\n3.610\n\n\nhr\nHazard-rate with cosine adjustment term of order 2\n~1\n0.923\n0.554\n0.122\n4.929\n\n\n\n\n\n\n\nModels without sex as a covariate estimate a larger \\(\\hat{P_a}\\), so even with only 4 female detections, those detections do exert an influence upon the shape of the estimated detection function and consequently upon \\(\\hat{P_a}\\)."
  },
  {
    "objectID": "announce/2023-01-02-mysterysoln/index.html#best-model-half-normal-with-sex-covariate",
    "href": "announce/2023-01-02-mysterysoln/index.html#best-model-half-normal-with-sex-covariate",
    "title": "Solution to mystery analysis",
    "section": "Best model, Half normal with sex covariate",
    "text": "Best model, Half normal with sex covariate\n\nprint(hn.sex$dht$individual$N)\n\n  Label Estimate       se        cv      lcl      ucl      df\n1 Total 164.5661 32.04176 0.1947045 111.3911 243.1253 38.7038\n\n\nIf we wanted to employ the gold standard in precision estimation, we would apply a bootstrap\n\nbootout &lt;- bootdht(hn.sex, flatfile=mydata, summary_fun = bootdht_Nhat_summarize,\n                   nboot=500, convert_units = myunits)\n\nPerforming 500 bootstraps\n\n\n\nhist(bootout$Nhat, breaks = 20, \n     main=\"Distribution of bootstrap replicates\", xlab=\"Abundance estimate\")\nmybounds &lt;- round(quantile(bootout$Nhat, c(0.025, 0.975),na.rm=TRUE))\nabline(v=mybounds, lwd=2, lty=3)\n\n\n\n\n\n\n\n\nConfidence interval bounds from bootstrap are (121, 411), somewhat wider than the analytical confidence interval bounds specified above."
  },
  {
    "objectID": "announce/2023-01-02-mysterysoln/index.html#closest-aic-score-competitor-hazard-rate-with-sex-covariate",
    "href": "announce/2023-01-02-mysterysoln/index.html#closest-aic-score-competitor-hazard-rate-with-sex-covariate",
    "title": "Solution to mystery analysis",
    "section": "Closest AIC score competitor, Hazard rate with sex covariate",
    "text": "Closest AIC score competitor, Hazard rate with sex covariate\n\nprint(hr.sex$dht$individual$N)\n\n  Label Estimate      se        cv     lcl      ucl       df\n1 Total 207.7639 95.9443 0.4617948 85.8307 502.9186 47.96291\n\n\nNotice the price, in terms of precision, paid for the extra parameter estimated in the hazard rate model compared to the half normal model.\n\n\n\n\n\nPoint and 95% interval estimates based upon truncation distance of 50m."
  },
  {
    "objectID": "announce/2022-12-31-moreresources/index.html",
    "href": "announce/2022-12-31-moreresources/index.html",
    "title": "More resources",
    "section": "",
    "text": "Database of papers related to these lectures\nExamine the bibliography associated with St Andrews workshops. This is a subset of a much larger searchable data base of publications dealing with distance sampling compiled by Tiago Marques.\nI encourage you to keep this paper as a reference for the coding done during the workshop:\n\nMiller, D., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. 2019. Distance Sampling in R. Journal of Statistical Software. [Online] 89:1 https://dx.doi.org/10.18637/jss.v089.i01\n\n\n\nBooks\nThe standard reference for distance sampling is Buckland et al. (2001). This was most recently updated in Buckland et al. (2015). If you are affiliated with a university that is a member of ‘Springerlink’, the 2015 version can be purchased in softcover for ~40 USD/GBP/EUR; look up the MyCopy program.\n\n\nWebsites\n\nNews and general information about distance sampling can be found at our home website distancesampling.org.\n\nOur workshop materials reside on their own website workshops.distancesampling.org.\nIf you wish to watch narrated lecture videos and narrated exercises, visit workshops.distancesampling.org/online-course.\n\n\n\nEmail list for assistance\nWe operate a forum where you can ask questions and see if others have asked questions similar to yours. Joining instructions to access the Google list can be found following this link."
  }
]